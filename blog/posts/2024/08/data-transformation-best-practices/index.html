<!DOCTYPE html><html lang="en"><head itemscope=""><script type="application/ld+json">{"@context":"https://schema.org","@graph":[{"@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"item":{"@id":"insights","name":"Insights"}}]},{"@type":"BlogPosting","@id":"https://double.cloud/blog/posts/2024/08/data-transformation-best-practices/","url":"https://double.cloud/blog/posts/2024/08/data-transformation-best-practices/","name":"Best practices for data transformation as pipeline complexity grows","headline":"Best practices for data transformation as pipeline complexity grows","abstract":"","description":"","dateCreated":"2024-08-20T00:00:00Z","datePublished":"2024-08-20T00:00:00Z","dateModified":"2024-08-20T00:00:00Z","author":{"@type":"Organization","name":"DoubleCloud","legalName":"DoubleCloud Inc","url":"https://double.cloud/","logo":{"@type":"ImageObject","url":"","width":32,"height":32}},"creator":{"@type":"Organization","name":"DoubleCloud","legalName":"DoubleCloud Inc","url":"https://double.cloud/","logo":{"@type":"ImageObject","url":"","width":32,"height":32}},"publisher":{"@type":"Organization","name":"DoubleCloud","legalName":"DoubleCloud Inc","url":"https://double.cloud/","logo":{"@type":"ImageObject","url":"","width":32,"height":32}},"copyrightHolder":{"@type":"Organization","name":"DoubleCloud","legalName":"DoubleCloud Inc","url":"https://double.cloud/","logo":{"@type":"ImageObject","url":"","width":32,"height":32}},"copyrightYear":2025,"mainEntityOfPage":{"@type":"WebPage","@id":"https://double.cloud/blog/posts/2024/08/data-transformation-best-practices/"},"inLanguage":{"@type":"Language","name":"English","alternateName":"en"},"keywords":["Insights"],"image":"https://double.cloud/assets/blog/articles/2024/best-practices-for-data-transformation-as-pipeline-complexity-grows-small-cover.png","sharedContent":{"@type":"WebPage","headline":"Best practices for data transformation as pipeline complexity grows","url":"https://double.cloud/blog/posts/2024/08/data-transformation-best-practices/","author":{"@type":"Organization","name":"DoubleCloud","legalName":"DoubleCloud Inc","url":"https://double.cloud/","logo":{"@type":"ImageObject","url":"","width":32,"height":32}}},"wordCount":"","articleBody":""}]}</script><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Best practices for data transformation as pipeline complexity grows | DoubleCloud</title><meta name="description" content="Modern data platforms revolve around data transformation. There are different levels of transformation, and you must choose the appropriate tools for each level. Discover our experience and best practices."/><link rel="canonical" href="../data-transformation-best-practices.html"/><meta itemProp="name" content="Best practices for data transformation as pipeline complexity grows"/><meta itemProp="image" content="https://double.cloud/assets/blog/articles/2024/best-practices-for-data-transformation-as-pipeline-complexity-grow-sharing.png"/><meta property="og:type" content="website"/><meta property="og:url" content="https://double.cloud/blog/posts/2024/08/data-transformation-best-practices/"/><meta property="og:title" content="Best practices for data transformation as pipeline complexity grows"/><meta property="og:image" content="https://double.cloud/assets/blog/articles/2024/best-practices-for-data-transformation-as-pipeline-complexity-grow-sharing.png"/><meta property="og:locale" content="en"/><meta property="og:site_name" content="DoubleCloud"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Best practices for data transformation as pipeline complexity grows"/><meta name="twitter:image" content="https://double.cloud/assets/blog/articles/2024/best-practices-for-data-transformation-as-pipeline-complexity-grow-sharing.png"/><meta name="robots" content="follow, index"/><meta property="article:published_time" content="2024-08-20T00:00:00Z"/><meta property="article:author" content=""/><meta property="article:tag" content="Insights"/><meta name="next-head-count" content="21"/><link rel="icon" href="../../../../../assets/favicon/favicon.ico" sizes="any"/><link type="image/x-icon" rel="shortcut icon" href="../../../../../assets/favicon/favicon.ico"/><link type="image/png" sizes="16x16" rel="icon" href="../../../../../assets/favicon/favicon-16x16.png"/><link type="image/png" sizes="32x32" rel="icon" href="../../../../../assets/favicon/favicon-32x32.png"/><link type="image/png" sizes="120x120" rel="icon" href="../../../../../assets/favicon/favicon-120x120.png"/><link type="image/png" sizes="192x192" rel="icon" href="../../../../../assets/favicon/favicon-192x192.png"/><link type="image/png" sizes="76x76" rel="apple-touch-icon" href="https://double.cloud/assets/favicon/favicon-76x76.png"/><link type="image/png" sizes="152x152" rel="apple-touch-icon" href="../../../../../assets/favicon/favicon-152x152.png"/><link type="image/png" sizes="180x180" rel="apple-touch-icon" href="../../../../../assets/favicon/favicon-180x180.png"/><script id="data-google-tag-manager" nonce="MnuBsw6YlmEOCSocT1fUiQ==" data-nonce="MnuBsw6YlmEOCSocT1fUiQ==">
                // Define dataLayer and the gtag function.
                window.dataLayer = window.dataLayer || [];
                function gtag(){dataLayer.push(arguments);}

                // Default analytics_storage to 'denied'.
                window.gtag = window.gtag || gtag;

                const hasAnalyticsConsent = window?.localStorage.getItem('hasAnalyticsConsent');
                const consent =  hasAnalyticsConsent === 'true' ? 'granted' : 'denied';

                window.gtag('consent', 'default', {
                    'analytics_storage': consent,
                    'ad_storage': consent,
                    'wait_for_update': hasAnalyticsConsent === 'true' ? 0 : Infinity,
                });

                dataLayer.push({
                    'event': 'default_consent'
                });

                (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
                j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
                'https://www.googletagmanager.com/gtm.js?id='+i+dl;var n=d.querySelector('[nonce]');
                n&&j.setAttribute('nonce',n.nonce||n.getAttribute('nonce'));f.parentNode.insertBefore(j,f);
                })(window,document,'script','dataLayer','GTM-5M39N8J');
            </script><script nonce="MnuBsw6YlmEOCSocT1fUiQ==">window.__webpack_nonce__ = "MnuBsw6YlmEOCSocT1fUiQ=="</script><link rel="preconnect" href="https://www.googletagmanager.com"/><link rel="preconnect" href="https://snap.licdn.com"/><link rel="preconnect" href="https://www.google.com"/><link nonce="MnuBsw6YlmEOCSocT1fUiQ==" rel="preload" href="../../../../../_next/static/css/a4c87e381fd61058.css" as="style"/><link nonce="MnuBsw6YlmEOCSocT1fUiQ==" rel="stylesheet" href="../../../../../_next/static/css/a4c87e381fd61058.css" data-n-g=""/><link nonce="MnuBsw6YlmEOCSocT1fUiQ==" rel="preload" href="../../../../../_next/static/css/2facd84af36bff2e.css" as="style"/><link nonce="MnuBsw6YlmEOCSocT1fUiQ==" rel="stylesheet" href="../../../../../_next/static/css/2facd84af36bff2e.css" data-n-p=""/><link nonce="MnuBsw6YlmEOCSocT1fUiQ==" rel="preload" href="../../../../../_next/static/css/c413166e8b0da734.css" as="style"/><link nonce="MnuBsw6YlmEOCSocT1fUiQ==" rel="stylesheet" href="../../../../../_next/static/css/c413166e8b0da734.css" data-n-p=""/><link nonce="MnuBsw6YlmEOCSocT1fUiQ==" rel="preload" href="../../../../../_next/static/css/248e88462928fa2f.css" as="style"/><link nonce="MnuBsw6YlmEOCSocT1fUiQ==" rel="stylesheet" href="../../../../../_next/static/css/248e88462928fa2f.css" data-n-p=""/><link nonce="MnuBsw6YlmEOCSocT1fUiQ==" rel="preload" href="../../../../../_next/static/css/eb8a627e7f585420.css" as="style"/><link nonce="MnuBsw6YlmEOCSocT1fUiQ==" rel="stylesheet" href="../../../../../_next/static/css/eb8a627e7f585420.css" data-n-p=""/><noscript data-n-css="MnuBsw6YlmEOCSocT1fUiQ=="></noscript><script defer="" nonce="MnuBsw6YlmEOCSocT1fUiQ==" nomodule="" src="../../../../../_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="../../../../../_next/static/chunks/webpack-d326a7489defa990.js" nonce="MnuBsw6YlmEOCSocT1fUiQ==" defer=""></script><script src="../../../../../_next/static/chunks/framework-cc7effedd0fd3d95.js" nonce="MnuBsw6YlmEOCSocT1fUiQ==" defer=""></script><script src="../../../../../_next/static/chunks/main-ebfff3515213fa2f.js" nonce="MnuBsw6YlmEOCSocT1fUiQ==" defer=""></script><script src="../../../../../_next/static/chunks/pages/_app-4cd98c5be1eceb26.js" nonce="MnuBsw6YlmEOCSocT1fUiQ==" defer=""></script><script src="../../../../../_next/static/chunks/f69bbb46-eed95df46583a2d8.js" nonce="MnuBsw6YlmEOCSocT1fUiQ==" defer=""></script><script src="../../../../../_next/static/chunks/030d571f-c7510aa4f8d650e7.js" nonce="MnuBsw6YlmEOCSocT1fUiQ==" defer=""></script><script src="../../../../../_next/static/chunks/193-fdb54e47dd6b7c7b.js" nonce="MnuBsw6YlmEOCSocT1fUiQ==" defer=""></script><script src="../../../../../_next/static/chunks/756-04d1c95c632019ed.js" nonce="MnuBsw6YlmEOCSocT1fUiQ==" defer=""></script><script src="../../../../../_next/static/chunks/387-27526d5e8e2a9173.js" nonce="MnuBsw6YlmEOCSocT1fUiQ==" defer=""></script><script src="../../../../../_next/static/chunks/pages/blog/posts/[...slug]-896d627301783262.js" nonce="MnuBsw6YlmEOCSocT1fUiQ==" defer=""></script><script src="../../../../../_next/static/HkxA3M0ES7gp3V0n_0ecw/_buildManifest.js" nonce="MnuBsw6YlmEOCSocT1fUiQ==" defer=""></script><script src="../../../../../_next/static/HkxA3M0ES7gp3V0n_0ecw/_ssgManifest.js" nonce="MnuBsw6YlmEOCSocT1fUiQ==" defer=""></script></head><body class="dc-root g-root g-root_theme_dark"><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5M39N8J" title="Googletagmanager" height="0" width="0" style="display:none;visibility:hidden" loading="lazy"></iframe></noscript><div id="__next" data-reactroot=""><div class="layout"><a href="https://doublecloud-archive.github.io/blog/posts/2024/10/doublecloud-final-update/"><div class="pc-Grid header-anncouncement"><div class="container-fluid "><div class="row"><div class="col"><div class="yfm yfm_constructor"><p><b>DoubleCloud has wound down operations</b> | This is&nbsp;an&nbsp;archived version of&nbsp;the site. <b>Learn more &rarr; </b></p></div></div></div></div></div></a><div class="layout__content"><div class="g-root g-root_theme_dark pc-page-constructor"><div class="pc-page-constructor__wrapper"><div class="pc-layout"><div class="pc-Grid pc-navigation pc-layout__navigation"><div class="container-fluid "><div class="row"><div class="col"><nav><div class="pc-desktop-navigation__wrapper"><div class="pc-desktop-navigation__left"><div class="link" data-link-type="router"><span class="pc-logo pc-desktop-navigation__logo"><picture><img alt="Logo icon" src="../../../../../assets/logo/dc-logo-dark.svg" class="pc-logo__icon"/></picture><span class="pc-logo__text"></span></span></div></div><div class="pc-desktop-navigation__navigation-container"><div class="pc-overflow-scroller__container"><div class="pc-overflow-scroller pc-desktop-navigation__navigation"><div class="pc-overflow-scroller__wrapper" style="left:0"><ul class="pc-desktop-navigation__links"><li class="pc-navigation-item pc-navigation-item_menu-layout_desktop pc-desktop-navigation__item"><button class="dc-dropdown-navigation-item__control"><span class="dc-dropdown-navigation-item__title">Why DoubleCloud</span></button><div style="position:fixed;left:0;top:0" class="g-popup dc-dropdown-navigation-item__dropdown"><div class="g-popup__content dc-dropdown-navigation-item__dropdown-content-wrapper" tabindex="-1"><div class="group-list-content"><div class="row item-list-content"><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Performance" data-link-type="router" href="../../../../../performance-boost/index.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">Performance</span><span class="navigation-popup-item__description">Get the best performance with the highest ROI</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Security" data-link-type="router" href="../../../../../security.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">Security</span><span class="navigation-popup-item__description">Keep your data protected and maintain compliance</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="DoubleCloud vs. other solutions" data-link-type="router" href="../../../../../comparison/index.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">DoubleCloud vs. other solutions</span><span class="navigation-popup-item__description">Learn how DoubleCloud’s products compare to other solutions</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Customer stories" data-link-type="router" href="../../../../../resources/case-studies/index.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">Customer stories</span><span class="navigation-popup-item__description">See our solutions in action</span></div></a></div></div><div class="group-list-content__banner"><picture><img alt="" src="../../../../../assets/doublecloud/menu-bar/menu-banner-dc-results.png.webp" class="group-list-content__image" style="width:300px;height:300px"/></picture><span class="yfm yfm_constructor"><a href='../../../../../performance-boost/index.html' target='_self'>Get more and spend less with DoubleCloud  →</a></span></div></div></div></div></li><li class="pc-navigation-item pc-navigation-item_menu-layout_desktop pc-desktop-navigation__item"><button class="dc-dropdown-navigation-item__control"><span class="dc-dropdown-navigation-item__title">Products</span></button><div style="position:fixed;left:0;top:0" class="g-popup dc-dropdown-navigation-item__dropdown"><div class="g-popup__content dc-dropdown-navigation-item__dropdown-content-wrapper" tabindex="-1"><div class="row item-list-content"><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Managed Service for ClickHouse®" data-link-type="router" href="../../../../../services/managed-clickhouse.html"><picture class="navigation-popup-item__icon-container"><img alt="" src="../../../../../assets/icons/dc-clickhouse.svg" class="navigation-popup-item__icon"/></picture><div class="navigation-popup-item__container navigation-popup-item__container_with-margin"><span class="navigation-popup-item__title">Managed Service for ClickHouse®</span><span class="navigation-popup-item__description">The fastest, most resource-efficient OLAP database for real-time analytics</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Managed Service for Apache Kafka®" data-link-type="router" href="../../../../../services/managed-kafka.html"><picture class="navigation-popup-item__icon-container"><img alt="" src="../../../../../assets/icons/dc-kafka.svg" class="navigation-popup-item__icon"/></picture><div class="navigation-popup-item__container navigation-popup-item__container_with-margin"><span class="navigation-popup-item__title">Managed Service for Apache Kafka®</span><span class="navigation-popup-item__description">A leading data streaming technology for large-scale, data-intensive applications</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Managed Service for Apache Airflow®" data-link-type="router" href="../../../../../services/managed-airflow/index.html"><picture class="navigation-popup-item__icon-container"><img alt="" src="../../../../../assets/icons/dc-airflow.svg" class="navigation-popup-item__icon"/></picture><div class="navigation-popup-item__container navigation-popup-item__container_with-margin"><span class="navigation-popup-item__title">Managed Service for Apache Airflow®</span><span class="navigation-popup-item__description">Open-source tool to orchestrate and monitor workflows</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Data Transfer" data-link-type="router" href="../../../../../services/doublecloud-transfer.html"><picture class="navigation-popup-item__icon-container"><img alt="" src="../../../../../assets/icons/dc-transfer.svg" class="navigation-popup-item__icon"/></picture><div class="navigation-popup-item__container navigation-popup-item__container_with-margin"><span class="navigation-popup-item__title">Data Transfer</span><span class="navigation-popup-item__description">No-code ELT tool for aggregating, collecting, and migrating data</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Data Visualization" data-link-type="router" href="../../../../../services/doublecloud-visualization.html"><picture class="navigation-popup-item__icon-container"><img alt="" src="../../../../../assets/icons/dc-data-vis.svg" class="navigation-popup-item__icon"/></picture><div class="navigation-popup-item__container navigation-popup-item__container_with-margin"><span class="navigation-popup-item__title">Data Visualization</span><span class="navigation-popup-item__description">Free tool to create, modify, and share dashboards and charts</span></div></a></div></div></div></div></li><li class="pc-navigation-item pc-navigation-item_menu-layout_desktop pc-desktop-navigation__item"><button class="dc-dropdown-navigation-item__control"><span class="dc-dropdown-navigation-item__title">Solutions</span></button><div style="position:fixed;left:0;top:0" class="g-popup dc-dropdown-navigation-item__dropdown"><div class="g-popup__content dc-dropdown-navigation-item__dropdown-content-wrapper" tabindex="-1"><div class="group-list-content"><div class="row item-list-content"><h4 class="item-list-content__title">By use case</h4><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Customer-facing analytics" data-link-type="router" href="../../../../../customer-facing-analytics/index.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">Customer-facing analytics</span><span class="navigation-popup-item__description">Provide business insights for your clients or partners</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Real-time analytics" data-link-type="router" href="../../../../../solutions/real-time-analytics/index.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">Real-time analytics</span><span class="navigation-popup-item__description">Build a data infrastructure to collect, process, and analyze data in real time</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Observability and monitoring" data-link-type="router" href="../../../../../solutions/observability-and-monitoring/index.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">Observability and monitoring</span><span class="navigation-popup-item__description">Analyze terabytes of your logs, events, and traces with ease</span></div></a></div></div><div class="row item-list-content"><h4 class="item-list-content__title">By industry</h4><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="AdTech and MarTech data analytics" data-link-type="router" href="../../../../../solutions/adtech.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">AdTech and MarTech data analytics</span><span class="navigation-popup-item__description">Extract and analyze data from Meta ads, Google ads, LinkedIn ads, and others</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Analytics for mobile and gaming apps" data-link-type="router" href="../../../../../solutions/web-mobile-gaming-apps.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">Analytics for mobile and gaming apps</span><span class="navigation-popup-item__description">Optimize and scale your mobile and gaming app analytics</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="EdTech data analytics" data-link-type="router" href="../../../../../solutions/edtech/index.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">EdTech data analytics</span><span class="navigation-popup-item__description">Improve online learning and identify new sales opportunities</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="FinTech data analytics" data-link-type="router" href="../../../../../solutions/fintech-real-time-analytics/index.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">FinTech data analytics</span><span class="navigation-popup-item__description">Manage and process large amounts of financial data efficiently</span></div></a></div></div></div></div></div></li><li class="pc-navigation-item pc-navigation-item_menu-layout_desktop pc-desktop-navigation__item"><button class="dc-dropdown-navigation-item__control dc-dropdown-navigation-item__control_selected"><span class="dc-dropdown-navigation-item__title">Resources</span></button><div style="position:fixed;left:0;top:0" class="g-popup dc-dropdown-navigation-item__dropdown"><div class="g-popup__content dc-dropdown-navigation-item__dropdown-content-wrapper" tabindex="-1"><div class="group-list-content"><div class="row item-list-content"><h4 class="item-list-content__title">Using DoubleCloud</h4><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="DoubleCloud API" href="../../../../../docs/en/public-api/index.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">DoubleCloud API</span><span class="navigation-popup-item__description">Read up on API tutorials and instructions</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Terraform" href="../../../../../docs/en/developer-resources/terraform/create-resources.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">Terraform</span><span class="navigation-popup-item__description">Deploy and manage cloud resources with the infrastructure-as-code approach</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Status updates" data-link-type="router" href="https://status.double.cloud/"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">Status updates</span><span class="navigation-popup-item__description">Check the current operational status of our services</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Support" data-link-type="router" href="../../../../../support/index.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">Support</span><span class="navigation-popup-item__description">Learn more about our support tiers</span></div></a></div></div><div class="row item-list-content"><h4 class="item-list-content__title">Discover</h4><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Webinars" data-link-type="router" href="../../../../../webinars/index.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">Webinars</span><span class="navigation-popup-item__description">Sign up for the next webinar or watch previous ones</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover navigation-popup-item__content_selected" aria-label="Blog" data-link-type="router" href="../../../../index.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">Blog</span><span class="navigation-popup-item__description">Get insights from our team and the latest news</span></div></a></div></div><div class="group-list-content__banner"><picture><img alt="" src="../../../../../assets/doublecloud/menu-bar/menu-banners-dc-ebook.png.webp" class="group-list-content__image" style="width:300px;height:300px"/></picture><span class="yfm yfm_constructor"><a href='../../../../../resources/clickhouse-ebook/index.html' target='_self'>Grab your ebook  →</a></span></div></div></div></div></li><li class="pc-navigation-item pc-navigation-item_menu-layout_desktop pc-desktop-navigation__item"><button class="dc-dropdown-navigation-item__control"><span class="dc-dropdown-navigation-item__title">Company</span></button><div style="position:fixed;left:0;top:0" class="g-popup dc-dropdown-navigation-item__dropdown"><div class="g-popup__content dc-dropdown-navigation-item__dropdown-content-wrapper" tabindex="-1"><div class="row item-list-content"><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="About DoubleCloud" data-link-type="router" href="../../../../../company/about-us.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">About DoubleCloud</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Careers" data-link-type="router" href="../../../../../company/careers.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">Careers</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Contact us" data-link-type="router" href="../../../../../company/contact-us.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">Contact us</span></div></a></div></div></div></div></li><li class="pc-navigation-item pc-navigation-item_menu-layout_desktop pc-desktop-navigation__item"><a aria-label="Pricing" class="pc-navigation-item__content pc-navigation-item__content_type_link" data-link-type="router" href="../../../../../pricing.html"><div class="navigation-item"><span class="navigation-item__text">Pricing</span></div></a></li><li class="pc-navigation-item pc-navigation-item_menu-layout_desktop pc-desktop-navigation__item"><a href="../../../../../docs/index.html" aria-label="Documentation" class="pc-navigation-item__content pc-navigation-item__content_type_link" target="_self"><div class="navigation-item"><span class="navigation-item__text">Documentation</span></div></a></li></ul></div></div></div></div><div class="pc-desktop-navigation__right"><button type="button" aria-label="Button label" class="pc-control pc-control_size_l pc-control_theme_primary pc-mobile-menu-button"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="24" height="24" class="g-icon" fill="currentColor" stroke="none" data-qa="icon-test-id" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 16 16"><path fill="currentColor" fill-rule="evenodd" d="M1.25 3.25A.75.75 0 0 1 2 2.5h12A.75.75 0 0 1 14 4H2a.75.75 0 0 1-.75-.75Zm0 4.75A.75.75 0 0 1 2 7.25h12a.75.75 0 0 1 0 1.5H2A.75.75 0 0 1 1.25 8ZM2 12a.75.75 0 0 0 0 1.5h12a.75.75 0 0 0 0-1.5H2Z" clip-rule="evenodd"></path></svg></svg></button></div></div><div></div></nav></div></div></div></div><main class="pc-layout__content"><section class="bc-wrapper bc-wrapper_padding-top_xs bc-wrapper_padding-bottom_l"><header class="pc-header-block pc-header-block_media-view_full"><div class="pc-header-block__background pc-header-block__background_media" style="background-color:#000000"><div class="pc-Media pc-header-block__background-media" style="background-color:#000000"><div style="transform:"><div class="pc-storage-background-image pc-media-component-image__item pc-header-block__image" data-qa="background-image"><picture data-qa="background-image-image"><img fetchpriority="high" alt="" src="../../../../../assets/blog/articles/2024/best-practices-for-data-transformation-as-pipeline-complexity-grows-cover.png" class="pc-storage-background-image__img"/></picture></div></div></div></div><div class="pc-Grid"><div class="container-fluid pc-header-block__container-fluid"><div class="row pc-header-block__breadcrumbs"><div class="col"><div class="pc-header-breadcrumbs pc-header-breadcrumbs_theme_light" aria-label="You are here:"><div class="pc-header-breadcrumbs__item"><a href="../../../../index.html" class="pc-header-breadcrumbs__text">Blog</a></div><div class="pc-header-breadcrumbs__item"><a href="../../../../index.html%3Ftags=insights.html" class="pc-header-breadcrumbs__text">Insights</a></div></div></div></div><div class="row"><div class="col col-reset pc-header-block__content-wrapper"><div class="row"><div class="col pc-header-block__content pc-header-block__content_offset_default pc-header-block__content_theme_light pc-header-block__content_vertical-offset_l"><div class="col  col-lg-6 col-sm-12 col-md-8 col-12 pc-header-block__content-inner"><h1 class="pc-header-block__title" id="g-uniq-884150"><span>Best practices for data transformation as pipeline complexity grows</span></h1><div class="bc-post-info__container bc-post-info__container_theme_light"><div class="bc-post-info__item bc-post-info__item_size_s" data-qa="blog-header-meta-container-date">August 20, 2024</div><div class="bc-post-info__item bc-post-info__item_size_s" data-qa="blog-header-meta-container-reading-time"><span class="bc-post-info__icon"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="16" height="16" class="g-icon bc-post-info__icon-color" fill="currentColor" stroke="none" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 17" fill="currentColor" aria-hidden="true"><path fill-rule="evenodd" clip-rule="evenodd" d="M8 16.004a8 8 0 1 1 0-16 8 8 0 0 1 0 16Zm0-2a6 6 0 1 0 0-12 6 6 0 0 0 0 12Zm3.357-3.736a1 1 0 0 0-.342-1.372L9 7.688V5.004a1 1 0 0 0-2 0v3.25a1 1 0 0 0 .486.857l2.5 1.5a1 1 0 0 0 1.371-.343Z"></path></svg></svg></span>15 mins to read</div><div class="bc-post-info__item"><div class="bc-post-info__icon"><div class="g-popover gc-share-popover bc-post-info__share"><button class="gc-share-popover__container bc-post-info__switcher bc-post-info__switcher_theme_light" aria-expanded="false" aria-controls="g-uniq-884151" aria-describedby="g-uniq-884151"><div class="gc-share-popover__icon-container"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="16" height="16" class="g-icon gc-share-popover__icon bc-post-info__share-icon" fill="currentColor" stroke="none" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" fill="currentColor" aria-hidden="true"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.798 3.16a.5.5 0 0 0 .363.842H7V9a1 1 0 0 0 2 0V4.002h1.839a.5.5 0 0 0 .363-.844L8.363.156a.5.5 0 0 0-.726 0l-2.84 3.002.001.001ZM13 7a1 1 0 0 1 2 0v6.5a1.5 1.5 0 0 1-1.5 1.5h-11A1.5 1.5 0 0 1 1 13.5V7a1 1 0 0 1 2 0v6h10V7Z"></path></svg></svg></div><div class="gc-share-popover__title">Share</div></button></div></div></div></div></div></div></div></div></div></div></div></header></section><div class="pc-Grid"><div class="container-fluid "><div class="row pc-constructor-row"><div class="col"><div class="col col-reset pc-block-base pc-block-base_reset-paddings pc-block-base_indentTop_0 pc-block-base_indentBottom_0 pc-constructor-block pc-constructor-block_type_blog-layout-block"><section class="bc-wrapper bc-wrapper_padding-top_xs bc-wrapper_padding-bottom_xs"><div class="row bc-layout__row no-gutter"><div class="col  col-12 col-lg-8  order-3 order-lg-1 bc-layout__left-col"><div class="bc-layout__item"><div class="col col-reset pc-block-base pc-block-base_reset-paddings pc-block-base_indentTop_0 pc-block-base_indentBottom_0 pc-constructor-block pc-constructor-block_type_blog-yfm-block"><section class="bc-wrapper bc-wrapper_padding-top_xs bc-wrapper_padding-bottom_l"><div class="yfm yfm_blog yfm_reset_paddings yfm_no-list-reset"><p>Modern data platforms revolve around data transformation.</p>
<p>That is&nbsp;the entire point of&nbsp;data pipelines&ndash;<em>transforming raw data into actionable insights</em>. However, data transformation is&nbsp;also a&nbsp;gigantic process, ranging from simple cleaning and formatting to&nbsp;complex aggregations and machine learning models.</p>
<p>The correct transformation approach is&nbsp;crucial for efficiency, scalability, and data integrity. If&nbsp;you get this wrong, you risk creating a&nbsp;bottleneck that can slow down your entire data operation, lead to&nbsp;inconsistent or&nbsp;unreliable results, and make it&nbsp;difficult to&nbsp;adapt to&nbsp;changing business needs or&nbsp;scale your data infrastructure.</p>
<p>If&nbsp;you get it&nbsp;right, though, you can easily handle increasing data volumes and complexity, deliver consistent, high-quality insights across your team, and provide a&nbsp;solid foundation for advanced analytics like AI&nbsp;and machine learning.</p>
<p>Here, we&nbsp;want to&nbsp;explain how we&nbsp;think about data transformation best practices at&nbsp;DoubleCloud, concentrating on&nbsp;each level of&nbsp;transformation as&nbsp;your pipeline complexity grows.</p>
<h2 id="always-start-with-ingestion-level-transformations"><a href="../data-transformation-best-practices.html#always-start-with-ingestion-level-transformations" class="yfm-anchor" aria-hidden="true"><span class="visually-hidden">Always start with ingestion-level transformations</span></a>Always start with ingestion-level transformations</h2>
<p>Starting at&nbsp;the start may seem obvious, but teams can easily skip this step. They ingest vast amounts of&nbsp;data and then pump it&nbsp;down the pipeline without considering the problems this will cause. These problems can be&nbsp;segmented into two categories.</p>
<h3 id="data-quality"><a href="../data-transformation-best-practices.html#data-quality" class="yfm-anchor" aria-hidden="true"><span class="visually-hidden">Data quality</span></a>Data quality</h3>
<p>&ldquo;Garbage In, Garbage Out&rdquo; is&nbsp;a&nbsp;famous aphorism in&nbsp;data analytics that is&nbsp;starting to&nbsp;be&nbsp;forgotten. With the advent of&nbsp;AI&nbsp;and machine learning, teams can get into the habit of&nbsp;thinking that these models can easily work around or&nbsp;take care of&nbsp;any data quality issues.</p>
<p>But the refrain continues to&nbsp;be&nbsp;true. The better the quality of&nbsp;your data at&nbsp;the beginning of&nbsp;the pipeline, the better the quality of&nbsp;your insights at&nbsp;the&nbsp;end. Here are some first-line-of-defense transformations that can make it&nbsp;easier to&nbsp;work with your data downstream:</p>
<p><strong>1. Data type validation:</strong> Incoming data must adhere to&nbsp;expected data types, such as&nbsp;converting strings to&nbsp;appropriate numeric or&nbsp;date formats, parsing JSON or&nbsp;XML structures, or&nbsp;casting boolean values to&nbsp;a&nbsp;standardized format. This reduces the need for error handling and type conversions in&nbsp;the later stages of&nbsp;the pipeline.</p>
<p><strong>2. Null value handling:</strong> You can deal with null or&nbsp;missing values using <a href="https://scikit-learn.org/stable/modules/impute.html">imputation</a>, <a href="https://www.scaler.com/topics/data-science/categorical-missing-values/#:~:text=replace%20missing%20values.-,Flagging,-In%20some%20cases">flagging</a>, or&nbsp;removal. Null value handling is&nbsp;required to&nbsp;ensure that downstream analysis and pipelines don&rsquo;t break due to&nbsp;unexpected null values.</p>
<p><strong>3. Deduplication:</strong> Deduplication, removing duplicate records at&nbsp;the ingestion stage, improves data quality and reduces storage costs. Though you can brute-force this at&nbsp;scale, depending on&nbsp;your data characteristics and business rules, you must consider fuzzy matching techniques or&nbsp;libraries such as&nbsp;<a href="https://pypi.org/project/dedupe/1.6.5/">dedupe</a>.</p>
<p><strong>4. Trimming and cleaning:</strong> Remove leading/trailing whitespaces, control characters, or&nbsp;other unwanted artifacts from string fields. This seemingly simple step can prevent numerous issues downstream, such as&nbsp;unexpected behavior in&nbsp;string comparisons or&nbsp;joins.</p>
<p><strong>5. Schema validation:</strong> This ensures the incoming data structure matches the expected schema, handling any discrepancies or&nbsp;schema evolution. Robust <a href="https://python-jsonschema.readthedocs.io/en/latest/validate/">schema validation</a> can prevent data inconsistencies. You might want to&nbsp;implement versioning for your schemas to&nbsp;ensure your pipeline can adapt to&nbsp;evolving data structures without breaking.</p>
<p><strong>6. Referential integrity checks:</strong> Verify that foreign key relationships are maintained, especially when ingesting data from multiple sources. This maintains logical consistency of&nbsp;your data across different tables or&nbsp;datasets. Implement checks that validate the existence of&nbsp;referenced keys and handle scenarios where referential integrity might be&nbsp;temporarily broken due to&nbsp;the order of&nbsp;data ingestion (essential in&nbsp;<a href="https://neon.tech/blog/postgres-roles#:~:text=Onto%20the%20more%20significant%20issue%3A%20migrations.">migrations</a>).</p>
<p>All these are for a&nbsp;single reason: improve data quality now to&nbsp;reduce complexity later.</p>
<h3 id="data-security"><a href="../data-transformation-best-practices.html#data-security" class="yfm-anchor" aria-hidden="true"><span class="visually-hidden">Data security</span></a>Data security</h3>
<p>We&nbsp;left data security checks out of&nbsp;the list above because they require more discussion. You must deal with personal or&nbsp;sensitive data at&nbsp;the ingestion level. This is&nbsp;not just a&nbsp;best practice but often a&nbsp;legal requirement under regulations like GDPR, CCPA, or&nbsp;industry-specific standards like HIPAA.</p>
<p>Here are some key data security transformations to&nbsp;consider at&nbsp;the ingestion stage:</p>
<p><strong>1. Data masking:</strong> Apply masking techniques to&nbsp;sensitive fields such as&nbsp;personal identifiers, credit card numbers, or&nbsp;health information. This involves replacing sensitive data with fictitious but realistic data maintaining the data&rsquo;s format and consistency while protecting individual privacy.</p>
<p><strong>2. Encryption:</strong> Implement encryption for highly sensitive data fields. This could involve using robust encryption algorithms to&nbsp;protect data at&nbsp;rest and in&nbsp;transit. Consider using format-preserving encryption for fields where the encrypted data needs to&nbsp;maintain the original format.</p>
<p><strong>3. Tokenization:</strong> Replace sensitive data elements with non-sensitive equivalents or&nbsp;tokens. This is&nbsp;particularly useful for fields that must maintain uniqueness without exposing the original data.</p>
<p><strong>4. Data anonymization:</strong> For datasets used in&nbsp;analytics or&nbsp;machine learning, consider anonymization techniques that remove or&nbsp;alter personally identifiable information while preserving the data&rsquo;s analytical value.</p>
<h2 id="move-to-materialized-views-as-data-starts-to-scale"><a href="../data-transformation-best-practices.html#move-to-materialized-views-as-data-starts-to-scale" class="yfm-anchor" aria-hidden="true"><span class="visually-hidden">Move to&nbsp;materialized views as&nbsp;data starts to&nbsp;scale</span></a>Move to&nbsp;materialized views as&nbsp;data starts to&nbsp;scale</h2>
<p>As&nbsp;data volume grows and query complexity increases, simple transformations are still necessary but no&nbsp;longer sufficient.</p>
<p>At&nbsp;this point, data engineers start working with <a href="../../../../../docs/managed-clickhouse/glossary.html#materialized-view">materialized views</a>, which are pre-computed result sets stored for faster query performance.</p></div></section></div></div><div class="bc-layout__item"><div class="col col-reset pc-block-base pc-block-base_reset-paddings pc-block-base_indentTop_0 pc-block-base_indentBottom_0 pc-constructor-block pc-constructor-block_type_blog-media-block"><section class="bc-wrapper bc-wrapper_padding-top_xs bc-wrapper_padding-bottom_l bc-media__container"><div class="bc-media__border" data-qa="blog-media-content"><div class="pc-Media bc-media__content"><picture><source srcSet="../../../../../assets/blog/articles/2024/best-practices-for-data-transformation-as-pipeline-complexity-grows-img-1-2.png.webp" type="image/webp"/><img alt="" src="../../../../../assets/blog/articles/2024/best-practices-for-data-transformation-as-pipeline-complexity-grows-img-1-2.png" class="pc-media-component-image__item bc-media__image"/></picture></div></div><div class="bc-media__text-content"><div class="yfm yfm_blog yfm_blog_media yfm_reset_paddings"><p>(Source: <a href="https://clickhouse.com/docs/en/materialized-view">ClickHouse</a>)</p></div></div></section></div></div></div><div class="col  col-12 col-lg-3  offset-0 offset-lg-1  order-2 order-lg-2 bc-layout__right-col"><div class="bc-layout__item"><div class="col col-reset pc-block-base pc-block-base_reset-paddings pc-block-base_indentTop_0 pc-block-base_indentBottom_0 pc-constructor-block pc-constructor-block_type_blog-yfm-block"><section class="bc-wrapper bc-wrapper_padding-top_xs bc-wrapper_padding-bottom_l"><div class="yfm yfm_blog yfm_reset_paddings yfm_no-list-reset"><p>Quick navigation:</p>
<ul>
<li><a href="../data-transformation-best-practices.html#always-start-with-ingestion-level-transformations">Always start with ingestion-level transformations</a></li>
<li><a href="../data-transformation-best-practices.html#move-to-materialized-views-as-data-starts-to-scale">Move to&nbsp;materialized views as&nbsp;data starts to&nbsp;scale</a></li>
<li><a href="../data-transformation-best-practices.html#choose-dbt-when-quality-and-complexity-becomes-the-focus">Choose dbt when quality and complexity becomes the focus</a></li>
<li><a href="../data-transformation-best-practices.html#use-airflow-for-the-most-complex-workflows">Use airflow for the most complex workflows</a></li>
<li><a href="../data-transformation-best-practices.html#5-tips-for-every-data-transformation">5 tips for every data transformation</a></li>
<li><a href="../data-transformation-best-practices.html#doubleclouds-approach-to-data-transformation">DoubleCloud&rsquo;s approach to&nbsp;data transformation</a></li>
<li><a href="../data-transformation-best-practices.html#elevate-your-data-transformation-strategy-with-doublecloud">Elevate your data transformation strategy with DoubleCloud</a></li>
</ul></div></section></div></div></div></div></section></div><div class="col col-reset pc-block-base pc-block-base_reset-paddings pc-block-base_indentTop_0 pc-block-base_indentBottom_0 pc-constructor-block pc-constructor-block_type_blog-layout-block"><section class="bc-wrapper bc-wrapper_padding-top_xs bc-wrapper_padding-bottom_xs"><div class="row bc-layout__row no-gutter"><div class="col  col-12 col-lg-8  order-3 order-lg-1 bc-layout__left-col"><div class="bc-layout__item"><div class="col col-reset pc-block-base pc-block-base_reset-paddings pc-block-base_indentTop_0 pc-block-base_indentBottom_0 pc-constructor-block pc-constructor-block_type_blog-yfm-block"><section class="bc-wrapper bc-wrapper_padding-top_xs bc-wrapper_padding-bottom_l"><div class="yfm yfm_blog yfm_reset_paddings yfm_no-list-reset"><p>Imagine running an&nbsp;e-commerce platform that generates billions of&nbsp;clickstream events daily. You&rsquo;ll frequently need to&nbsp;analyze user engagement and sales performance. Instead of&nbsp;running complex queries on&nbsp;this large raw dataset every time, you can create materialized views to&nbsp;pre-compute common aggregations:</p>

    <div class="yfm-clipboard">
    <pre><code class="hljs sql"><span class="hljs-keyword">CREATE</span> MATERIALIZED <span class="hljs-keyword">VIEW</span> daily_user_activity
ENGINE <span class="hljs-operator">=</span> SummingMergeTree()
<span class="hljs-keyword">ORDER</span> <span class="hljs-keyword">BY</span> (<span class="hljs-type">date</span>, user_id)
POPULATE
<span class="hljs-keyword">AS</span> <span class="hljs-keyword">SELECT</span>
    toDate(event_time) <span class="hljs-keyword">AS</span> <span class="hljs-type">date</span>,
    user_id,
    countIf(event_type <span class="hljs-operator">=</span> <span class="hljs-string">'pageview'</span>) <span class="hljs-keyword">AS</span> pageviews,
    countIf(event_type <span class="hljs-operator">=</span> <span class="hljs-string">'click'</span>) <span class="hljs-keyword">AS</span> clicks,
    countIf(event_type <span class="hljs-operator">=</span> <span class="hljs-string">'purchase'</span>) <span class="hljs-keyword">AS</span> purchases,
    <span class="hljs-built_in">sum</span>(purchase_amount) <span class="hljs-keyword">AS</span> total_spent
<span class="hljs-keyword">FROM</span> raw_events
<span class="hljs-keyword">GROUP</span> <span class="hljs-keyword">BY</span> <span class="hljs-type">date</span>, user_id;

</code></pre>

    <svg width="16" height="16" viewBox="0 0 24 24" class="yfm-clipboard-button" data-animation="3">
        <path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path>
        <path stroke="currentColor" fill="transparent" stroke-width="1.5" d="M9.5 13l3 3l5 -5" visibility="hidden">
            <animate id="visibileAnimation-3" attributeName="visibility" from="hidden" to="visible" dur="0.2s" fill="freeze" begin></animate>
            <animate id="hideAnimation-3" attributeName="visibility" from="visible" to="hidden" dur="1s" begin="visibileAnimation-3.end+1" fill="freeze"></animate>
        </path>
    </svg>
    </div>
<p>The view stores only the aggregated data, which updates automatically as&nbsp;source data changes, significantly reducing the amount of&nbsp;data scanned for common queries.</p>
<p>The golden rule for materialized views is&nbsp;to&nbsp;understand your query performance. Materialized views perform best in&nbsp;situations with frequent, expensive queries. In&nbsp;particular, you want to&nbsp;precompute expensive joins and aggregations. To&nbsp;do&nbsp;that, you can use <a href="https://www.postgresql.org/docs/current/using-explain.html#USING-EXPLAIN-ANALYZE">EXPLAIN ANALYZE</a> to&nbsp;understand query costs:</p>
<ol>
<li>Analyze execution time: Anything taking several seconds or&nbsp;more could be&nbsp;considered expensive, especially if&nbsp;run frequently.</li>
<li>Check for Seq Scans on&nbsp;large tables: These are often indicators of&nbsp;expensive operations.</li>
<li>Compare cost estimates: Higher costs suggest more expensive queries.</li>
<li>Consider the frequency of&nbsp;the query: Even a&nbsp;moderately expensive query can be&nbsp;problematic if&nbsp;run very often.</li>
</ol>
<p>Materialized views are a&nbsp;crucial layer in&nbsp;your data transformation pipeline, enabling real-time analytics on&nbsp;massive datasets while maintaining the flexibility to&nbsp;access raw data when needed.</p>
<h2 id="choose-dbt-when-quality-and-complexity-becomes-the-focus"><a href="../data-transformation-best-practices.html#choose-dbt-when-quality-and-complexity-becomes-the-focus" class="yfm-anchor" aria-hidden="true"><span class="visually-hidden">Choose dbt when quality and complexity becomes the focus</span></a>Choose dbt when quality and complexity becomes the focus</h2>
<p>At&nbsp;some point, manual transformation isn&rsquo;t going to&nbsp;cut&nbsp;it. The rule of&nbsp;thumb for when you need to&nbsp;bring in&nbsp;automation to&nbsp;your transformations is&nbsp;when:</p>
<ul>
<li>
<p>You have more than five materialized views or</p>
</li>
<li>
<p>when data transformations become too complex to&nbsp;manage manually.</p>
</li>
</ul>
<p>Then, data quality tools such as&nbsp;<a href="https://www.getdbt.com/">dbt</a> are needed. dbt is&nbsp;an&nbsp;open-source tool that enables data analysts and engineers to&nbsp;transform data in&nbsp;their warehouses more effectively. Engineers primarily use it&nbsp;for more complex transformations and ensuring data quality.</p>
<p>dbt brings a&nbsp;more developer-like workflow to&nbsp;data transformation. Instead of&nbsp;individual SQL queries that need to&nbsp;be&nbsp;run manually for part of&nbsp;an&nbsp;ad-hoc transformation process, dbt brings structured queries and version control into the&nbsp;mix. Engineers can use dbt to&nbsp;create modular, reusable SQL code that defines how raw data should be&nbsp;transformed into analytics-ready models.</p>
<p>In&nbsp;dbt, you break down complex transformations into smaller, more manageable pieces. This modular approach makes understanding, maintaining, and reusing transformation logic easier. Then, dbt includes built-in functionality for testing your data transformations. You can define tests to&nbsp;ensure data quality, such as&nbsp;checking for null values, unique constraints, or&nbsp;custom business logic.</p>
<p>These are the fundamentals we&nbsp;mentioned for the ingestion phase. Even though you might only start using dbt once you have more complexity in&nbsp;your pipelines, you should use it&nbsp;throughout your pipeline once you hit that level of&nbsp;complexity. <a href="../../../../../docs/managed-clickhouse/integrations/transform-data-in-clickhouse-with-dbt.html">dbt integrates with DoubleCloud&rsquo;s Managed ClickHouse cluster</a> to&nbsp;do&nbsp;precisely that, allowing you to&nbsp;build a&nbsp;seamless workflow from data ingestion to&nbsp;transformation:</p></div></section></div></div><div class="bc-layout__item"><div class="col col-reset pc-block-base pc-block-base_reset-paddings pc-block-base_indentTop_0 pc-block-base_indentBottom_0 pc-constructor-block pc-constructor-block_type_blog-media-block"><section class="bc-wrapper bc-wrapper_padding-top_xs bc-wrapper_padding-bottom_l bc-media__container"><div class="bc-media__border" data-qa="blog-media-content"><div class="pc-Media bc-media__content"><picture><source srcSet="../../../../../assets/blog/articles/2024/best-practices-for-data-transformation-as-pipeline-complexity-grows-img-2.png.webp" type="image/webp"/><img alt="" src="../../../../../assets/blog/articles/2024/best-practices-for-data-transformation-as-pipeline-complexity-grows-img-2.png" class="pc-media-component-image__item bc-media__image"/></picture></div></div></section></div></div></div><div class="col  col-12 col-lg-3  offset-0 offset-lg-1  order-2 order-lg-2 bc-layout__right-col"></div></div></section></div><div class="col col-reset pc-block-base pc-block-base_reset-paddings pc-block-base_indentTop_0 pc-block-base_indentBottom_0 pc-constructor-block pc-constructor-block_type_blog-layout-block"><section class="bc-wrapper bc-wrapper_padding-top_xs bc-wrapper_padding-bottom_xs"><div class="row bc-layout__row no-gutter"><div class="col  col-12 col-lg-8  order-3 order-lg-1 bc-layout__left-col"><div class="bc-layout__item"><div class="col col-reset pc-block-base pc-block-base_reset-paddings pc-block-base_indentTop_0 pc-block-base_indentBottom_0 pc-constructor-block pc-constructor-block_type_blog-yfm-block"><section class="bc-wrapper bc-wrapper_padding-top_xs bc-wrapper_padding-bottom_l"><div class="yfm yfm_blog yfm_reset_paddings yfm_no-list-reset"><p>To&nbsp;get started with dbt in&nbsp;your data transformation workflow:</p>
<p><strong>1. Define your models:</strong> Create SQL files that represent your data models, from staging tables that closely represent your raw data to&nbsp;intermediate and final models that power your analytics.</p>
<p><strong>2. Set up&nbsp;tests:</strong> Write simple YAML configurations to&nbsp;test your data for nulls, uniqueness, accepted values, and custom business logic.</p>
<p><strong>3. Document your models:</strong> Use YAML files and markdown to&nbsp;document your models, making it&nbsp;easier for others to&nbsp;understand and use your transformed data.</p>
<p><strong>4. Create a&nbsp;project structure:</strong> Organize your dbt project with a&nbsp;clear folder structure, separating staging models, intermediate models, and final models.</p>
<p><strong>5. Implement CI/CD:</strong> Set up&nbsp;continuous integration to&nbsp;run your dbt models and tests automatically when changes are pushed to&nbsp;your repository.</p>
<p>The transition to&nbsp;using dbt should be&nbsp;gradual. Start with a&nbsp;few key models, get comfortable with the workflow, and expand its use across your data transformation process. By&nbsp;adopting dbt, you&rsquo;re not just automating your transformations but bringing software engineering best practices to&nbsp;your data work. This results in&nbsp;more reliable, maintainable, and scalable data transformations, which is&nbsp;crucial as&nbsp;your data pipeline grows in&nbsp;complexity.</p>
<h2 id="use-airflow-for-the-most-complex-workflows"><a href="../data-transformation-best-practices.html#use-airflow-for-the-most-complex-workflows" class="yfm-anchor" aria-hidden="true"><span class="visually-hidden">Use Airflow for the most complex workflows</span></a>Use Airflow for the most complex workflows</h2>
<p>With dbt, you can build workflows, but dbt itself doesn&rsquo;t handle scheduling. For that, you need an&nbsp;orchestration tool, such as&nbsp;<a href="https://airflow.apache.org/">Airflow</a>.</p>
<p>Airflow will allow you to&nbsp;manage dependencies between tasks and schedule complex jobs, give you the flexibility to&nbsp;handle diverse workflows, provide scalability for large-scale data processing, and provide robust error handling and monitoring. Airflow&rsquo;s &ldquo;secret sauce&rdquo; is&nbsp;the <a href="../../../../../docs/managed-airflow/concepts/dag.html">Directed Acyclic Graph</a>, or&nbsp;DAG. Airflow leverages DAGs to&nbsp;manage complex workflows and scheduling.</p>
<p>In&nbsp;Airflow, you define your workflow as&nbsp;a&nbsp;DAG using Python code. This lets you programmatically create a&nbsp;structure representing your entire data pipeline or&nbsp;process. Within a&nbsp;DAG, you define individual tasks and their dependencies. Tasks can be&nbsp;anything from running a&nbsp;SQL query to&nbsp;executing a&nbsp;Python function or&nbsp;interacting with an&nbsp;external system. Dependencies determine the order in&nbsp;which tasks should run.</p></div></section></div></div><div class="bc-layout__item"><div class="col col-reset pc-block-base pc-block-base_reset-paddings pc-block-base_indentTop_0 pc-block-base_indentBottom_0 pc-constructor-block pc-constructor-block_type_blog-media-block"><section class="bc-wrapper bc-wrapper_padding-top_xs bc-wrapper_padding-bottom_l bc-media__container"><div class="bc-media__border" data-qa="blog-media-content"><div class="pc-Media bc-media__content"><picture><source srcSet="../../../../../assets/blog/articles/2024/best-practices-for-data-transformation-as-pipeline-complexity-grows-img-3.png.webp" type="image/webp"/><img alt="" src="../../../../../assets/blog/articles/2024/best-practices-for-data-transformation-as-pipeline-complexity-grows-img-3.png" class="pc-media-component-image__item bc-media__image"/></picture></div></div><div class="bc-media__text-content"><div class="yfm yfm_blog yfm_blog_media yfm_reset_paddings"><p>(Source: <a href="https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html">Apache Airflow</a>)</p></div></div></section></div></div></div><div class="col  col-12 col-lg-3  offset-0 offset-lg-1  order-2 order-lg-2 bc-layout__right-col"></div></div></section></div><div class="col col-reset pc-block-base pc-block-base_reset-paddings pc-block-base_indentTop_0 pc-block-base_indentBottom_0 pc-constructor-block pc-constructor-block_type_blog-layout-block"><section class="bc-wrapper bc-wrapper_padding-top_xs bc-wrapper_padding-bottom_xs"><div class="row bc-layout__row no-gutter"><div class="col  col-12 col-lg-8  order-3 order-lg-1 bc-layout__left-col"><div class="bc-layout__item"><div class="col col-reset pc-block-base pc-block-base_reset-paddings pc-block-base_indentTop_0 pc-block-base_indentBottom_0 pc-constructor-block pc-constructor-block_type_blog-yfm-block"><section class="bc-wrapper bc-wrapper_padding-top_xs bc-wrapper_padding-bottom_l"><div class="yfm yfm_blog yfm_reset_paddings yfm_no-list-reset"><p>Each DAG can have a&nbsp;schedule associated with&nbsp;it. This could be&nbsp;as&nbsp;simple as&nbsp;&ldquo;run daily at&nbsp;midnight&rdquo; or&nbsp;as&nbsp;complex as&nbsp;&ldquo;run every 15&nbsp;minutes on&nbsp;weekdays&rdquo;. Airflow&rsquo;s scheduler uses these definitions to&nbsp;determine when to&nbsp;trigger DAG runs. When it&rsquo;s time for a&nbsp;DAG to&nbsp;run, Airflow&rsquo;s executor starts processing tasks. It&nbsp;begins with tasks with no&nbsp;dependencies and moves through the graph as&nbsp;tasks are completed, always respecting the defined dependencies.</p>
<p>Because the workflow is&nbsp;defined as&nbsp;a&nbsp;graph, Airflow can quickly identify which tasks can run in&nbsp;parallel. This allows for efficient execution of&nbsp;complex workflows. Airflow keeps track of&nbsp;the state of&nbsp;each task (running, success, failed, etc.). If&nbsp;a&nbsp;task fails, Airflow can automatically retry it&nbsp;based on&nbsp;your configuration.</p>
<p>This approach allows Airflow to&nbsp;handle everything from simple linear workflows to&nbsp;complex branching processes with multiple dependencies and parallel execution paths. It&rsquo;s ideal for the most complex data engineering scenarios where you might need to&nbsp;orchestrate ETL processes, run machine learning pipelines, or&nbsp;manage complex data transformation workflows.</p>
<h2 id="5-tips-for-every-data-transformation"><a href="../data-transformation-best-practices.html#5-tips-for-every-data-transformation" class="yfm-anchor" aria-hidden="true"><span class="visually-hidden">5 tips for every data transformation</span></a>5 tips for every data transformation</h2>
<p>While we&rsquo;ve covered specific approaches for different stages of&nbsp;data pipeline complexity, some overarching principles apply universally.</p>
<h3 id="1-simplicity-and-maintainability-as-key-goals"><a href="../data-transformation-best-practices.html#1-simplicity-and-maintainability-as-key-goals" class="yfm-anchor" aria-hidden="true"><span class="visually-hidden">1. Simplicity and maintainability as&nbsp;key goals</span></a>1. Simplicity and maintainability as&nbsp;key goals</h3>
<p>Always strive for simplicity in&nbsp;your transformations. Complex transformations might seem impressive, but they often lead to&nbsp;headaches down the road. Here&rsquo;s why simplicity matters:</p>
<ul>
<li>
<p><strong>Easier debugging:</strong> When issues arise (and they will), more straightforward transformations are quicker to&nbsp;troubleshoot.</p>
</li>
<li>
<p><strong>Improved maintainability:</strong> As&nbsp;your team evolves, simpler code is&nbsp;easier for new team members to&nbsp;understand and modify.</p>
</li>
<li>
<p><strong>Reduced error risk:</strong> Complex transformations introduce more points of&nbsp;failure. Keeping things simple minimizes this risk.</p>
</li>
</ul>
<p>Implement this by&nbsp;breaking down complex transformations into smaller, manageable steps. Use clear naming conventions and add comments to&nbsp;explain the logic behind each transformation. Remember, the goal is&nbsp;not just to&nbsp;transform data but to&nbsp;do&nbsp;so&nbsp;in&nbsp;a&nbsp;sustainable and scalable way as&nbsp;your data needs grow.</p>
<h3 id="2-denormalization-in-data-warehousing"><a href="../data-transformation-best-practices.html#2-denormalization-in-data-warehousing" class="yfm-anchor" aria-hidden="true"><span class="visually-hidden">2. Denormalization in&nbsp;data warehousing</span></a>2. Denormalization in&nbsp;data warehousing</h3>
<p>In&nbsp;traditional database design, normalization is&nbsp;king. But denormalization often takes the crown in&nbsp;the world of&nbsp;data warehousing and analytics. Here&rsquo;s why:</p>
<ul>
<li>
<p><strong>Faster queries:</strong> Denormalized data requires fewer joins, significantly speeding up&nbsp;query performance.</p>
</li>
<li>
<p><strong>Simpler joins:</strong> When joins are necessary, they&rsquo;re typically simpler in&nbsp;a&nbsp;denormalized model.</p>
</li>
<li>
<p><strong>Improved read performance:</strong> Analytics workloads are often read-heavy, and denormalization optimizes for reads.</p>
</li>
</ul>
<p>Denormalization isn&rsquo;t without its trade-offs. You&rsquo;re storing redundant data, which increases storage needs, and updating denormalized data requires careful management to&nbsp;maintain consistency.</p>
<h3 id="3-performance-considerations-for-real-time-analytics"><a href="../data-transformation-best-practices.html#3-performance-considerations-for-real-time-analytics" class="yfm-anchor" aria-hidden="true"><span class="visually-hidden">3. Performance considerations for real-time analytics</span></a>3. Performance considerations for real-time analytics</h3>
<p>Optimizing your transformations for low-latency queries becomes crucial when building for real-time insights. As&nbsp;discussed earlier, materialized views can significantly speed up&nbsp;common queries, but that isn&rsquo;t the only performance optimization you can make.</p>
<p>Firstly, you can consider your own data transformation strategy. Do&nbsp;you need to&nbsp;reprocess all data? Or&nbsp;can you focus on&nbsp;transforming only the new or&nbsp;changed data?</p>
<p>Second, can you take advantage of&nbsp;tooling for better optimization? This might be&nbsp;in-memory stores for the most time-sensitive operations or&nbsp;properly partitioned and indexed data to&nbsp;improve query performance.</p>
<p>Remember, real-time doesn&rsquo;t always mean instantaneous. Understand your actual latency requirements and optimize accordingly. Sometimes, &ldquo;near real-time&rdquo; is&nbsp;sufficient and much easier to&nbsp;achieve.</p>
<h3 id="4-security-and-compliance-throughout-the-transformation-process"><a href="../data-transformation-best-practices.html#4-security-and-compliance-throughout-the-transformation-process" class="yfm-anchor" aria-hidden="true"><span class="visually-hidden">4. Security and compliance throughout the transformation process</span></a>4. Security and compliance throughout the transformation process</h3>
<p>Data security isn&rsquo;t just an&nbsp;ingestion-level concern. It&nbsp;needs to&nbsp;be&nbsp;baked into every stage of&nbsp;your data pipeline. Here&rsquo;s how:</p>
<ul>
<li>
<p><strong>Data encryption:</strong> Ensure data is&nbsp;encrypted at&nbsp;rest and in&nbsp;transit, even during transformation processes.</p>
</li>
<li>
<p><strong>Access controls:</strong> Implement fine-grained access controls. Not everyone needs access to&nbsp;all transformed data.</p>
</li>
<li>
<p><strong>Audit trails:</strong> Maintain detailed logs of&nbsp;all data transformations for compliance and troubleshooting.</p>
</li>
<li>
<p><strong>Data lineage:</strong> Track the origin and transformations of&nbsp;each data element. This is&nbsp;crucial for both compliance and data governance.</p>
</li>
</ul>
<h3 id="5-consider-the-end-use"><a href="../data-transformation-best-practices.html#5-consider-the-end-use" class="yfm-anchor" aria-hidden="true"><span class="visually-hidden">5. Consider the end-use</span></a>5. Consider the end-use</h3>
<p>Finally, always consider the end-use of&nbsp;your data. Your transformation strategy should be&nbsp;tailored to&nbsp;how the data will ultimately be&nbsp;consumed.</p>
<p>Do&nbsp;you have a&nbsp;data contract or&nbsp;SLA with downstream users that decides latency or&nbsp;freshness? Establish clear data contracts with downstream users. These should define expectations for data quality, freshness, and availability. For instance, a&nbsp;real-time dashboard might require 99.9% uptime and data no&nbsp;older than 5&nbsp;minutes, while a&nbsp;monthly report might tolerate 24&nbsp;hours of&nbsp;lag.</p>
<p>Remember, the data team&rsquo;s job is&nbsp;to&nbsp;ship insights, not data. Align your transformation strategy with key business metrics and decision-making processes. Understand which transformations directly support critical business KPIs and prioritize these. This approach transforms data engineering from a&nbsp;support function into a&nbsp;strategic driver of&nbsp;business value, directly tying your technical decisions to&nbsp;business outcomes and user satisfaction.</p>
<h2 id="doubleclouds-approach-to-data-transformation"><a href="../data-transformation-best-practices.html#doubleclouds-approach-to-data-transformation" class="yfm-anchor" aria-hidden="true"><span class="visually-hidden">DoubleCloud&rsquo;s approach to&nbsp;data transformation</span></a>DoubleCloud&rsquo;s approach to&nbsp;data transformation</h2>
<p>Effective data transformation is&nbsp;the cornerstone of&nbsp;any successful data strategy. At&nbsp;DoubleCloud, our approach is&nbsp;designed around transformation to&nbsp;meet the needs of&nbsp;modern data teams, from simple ETL processes to&nbsp;complex, real-time analytics pipelines.</p>
<p>In&nbsp;particular, <a href="../../../../../services/doublecloud-transfer.html">DoubleCloud Data Transfer</a> is&nbsp;designed to&nbsp;be&nbsp;a&nbsp;simple data integration solution that excels at&nbsp;extracting, loading, and transforming data. It&nbsp;is&nbsp;nine times faster than Airbyte and ideal for working with ClickHouse.</p></div></section></div></div><div class="bc-layout__item"><div class="col col-reset pc-block-base pc-block-base_reset-paddings pc-block-base_indentTop_0 pc-block-base_indentBottom_0 pc-constructor-block pc-constructor-block_type_blog-media-block"><section class="bc-wrapper bc-wrapper_padding-top_xs bc-wrapper_padding-bottom_l bc-media__container"><div class="bc-media__border" data-qa="blog-media-content"><div class="pc-Media bc-media__content"><picture><source srcSet="../../../../../assets/blog/articles/2024/best-practices-for-data-transformation-as-pipeline-complexity-grows-img-4.png.webp" type="image/webp"/><img alt="" src="../../../../../assets/blog/articles/2024/best-practices-for-data-transformation-as-pipeline-complexity-grows-img-4.png" class="pc-media-component-image__item bc-media__image"/></picture></div></div></section></div></div></div><div class="col  col-12 col-lg-3  offset-0 offset-lg-1  order-2 order-lg-2 bc-layout__right-col"></div></div></section></div><div class="col col-reset pc-block-base pc-block-base_reset-paddings pc-block-base_indentTop_0 pc-block-base_indentBottom_0 pc-constructor-block pc-constructor-block_type_blog-layout-block"><section class="bc-wrapper bc-wrapper_padding-top_xs bc-wrapper_padding-bottom_xs"><div class="row bc-layout__row no-gutter"><div class="col  col-12 col-lg-8  order-3 order-lg-1 bc-layout__left-col"><div class="bc-layout__item"><div class="col col-reset pc-block-base pc-block-base_reset-paddings pc-block-base_indentTop_0 pc-block-base_indentBottom_0 pc-constructor-block pc-constructor-block_type_blog-yfm-block"><section class="bc-wrapper bc-wrapper_padding-top_xs bc-wrapper_padding-bottom_l"><div class="yfm yfm_blog yfm_reset_paddings yfm_no-list-reset"><p>With high-speed data ingestion into ClickHouse, Data Transfer can reach up&nbsp;to&nbsp;2&nbsp;GB/s, making it&nbsp;an&nbsp;ideal choice for organizations looking to&nbsp;build customer-facing or&nbsp;real-time analytics solutions.</p>
<p>This lets you move data in&nbsp;any arrangement, giving you complete control over your data flow. This flexibility is&nbsp;crucial for complex data transformation pipelines. DoubleClod Data Transfer natively integrates with dbt, allowing users to&nbsp;quickly transform, clean, and summarize data for analysis. This integration enhances data modeling capabilities and improves team collaboration to&nbsp;build out the data products your organization needs.</p></div></section></div></div><div class="bc-layout__item"><div class="col col-reset pc-block-base pc-block-base_reset-paddings pc-block-base_indentTop_0 pc-block-base_indentBottom_0 pc-constructor-block pc-constructor-block_type_blog-media-block"><section class="bc-wrapper bc-wrapper_padding-top_xs bc-wrapper_padding-bottom_l bc-media__container"><div class="bc-media__border" data-qa="blog-media-content"><div class="pc-Media bc-media__content"><picture><source srcSet="../../../../../assets/blog/articles/2024/best-practices-for-data-transformation-as-pipeline-complexity-grows-img-5.png.webp" type="image/webp"/><img alt="" src="../../../../../assets/blog/articles/2024/best-practices-for-data-transformation-as-pipeline-complexity-grows-img-5.png" class="pc-media-component-image__item bc-media__image"/></picture></div></div></section></div></div></div><div class="col  col-12 col-lg-3  offset-0 offset-lg-1  order-2 order-lg-2 bc-layout__right-col"></div></div></section></div><div class="col col-reset pc-block-base pc-block-base_reset-paddings pc-block-base_indentTop_0 pc-block-base_indentBottom_0 pc-constructor-block pc-constructor-block_type_blog-layout-block"><section class="bc-wrapper bc-wrapper_padding-top_xs bc-wrapper_padding-bottom_xs"><div class="row bc-layout__row no-gutter"><div class="col  col-12 col-lg-8  order-3 order-lg-1 bc-layout__left-col"><div class="bc-layout__item"><div class="col col-reset pc-block-base pc-block-base_reset-paddings pc-block-base_indentTop_0 pc-block-base_indentBottom_0 pc-constructor-block pc-constructor-block_type_blog-yfm-block"><section class="bc-wrapper bc-wrapper_padding-top_xs bc-wrapper_padding-bottom_l"><div class="yfm yfm_blog yfm_reset_paddings yfm_no-list-reset"><p>DoubleCloud&rsquo;s approach to&nbsp;data transformation aims to&nbsp;provide a&nbsp;comprehensive, efficient, and cost-effective solution for modern data teams. DoubleCloud Data Transfer is&nbsp;designed to&nbsp;meet data transformation needs while optimizing for performance and cost, whether you&rsquo;re dealing with simple ETL processes or&nbsp;complex, real-time analytics pipelines.</p>
<h2 id="elevate-your-data-transformation-strategy-with-doublecloud"><a href="../data-transformation-best-practices.html#elevate-your-data-transformation-strategy-with-doublecloud" class="yfm-anchor" aria-hidden="true"><span class="visually-hidden">Elevate your data transformation strategy with DoubleCloud</span></a>Elevate your data transformation strategy with DoubleCloud</h2>
<p>Data transformation is&nbsp;a&nbsp;strategic imperative for any organization looking to&nbsp;harness the full power of&nbsp;its data. From ingestion-level transformations to&nbsp;complex orchestration with tools like Airflow, each stage of&nbsp;your data pipeline requires careful consideration and planning.</p>
<p>The best practices form the backbone of&nbsp;a&nbsp;robust data transformation strategy. By&nbsp;implementing these practices, your data team can produce the insights the rest of&nbsp;your organization requires. Building your data solutions on&nbsp;DoubleCloud infrastructure gives you the ability to:</p>
<ul>
<li>
<p>Scale your data pipelines to&nbsp;handle growing volumes and complexity</p>
</li>
<li>
<p>Optimize performance with ClickHouse-native tools and high-speed data ingestion</p>
</li>
<li>
<p>Maintain data quality and security throughout the transformation process</p>
</li>
<li>
<p>Streamline your workflow with built-in dbt integration for advanced transformations</p>
</li>
</ul>
<p>This is&nbsp;all necessary for robust data transformation. You can get <a href="https://auth.double.cloud/s/signup?_gl=1*19qneeh*_ga*NzU0OTMyNzI2LjE3MTk0MTg0OTE.*_ga_3G0X0VK41E*MTcyMzUxODEyMy4xNC4xLjE3MjM1MTg5NzcuNjAuMC4zODkxNTU0MDA.*_gcl_au*ODAxMjQyMTg2LjE3MTk0MTg0OTI.">started right away</a> with DoubleCloud to&nbsp;move your data, or&nbsp;you can <a href="../../../../../index.html#contact-us-form">reach out to&nbsp;the DoubleCloud team</a> to&nbsp;tell us&nbsp;more about your data needs.</p></div></section></div></div></div><div class="col  col-12 col-lg-3  offset-0 offset-lg-1  order-2 order-lg-2 bc-layout__right-col"></div></div></section></div><div class="col col-reset pc-block-base pc-block-base_indentTop_l pc-block-base_indentBottom_l pc-constructor-block pc-constructor-block_type_content-layout-block"><div class="pc-content-layout-block pc-content-layout-block_size_l pc-content-layout-block_theme_default pc-content-layout-block_background"><div class="col  col-12 col-md-8 col-reset pc-content pc-content_size_l pc-content_centered pc-content_theme_default pc-content-layout-block__content"><div class="pc-title pc-content__title" id="g-uniq-884153"><div class="col  col-12 col-reset"><h2 class="pc-title-item pc-title-item_size_m pc-title-item_reset-margin" data-qa="undefined-header"><span class="pc-title-item__text">Get started with DoubleCloud</span></h2></div></div><div class="pc-buttons pc-buttons_size_l pc-content__buttons pc-content__buttons_size_l"><a aria-describedby="g-uniq-884153" class="g-button g-button_view_action g-button_size_xl g-button_pin_round-round pc-button-block pc-button-block_size_xl pc-button-block_theme_accent pc-buttons__button" href="https://auth.double.cloud/s/signup" aria-disabled="false"><span class="g-button__text"><span class="pc-button-block__content"><span class="pc-button-block__text">Start free trial</span></span></span></a><a aria-describedby="g-uniq-884153" class="g-button g-button_view_outlined g-button_size_xl g-button_pin_round-round pc-button-block pc-button-block_size_xl pc-button-block_theme_pseudo pc-buttons__button" href="../data-transformation-best-practices.html#contact-us-form" aria-disabled="false"><span class="g-button__text"><span class="pc-button-block__content"><span class="pc-button-block__text">Contact us</span></span></span></a></div></div><div class="pc-content-layout-block__background"><div class="pc-storage-background-image pc-content-layout-block__background-item" style="background-color:#CA1551" data-qa="background-image"><picture data-qa="background-image-image"><source srcSet="../../../../../assets/doublecloud/doublecloud-cover-6.png.webp" type="image/webp" data-qa="background-image-image-desktop-source-compressed"/><img alt="" src="../../../../../assets/doublecloud/doublecloud-cover-6.png" class="pc-storage-background-image__img" style="background-color:#CA1551"/></picture></div></div></div></div><div class="col col-reset pc-block-base pc-block-base_reset-paddings pc-block-base_indentTop_0 pc-block-base_indentBottom_0 pc-constructor-block pc-constructor-block_type_blog-suggest-block"><section class="bc-wrapper bc-wrapper_padding-top_l bc-wrapper_padding-bottom_l"><div class="pc-SliderBlock"><div class="pc-title pc-SliderBlock__header pc-SliderBlock__header_no-description"><div class="col  col-12 col-sm-8 col-reset"><h2 class="pc-title-item pc-title-item_size_m pc-title-item_reset-margin" data-qa="undefined-header"><span class="pc-title-item__text">See also</span></h2></div></div><div class="pc-SliderBlock__animate-slides"><span style="font-size:0"></span><div><div class="slick-slider pc-slick-origin slick-initialized"><div class="slick-list"><div class="slick-track" style="width:100%;left:0%"><div data-index="0" class="slick-slide slick-active slick-current" tabindex="-1" aria-hidden="false" style="outline:none;width:33.333333333333336%"><div><div class="link" data-link-type="router"><a draggable="false" aria-labelledby="g-uniq-884154" aria-describedby="g-uniq-884156 g-uniq-884158" class="g-link g-link_view_normal pc-card-base-block pc-card-base-block_border_shadow bc-post-card__card" href="../../../2023/01/why-etl-pipelines-are-essential-for-businesses.html"><div class="pc-storage-background-image pc-card-base-block__header bc-post-card__header" data-qa="background-image"><picture data-qa="background-image-image"><source srcSet="../../../../../assets/blog/articles/why-etl-small-cover.png.webp" type="image/webp" data-qa="background-image-image-desktop-source-compressed"/><img alt="" src="../../../../../assets/blog/articles/why-etl-small-cover.png" class="pc-storage-background-image__img"/></picture><div class="pc-storage-background-image__container"><div class="pc-card-base-block__header-content"><div class="bc-post-card__image-container" data-qa="blog-suggest-header"></div></div></div></div><div class="pc-card-base-block__body"><div class="pc-card-base-block__content"><h3 class="bc-post-card__title bc-post-card__title_size_s"><span><span id="g-uniq-884154">Why ETL pipelines are essential for businesses</span></span></h3></div><div class="pc-card-base-block__footer"><div class="bc-post-info__container"><div class="bc-post-info__suggest-container"><div class="bc-post-info__item bc-post-info__item_size_s" id="g-uniq-884156">January 13, 2023</div><div class="bc-post-info__item bc-post-info__item_size_s" id="g-uniq-884158"><span class="bc-post-info__icon"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="16" height="16" class="g-icon bc-post-info__icon-color" fill="currentColor" stroke="none" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 17" fill="currentColor" aria-hidden="true"><path fill-rule="evenodd" clip-rule="evenodd" d="M8 16.004a8 8 0 1 1 0-16 8 8 0 0 1 0 16Zm0-2a6 6 0 1 0 0-12 6 6 0 0 0 0 12Zm3.357-3.736a1 1 0 0 0-.342-1.372L9 7.688V5.004a1 1 0 0 0-2 0v3.25a1 1 0 0 0 .486.857l2.5 1.5a1 1 0 0 0 1.371-.343Z"></path></svg></svg></span>10 mins to read</div></div></div></div></div></a></div></div></div><div data-index="1" class="slick-slide slick-active" tabindex="-1" aria-hidden="false" style="outline:none;width:33.333333333333336%"><div><div class="link" data-link-type="router"><a draggable="false" aria-labelledby="g-uniq-884159" aria-describedby="g-uniq-884161 g-uniq-884163" class="g-link g-link_view_normal pc-card-base-block pc-card-base-block_border_shadow bc-post-card__card" href="../../../2023/06/etl-vs-datapipelines.html"><div class="pc-storage-background-image pc-card-base-block__header bc-post-card__header" data-qa="background-image"><picture data-qa="background-image-image"><source srcSet="../../../../../assets/blog/articles/etl_vs_data_pipelines_small.png.webp" type="image/webp" data-qa="background-image-image-desktop-source-compressed"/><img alt="" src="../../../../../assets/blog/articles/etl_vs_data_pipelines_small.png" class="pc-storage-background-image__img"/></picture><div class="pc-storage-background-image__container"><div class="pc-card-base-block__header-content"><div class="bc-post-card__image-container" data-qa="blog-suggest-header"></div></div></div></div><div class="pc-card-base-block__body"><div class="pc-card-base-block__content"><h3 class="bc-post-card__title bc-post-card__title_size_s"><span><span id="g-uniq-884159">ETL vs data pipelines: What are they and how do they work?</span></span></h3></div><div class="pc-card-base-block__footer"><div class="bc-post-info__container"><div class="bc-post-info__suggest-container"><div class="bc-post-info__item bc-post-info__item_size_s" id="g-uniq-884161">June 16, 2023</div><div class="bc-post-info__item bc-post-info__item_size_s" id="g-uniq-884163"><span class="bc-post-info__icon"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="16" height="16" class="g-icon bc-post-info__icon-color" fill="currentColor" stroke="none" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 17" fill="currentColor" aria-hidden="true"><path fill-rule="evenodd" clip-rule="evenodd" d="M8 16.004a8 8 0 1 1 0-16 8 8 0 0 1 0 16Zm0-2a6 6 0 1 0 0-12 6 6 0 0 0 0 12Zm3.357-3.736a1 1 0 0 0-.342-1.372L9 7.688V5.004a1 1 0 0 0-2 0v3.25a1 1 0 0 0 .486.857l2.5 1.5a1 1 0 0 0 1.371-.343Z"></path></svg></svg></span>10 mins to read</div></div></div></div></div></a></div></div></div><div data-index="2" class="slick-slide slick-active" tabindex="-1" aria-hidden="false" style="outline:none;width:33.333333333333336%"><div><div class="link" data-link-type="router"><a draggable="false" aria-labelledby="g-uniq-884164" aria-describedby="g-uniq-884166 g-uniq-884168" class="g-link g-link_view_normal pc-card-base-block pc-card-base-block_border_shadow bc-post-card__card" href="../../../2023/05/what-is-data-pipeline.html"><div class="pc-storage-background-image pc-card-base-block__header bc-post-card__header" data-qa="background-image"><picture data-qa="background-image-image"><source srcSet="../../../../../assets/blog/articles/what-is-data-pipeline-small-cover.jpg.webp" type="image/webp" data-qa="background-image-image-desktop-source-compressed"/><img alt="" src="../../../../../assets/blog/articles/what-is-data-pipeline-small-cover.jpg" class="pc-storage-background-image__img"/></picture><div class="pc-storage-background-image__container"><div class="pc-card-base-block__header-content"><div class="bc-post-card__image-container" data-qa="blog-suggest-header"></div></div></div></div><div class="pc-card-base-block__body"><div class="pc-card-base-block__content"><h3 class="bc-post-card__title bc-post-card__title_size_s"><span><span id="g-uniq-884164">What is data pipeline: A comprehensive guide</span></span></h3></div><div class="pc-card-base-block__footer"><div class="bc-post-info__container"><div class="bc-post-info__suggest-container"><div class="bc-post-info__item bc-post-info__item_size_s" id="g-uniq-884166">May 12, 2023</div><div class="bc-post-info__item bc-post-info__item_size_s" id="g-uniq-884168"><span class="bc-post-info__icon"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="16" height="16" class="g-icon bc-post-info__icon-color" fill="currentColor" stroke="none" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 17" fill="currentColor" aria-hidden="true"><path fill-rule="evenodd" clip-rule="evenodd" d="M8 16.004a8 8 0 1 1 0-16 8 8 0 0 1 0 16Zm0-2a6 6 0 1 0 0-12 6 6 0 0 0 0 12Zm3.357-3.736a1 1 0 0 0-.342-1.372L9 7.688V5.004a1 1 0 0 0-2 0v3.25a1 1 0 0 0 .486.857l2.5 1.5a1 1 0 0 0 1.371-.343Z"></path></svg></svg></span>15 mins to read</div></div></div></div></div></a></div></div></div></div></div></div><div class="pc-SliderBlock__footer"></div></div></div></div></section></div></div></div></div></div></main></div></div></div><div class="bc-prompt bc-prompt_close"><div class="bc-prompt__content"><span class="bc-prompt__text">Sign in to save this post</span><div class="bc-prompt__actions"><button class="g-button g-button_view_action g-button_size_l g-button_pin_round-round bc-prompt__action" type="button"><span class="g-button__text">Sign In</span></button></div></div></div></div><footer class="footer"><div class="pc-Grid"><div class="container-fluid "><div class="row"><div class="col  col-12 col-md-4 footer__column"><div class="link" data-link-type="router"><div class="logo footer__logo"><img alt="Logo Icon" src="../../../../../assets/logo/dc-logo-dark.svg" width="178" height="36" decoding="async" data-nimg="future" class="logo__icon" loading="lazy" style="color:transparent"/><span class="logo__text"></span></div></div></div><div class="col  col-6 col-sm-3 col-md-2 footer__column"><div class="footer__column-title">Products</div><a aria-label="Managed Service for ClickHouse®" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../../services/managed-clickhouse.html"><div class="navigation-item"><span class="navigation-item__text">Managed Service for ClickHouse®</span></div></a><a aria-label="Managed Service for Apache Kafka®" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../../services/managed-kafka.html"><div class="navigation-item"><span class="navigation-item__text">Managed Service for Apache Kafka®</span></div></a><a aria-label="Managed Service for Apache Airflow®" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../../services/managed-airflow/index.html"><div class="navigation-item"><span class="navigation-item__text">Managed Service for Apache Airflow®</span></div></a><a aria-label="Data Transfer" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../../services/doublecloud-transfer.html"><div class="navigation-item"><span class="navigation-item__text">Data Transfer</span></div></a><a aria-label="Data Visualization" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../../services/doublecloud-visualization.html"><div class="navigation-item"><span class="navigation-item__text">Data Visualization</span></div></a></div><div class="col  col-6 col-sm-3 col-md-2 footer__column"><div class="footer__column-title">Solutions</div><a aria-label="Case studies" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../../resources/case-studies/index.html"><div class="navigation-item"><span class="navigation-item__text">Case studies</span></div></a><a aria-label="Customer-facing analytics" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../../customer-facing-analytics/index.html"><div class="navigation-item"><span class="navigation-item__text">Customer-facing analytics</span></div></a><a aria-label="Real-time analytics" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../../solutions/real-time-analytics/index.html"><div class="navigation-item"><span class="navigation-item__text">Real-time analytics</span></div></a><a aria-label="Observability and monitoring" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../../solutions/observability-and-monitoring/index.html"><div class="navigation-item"><span class="navigation-item__text">Observability and monitoring</span></div></a><a aria-label="AdTech and MarTech data analytics" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../../solutions/adtech.html"><div class="navigation-item"><span class="navigation-item__text">AdTech and MarTech data analytics</span></div></a><a aria-label="Analytics for mobile and gaming Apps" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../../solutions/web-mobile-gaming-apps.html"><div class="navigation-item"><span class="navigation-item__text">Analytics for mobile and gaming Apps</span></div></a><a aria-label="EdTech data analytics" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../../solutions/edtech/index.html"><div class="navigation-item"><span class="navigation-item__text">EdTech data analytics</span></div></a><a aria-label="FinTech data analytics" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../../solutions/fintech-real-time-analytics/index.html"><div class="navigation-item"><span class="navigation-item__text">FinTech data analytics</span></div></a></div><div class="col  col-6 col-sm-3 col-md-2 footer__column"><div class="footer__column-title">Resources</div><a aria-label="Documentation" class="navigation-item navigation-item_type_link footer__column-link" href="../../../../../docs/index.html"><div class="navigation-item"><span class="navigation-item__text">Documentation</span></div></a><a aria-label="Webinars" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../../webinars/index.html"><div class="navigation-item"><span class="navigation-item__text">Webinars</span></div></a><a aria-label="Blog" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../index.html"><div class="navigation-item navigation-item_selected"><span class="navigation-item__text">Blog</span></div></a><a aria-label="Support" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../../support/index.html"><div class="navigation-item"><span class="navigation-item__text">Support</span></div></a><a href="https://status.double.cloud/" aria-label="Status updates" class="navigation-item navigation-item_type_link footer__column-link" target="_blank" rel="noopener noreferrer"><div class="navigation-item"><span class="navigation-item__text">Status updates</span></div></a><a aria-label="Product comparisons" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../../comparison/index.html"><div class="navigation-item"><span class="navigation-item__text">Product comparisons</span></div></a><a aria-label="Site map" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../../sitemap/index.html"><div class="navigation-item"><span class="navigation-item__text">Site map</span></div></a></div><div class="col  col-6 col-sm-3 col-md-2 footer__column"><div class="footer__column-title">Company</div><a aria-label="About DoubleCloud" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../../company/about-us.html"><div class="navigation-item"><span class="navigation-item__text">About DoubleCloud</span></div></a><a aria-label="Careers" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../../company/careers.html"><div class="navigation-item"><span class="navigation-item__text">Careers</span></div></a><a aria-label="AWS Partnership" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../../aws-partnership/index.html"><div class="navigation-item"><span class="navigation-item__text">AWS Partnership</span></div></a></div></div><div class="row"><div class="col  col-12 footer__underline"><div class="footer__underline-links"><a href="../../../../../legal/customer_agreement/index.html" aria-label="Customer Agreement" class="navigation-item navigation-item_type_link footer__underline-link" target="_blank"><div class="navigation-item"><span class="navigation-item__text">Customer Agreement</span></div></a><a href="../../../../../legal/privacy.html" aria-label="Privacy Policy" class="navigation-item navigation-item_type_link footer__underline-link" target="_blank"><div class="navigation-item"><span class="navigation-item__text">Privacy Policy</span></div></a><a aria-label="Pricing" class="navigation-item navigation-item_type_link footer__underline-link" data-link-type="router" href="../../../../../pricing.html"><div class="navigation-item"><span class="navigation-item__text">Pricing</span></div></a><a href="../../../../../security.html" aria-label="Security" class="navigation-item navigation-item_type_link footer__underline-link" target="_blank"><div class="navigation-item"><span class="navigation-item__text">Security</span></div></a></div><div class="footer__underline-copyright">© 2024 DoubleCloud</div></div></div></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json" nonce="MnuBsw6YlmEOCSocT1fUiQ==">{"props":{"pageProps":{"data":{"status":"fulfilled","pageContent":{"page":{"id":166,"name":"blog/posts/2024/08/data-transformation-best-practices","createdAt":"2024-09-03T15:39:19.731Z","updatedAt":"2024-09-03T15:39:19.731Z","type":"default","isDeleted":false,"versionOnTranslationId":null,"pageId":166,"locale":"en","publishedVersionId":2280,"lastVersionId":2280,"content":{"blocks":[{"type":"blog-header-block","resetPaddings":true,"paddingBottom":"l","width":"s","verticalOffset":"l","background":{"image":{"src":"/assets/blog/articles/2024/best-practices-for-data-transformation-as-pipeline-complexity-grows-cover.png","disableCompress":true,"fetchPriority":"high"},"color":"#000000","fullWidth":false}},{"type":"blog-layout-block","resetPaddings":true,"mobileOrder":"reverse","children":[{"type":"blog-yfm-block","column":"right","resetPaddings":true,"text":"\u003cp\u003eQuick navigation:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#always-start-with-ingestion-level-transformations\"\u003eAlways start with ingestion-level transformations\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#move-to-materialized-views-as-data-starts-to-scale\"\u003eMove to\u0026nbsp;materialized views as\u0026nbsp;data starts to\u0026nbsp;scale\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#choose-dbt-when-quality-and-complexity-becomes-the-focus\"\u003eChoose dbt when quality and complexity becomes the focus\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#use-airflow-for-the-most-complex-workflows\"\u003eUse airflow for the most complex workflows\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#5-tips-for-every-data-transformation\"\u003e5 tips for every data transformation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#doubleclouds-approach-to-data-transformation\"\u003eDoubleCloud\u0026rsquo;s approach to\u0026nbsp;data transformation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#elevate-your-data-transformation-strategy-with-doublecloud\"\u003eElevate your data transformation strategy with DoubleCloud\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e"},{"type":"blog-yfm-block","column":"left","resetPaddings":true,"text":"\u003cp\u003eModern data platforms revolve around data transformation.\u003c/p\u003e\n\u003cp\u003eThat is\u0026nbsp;the entire point of\u0026nbsp;data pipelines\u0026ndash;\u003cem\u003etransforming raw data into actionable insights\u003c/em\u003e. However, data transformation is\u0026nbsp;also a\u0026nbsp;gigantic process, ranging from simple cleaning and formatting to\u0026nbsp;complex aggregations and machine learning models.\u003c/p\u003e\n\u003cp\u003eThe correct transformation approach is\u0026nbsp;crucial for efficiency, scalability, and data integrity. If\u0026nbsp;you get this wrong, you risk creating a\u0026nbsp;bottleneck that can slow down your entire data operation, lead to\u0026nbsp;inconsistent or\u0026nbsp;unreliable results, and make it\u0026nbsp;difficult to\u0026nbsp;adapt to\u0026nbsp;changing business needs or\u0026nbsp;scale your data infrastructure.\u003c/p\u003e\n\u003cp\u003eIf\u0026nbsp;you get it\u0026nbsp;right, though, you can easily handle increasing data volumes and complexity, deliver consistent, high-quality insights across your team, and provide a\u0026nbsp;solid foundation for advanced analytics like AI\u0026nbsp;and machine learning.\u003c/p\u003e\n\u003cp\u003eHere, we\u0026nbsp;want to\u0026nbsp;explain how we\u0026nbsp;think about data transformation best practices at\u0026nbsp;DoubleCloud, concentrating on\u0026nbsp;each level of\u0026nbsp;transformation as\u0026nbsp;your pipeline complexity grows.\u003c/p\u003e\n\u003ch2 id=\"always-start-with-ingestion-level-transformations\"\u003e\u003ca href=\"#always-start-with-ingestion-level-transformations\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eAlways start with ingestion-level transformations\u003c/span\u003e\u003c/a\u003eAlways start with ingestion-level transformations\u003c/h2\u003e\n\u003cp\u003eStarting at\u0026nbsp;the start may seem obvious, but teams can easily skip this step. They ingest vast amounts of\u0026nbsp;data and then pump it\u0026nbsp;down the pipeline without considering the problems this will cause. These problems can be\u0026nbsp;segmented into two categories.\u003c/p\u003e\n\u003ch3 id=\"data-quality\"\u003e\u003ca href=\"#data-quality\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eData quality\u003c/span\u003e\u003c/a\u003eData quality\u003c/h3\u003e\n\u003cp\u003e\u0026ldquo;Garbage In, Garbage Out\u0026rdquo; is\u0026nbsp;a\u0026nbsp;famous aphorism in\u0026nbsp;data analytics that is\u0026nbsp;starting to\u0026nbsp;be\u0026nbsp;forgotten. With the advent of\u0026nbsp;AI\u0026nbsp;and machine learning, teams can get into the habit of\u0026nbsp;thinking that these models can easily work around or\u0026nbsp;take care of\u0026nbsp;any data quality issues.\u003c/p\u003e\n\u003cp\u003eBut the refrain continues to\u0026nbsp;be\u0026nbsp;true. The better the quality of\u0026nbsp;your data at\u0026nbsp;the beginning of\u0026nbsp;the pipeline, the better the quality of\u0026nbsp;your insights at\u0026nbsp;the\u0026nbsp;end. Here are some first-line-of-defense transformations that can make it\u0026nbsp;easier to\u0026nbsp;work with your data downstream:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1. Data type validation:\u003c/strong\u003e Incoming data must adhere to\u0026nbsp;expected data types, such as\u0026nbsp;converting strings to\u0026nbsp;appropriate numeric or\u0026nbsp;date formats, parsing JSON or\u0026nbsp;XML structures, or\u0026nbsp;casting boolean values to\u0026nbsp;a\u0026nbsp;standardized format. This reduces the need for error handling and type conversions in\u0026nbsp;the later stages of\u0026nbsp;the pipeline.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2. Null value handling:\u003c/strong\u003e You can deal with null or\u0026nbsp;missing values using \u003ca href=\"https://scikit-learn.org/stable/modules/impute.html\"\u003eimputation\u003c/a\u003e, \u003ca href=\"https://www.scaler.com/topics/data-science/categorical-missing-values/#:~:text=replace%20missing%20values.-,Flagging,-In%20some%20cases\"\u003eflagging\u003c/a\u003e, or\u0026nbsp;removal. Null value handling is\u0026nbsp;required to\u0026nbsp;ensure that downstream analysis and pipelines don\u0026rsquo;t break due to\u0026nbsp;unexpected null values.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e3. Deduplication:\u003c/strong\u003e Deduplication, removing duplicate records at\u0026nbsp;the ingestion stage, improves data quality and reduces storage costs. Though you can brute-force this at\u0026nbsp;scale, depending on\u0026nbsp;your data characteristics and business rules, you must consider fuzzy matching techniques or\u0026nbsp;libraries such as\u0026nbsp;\u003ca href=\"https://pypi.org/project/dedupe/1.6.5/\"\u003ededupe\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e4. Trimming and cleaning:\u003c/strong\u003e Remove leading/trailing whitespaces, control characters, or\u0026nbsp;other unwanted artifacts from string fields. This seemingly simple step can prevent numerous issues downstream, such as\u0026nbsp;unexpected behavior in\u0026nbsp;string comparisons or\u0026nbsp;joins.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e5. Schema validation:\u003c/strong\u003e This ensures the incoming data structure matches the expected schema, handling any discrepancies or\u0026nbsp;schema evolution. Robust \u003ca href=\"https://python-jsonschema.readthedocs.io/en/latest/validate/\"\u003eschema validation\u003c/a\u003e can prevent data inconsistencies. You might want to\u0026nbsp;implement versioning for your schemas to\u0026nbsp;ensure your pipeline can adapt to\u0026nbsp;evolving data structures without breaking.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e6. Referential integrity checks:\u003c/strong\u003e Verify that foreign key relationships are maintained, especially when ingesting data from multiple sources. This maintains logical consistency of\u0026nbsp;your data across different tables or\u0026nbsp;datasets. Implement checks that validate the existence of\u0026nbsp;referenced keys and handle scenarios where referential integrity might be\u0026nbsp;temporarily broken due to\u0026nbsp;the order of\u0026nbsp;data ingestion (essential in\u0026nbsp;\u003ca href=\"https://neon.tech/blog/postgres-roles#:~:text=Onto%20the%20more%20significant%20issue%3A%20migrations.\"\u003emigrations\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003eAll these are for a\u0026nbsp;single reason: improve data quality now to\u0026nbsp;reduce complexity later.\u003c/p\u003e\n\u003ch3 id=\"data-security\"\u003e\u003ca href=\"#data-security\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eData security\u003c/span\u003e\u003c/a\u003eData security\u003c/h3\u003e\n\u003cp\u003eWe\u0026nbsp;left data security checks out of\u0026nbsp;the list above because they require more discussion. You must deal with personal or\u0026nbsp;sensitive data at\u0026nbsp;the ingestion level. This is\u0026nbsp;not just a\u0026nbsp;best practice but often a\u0026nbsp;legal requirement under regulations like GDPR, CCPA, or\u0026nbsp;industry-specific standards like HIPAA.\u003c/p\u003e\n\u003cp\u003eHere are some key data security transformations to\u0026nbsp;consider at\u0026nbsp;the ingestion stage:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1. Data masking:\u003c/strong\u003e Apply masking techniques to\u0026nbsp;sensitive fields such as\u0026nbsp;personal identifiers, credit card numbers, or\u0026nbsp;health information. This involves replacing sensitive data with fictitious but realistic data maintaining the data\u0026rsquo;s format and consistency while protecting individual privacy.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2. Encryption:\u003c/strong\u003e Implement encryption for highly sensitive data fields. This could involve using robust encryption algorithms to\u0026nbsp;protect data at\u0026nbsp;rest and in\u0026nbsp;transit. Consider using format-preserving encryption for fields where the encrypted data needs to\u0026nbsp;maintain the original format.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e3. Tokenization:\u003c/strong\u003e Replace sensitive data elements with non-sensitive equivalents or\u0026nbsp;tokens. This is\u0026nbsp;particularly useful for fields that must maintain uniqueness without exposing the original data.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e4. Data anonymization:\u003c/strong\u003e For datasets used in\u0026nbsp;analytics or\u0026nbsp;machine learning, consider anonymization techniques that remove or\u0026nbsp;alter personally identifiable information while preserving the data\u0026rsquo;s analytical value.\u003c/p\u003e\n\u003ch2 id=\"move-to-materialized-views-as-data-starts-to-scale\"\u003e\u003ca href=\"#move-to-materialized-views-as-data-starts-to-scale\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eMove to\u0026nbsp;materialized views as\u0026nbsp;data starts to\u0026nbsp;scale\u003c/span\u003e\u003c/a\u003eMove to\u0026nbsp;materialized views as\u0026nbsp;data starts to\u0026nbsp;scale\u003c/h2\u003e\n\u003cp\u003eAs\u0026nbsp;data volume grows and query complexity increases, simple transformations are still necessary but no\u0026nbsp;longer sufficient.\u003c/p\u003e\n\u003cp\u003eAt\u0026nbsp;this point, data engineers start working with \u003ca href=\"https://double.cloud/docs/en/managed-clickhouse/glossary#materialized-view\"\u003ematerialized views\u003c/a\u003e, which are pre-computed result sets stored for faster query performance.\u003c/p\u003e"},{"type":"blog-media-block","column":"left","resetPaddings":true,"text":"\u003cp\u003e(Source: \u003ca href=\"https://clickhouse.com/docs/en/materialized-view\"\u003eClickHouse\u003c/a\u003e)\u003c/p\u003e","image":"/assets/blog/articles/2024/best-practices-for-data-transformation-as-pipeline-complexity-grows-img-1-2.png","fullWidth":false}]},{"type":"blog-layout-block","resetPaddings":true,"mobileOrder":"reverse","children":[{"type":"blog-yfm-block","column":"left","resetPaddings":true,"text":"\u003cp\u003eImagine running an\u0026nbsp;e-commerce platform that generates billions of\u0026nbsp;clickstream events daily. You\u0026rsquo;ll frequently need to\u0026nbsp;analyze user engagement and sales performance. Instead of\u0026nbsp;running complex queries on\u0026nbsp;this large raw dataset every time, you can create materialized views to\u0026nbsp;pre-compute common aggregations:\u003c/p\u003e\n\n    \u003cdiv class=\"yfm-clipboard\"\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs sql\"\u003e\u003cspan class=\"hljs-keyword\"\u003eCREATE\u003c/span\u003e MATERIALIZED \u003cspan class=\"hljs-keyword\"\u003eVIEW\u003c/span\u003e daily_user_activity\nENGINE \u003cspan class=\"hljs-operator\"\u003e=\u003c/span\u003e SummingMergeTree()\n\u003cspan class=\"hljs-keyword\"\u003eORDER\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eBY\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003edate\u003c/span\u003e, user_id)\nPOPULATE\n\u003cspan class=\"hljs-keyword\"\u003eAS\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eSELECT\u003c/span\u003e\n    toDate(event_time) \u003cspan class=\"hljs-keyword\"\u003eAS\u003c/span\u003e \u003cspan class=\"hljs-type\"\u003edate\u003c/span\u003e,\n    user_id,\n    countIf(event_type \u003cspan class=\"hljs-operator\"\u003e=\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e'pageview'\u003c/span\u003e) \u003cspan class=\"hljs-keyword\"\u003eAS\u003c/span\u003e pageviews,\n    countIf(event_type \u003cspan class=\"hljs-operator\"\u003e=\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e'click'\u003c/span\u003e) \u003cspan class=\"hljs-keyword\"\u003eAS\u003c/span\u003e clicks,\n    countIf(event_type \u003cspan class=\"hljs-operator\"\u003e=\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e'purchase'\u003c/span\u003e) \u003cspan class=\"hljs-keyword\"\u003eAS\u003c/span\u003e purchases,\n    \u003cspan class=\"hljs-built_in\"\u003esum\u003c/span\u003e(purchase_amount) \u003cspan class=\"hljs-keyword\"\u003eAS\u003c/span\u003e total_spent\n\u003cspan class=\"hljs-keyword\"\u003eFROM\u003c/span\u003e raw_events\n\u003cspan class=\"hljs-keyword\"\u003eGROUP\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eBY\u003c/span\u003e \u003cspan class=\"hljs-type\"\u003edate\u003c/span\u003e, user_id;\n\n\u003c/code\u003e\u003c/pre\u003e\n\n    \u003csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\" class=\"yfm-clipboard-button\" data-animation=\"3\"\u003e\n        \u003cpath fill=\"currentColor\" d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\"\u003e\u003c/path\u003e\n        \u003cpath stroke=\"currentColor\" fill=\"transparent\" stroke-width=\"1.5\" d=\"M9.5 13l3 3l5 -5\" visibility=\"hidden\"\u003e\n            \u003canimate id=\"visibileAnimation-3\" attributeName=\"visibility\" from=\"hidden\" to=\"visible\" dur=\"0.2s\" fill=\"freeze\" begin\u003e\u003c/animate\u003e\n            \u003canimate id=\"hideAnimation-3\" attributeName=\"visibility\" from=\"visible\" to=\"hidden\" dur=\"1s\" begin=\"visibileAnimation-3.end+1\" fill=\"freeze\"\u003e\u003c/animate\u003e\n        \u003c/path\u003e\n    \u003c/svg\u003e\n    \u003c/div\u003e\n\u003cp\u003eThe view stores only the aggregated data, which updates automatically as\u0026nbsp;source data changes, significantly reducing the amount of\u0026nbsp;data scanned for common queries.\u003c/p\u003e\n\u003cp\u003eThe golden rule for materialized views is\u0026nbsp;to\u0026nbsp;understand your query performance. Materialized views perform best in\u0026nbsp;situations with frequent, expensive queries. In\u0026nbsp;particular, you want to\u0026nbsp;precompute expensive joins and aggregations. To\u0026nbsp;do\u0026nbsp;that, you can use \u003ca href=\"https://www.postgresql.org/docs/current/using-explain.html#USING-EXPLAIN-ANALYZE\"\u003eEXPLAIN ANALYZE\u003c/a\u003e to\u0026nbsp;understand query costs:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAnalyze execution time: Anything taking several seconds or\u0026nbsp;more could be\u0026nbsp;considered expensive, especially if\u0026nbsp;run frequently.\u003c/li\u003e\n\u003cli\u003eCheck for Seq Scans on\u0026nbsp;large tables: These are often indicators of\u0026nbsp;expensive operations.\u003c/li\u003e\n\u003cli\u003eCompare cost estimates: Higher costs suggest more expensive queries.\u003c/li\u003e\n\u003cli\u003eConsider the frequency of\u0026nbsp;the query: Even a\u0026nbsp;moderately expensive query can be\u0026nbsp;problematic if\u0026nbsp;run very often.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eMaterialized views are a\u0026nbsp;crucial layer in\u0026nbsp;your data transformation pipeline, enabling real-time analytics on\u0026nbsp;massive datasets while maintaining the flexibility to\u0026nbsp;access raw data when needed.\u003c/p\u003e\n\u003ch2 id=\"choose-dbt-when-quality-and-complexity-becomes-the-focus\"\u003e\u003ca href=\"#choose-dbt-when-quality-and-complexity-becomes-the-focus\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eChoose dbt when quality and complexity becomes the focus\u003c/span\u003e\u003c/a\u003eChoose dbt when quality and complexity becomes the focus\u003c/h2\u003e\n\u003cp\u003eAt\u0026nbsp;some point, manual transformation isn\u0026rsquo;t going to\u0026nbsp;cut\u0026nbsp;it. The rule of\u0026nbsp;thumb for when you need to\u0026nbsp;bring in\u0026nbsp;automation to\u0026nbsp;your transformations is\u0026nbsp;when:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eYou have more than five materialized views or\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ewhen data transformations become too complex to\u0026nbsp;manage manually.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThen, data quality tools such as\u0026nbsp;\u003ca href=\"https://www.getdbt.com/\"\u003edbt\u003c/a\u003e are needed. dbt is\u0026nbsp;an\u0026nbsp;open-source tool that enables data analysts and engineers to\u0026nbsp;transform data in\u0026nbsp;their warehouses more effectively. Engineers primarily use it\u0026nbsp;for more complex transformations and ensuring data quality.\u003c/p\u003e\n\u003cp\u003edbt brings a\u0026nbsp;more developer-like workflow to\u0026nbsp;data transformation. Instead of\u0026nbsp;individual SQL queries that need to\u0026nbsp;be\u0026nbsp;run manually for part of\u0026nbsp;an\u0026nbsp;ad-hoc transformation process, dbt brings structured queries and version control into the\u0026nbsp;mix. Engineers can use dbt to\u0026nbsp;create modular, reusable SQL code that defines how raw data should be\u0026nbsp;transformed into analytics-ready models.\u003c/p\u003e\n\u003cp\u003eIn\u0026nbsp;dbt, you break down complex transformations into smaller, more manageable pieces. This modular approach makes understanding, maintaining, and reusing transformation logic easier. Then, dbt includes built-in functionality for testing your data transformations. You can define tests to\u0026nbsp;ensure data quality, such as\u0026nbsp;checking for null values, unique constraints, or\u0026nbsp;custom business logic.\u003c/p\u003e\n\u003cp\u003eThese are the fundamentals we\u0026nbsp;mentioned for the ingestion phase. Even though you might only start using dbt once you have more complexity in\u0026nbsp;your pipelines, you should use it\u0026nbsp;throughout your pipeline once you hit that level of\u0026nbsp;complexity. \u003ca href=\"https://double.cloud/docs/en/managed-clickhouse/integrations/transform-data-in-clickhouse-with-dbt\"\u003edbt integrates with DoubleCloud\u0026rsquo;s Managed ClickHouse cluster\u003c/a\u003e to\u0026nbsp;do\u0026nbsp;precisely that, allowing you to\u0026nbsp;build a\u0026nbsp;seamless workflow from data ingestion to\u0026nbsp;transformation:\u003c/p\u003e"},{"type":"blog-media-block","column":"left","resetPaddings":true,"text":"","image":"/assets/blog/articles/2024/best-practices-for-data-transformation-as-pipeline-complexity-grows-img-2.png","fullWidth":false}]},{"type":"blog-layout-block","resetPaddings":true,"mobileOrder":"reverse","children":[{"type":"blog-yfm-block","column":"left","resetPaddings":true,"text":"\u003cp\u003eTo\u0026nbsp;get started with dbt in\u0026nbsp;your data transformation workflow:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1. Define your models:\u003c/strong\u003e Create SQL files that represent your data models, from staging tables that closely represent your raw data to\u0026nbsp;intermediate and final models that power your analytics.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2. Set up\u0026nbsp;tests:\u003c/strong\u003e Write simple YAML configurations to\u0026nbsp;test your data for nulls, uniqueness, accepted values, and custom business logic.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e3. Document your models:\u003c/strong\u003e Use YAML files and markdown to\u0026nbsp;document your models, making it\u0026nbsp;easier for others to\u0026nbsp;understand and use your transformed data.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e4. Create a\u0026nbsp;project structure:\u003c/strong\u003e Organize your dbt project with a\u0026nbsp;clear folder structure, separating staging models, intermediate models, and final models.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e5. Implement CI/CD:\u003c/strong\u003e Set up\u0026nbsp;continuous integration to\u0026nbsp;run your dbt models and tests automatically when changes are pushed to\u0026nbsp;your repository.\u003c/p\u003e\n\u003cp\u003eThe transition to\u0026nbsp;using dbt should be\u0026nbsp;gradual. Start with a\u0026nbsp;few key models, get comfortable with the workflow, and expand its use across your data transformation process. By\u0026nbsp;adopting dbt, you\u0026rsquo;re not just automating your transformations but bringing software engineering best practices to\u0026nbsp;your data work. This results in\u0026nbsp;more reliable, maintainable, and scalable data transformations, which is\u0026nbsp;crucial as\u0026nbsp;your data pipeline grows in\u0026nbsp;complexity.\u003c/p\u003e\n\u003ch2 id=\"use-airflow-for-the-most-complex-workflows\"\u003e\u003ca href=\"#use-airflow-for-the-most-complex-workflows\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eUse Airflow for the most complex workflows\u003c/span\u003e\u003c/a\u003eUse Airflow for the most complex workflows\u003c/h2\u003e\n\u003cp\u003eWith dbt, you can build workflows, but dbt itself doesn\u0026rsquo;t handle scheduling. For that, you need an\u0026nbsp;orchestration tool, such as\u0026nbsp;\u003ca href=\"https://airflow.apache.org/\"\u003eAirflow\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAirflow will allow you to\u0026nbsp;manage dependencies between tasks and schedule complex jobs, give you the flexibility to\u0026nbsp;handle diverse workflows, provide scalability for large-scale data processing, and provide robust error handling and monitoring. Airflow\u0026rsquo;s \u0026ldquo;secret sauce\u0026rdquo; is\u0026nbsp;the \u003ca href=\"https://double.cloud/docs/en/managed-airflow/concepts/dag\"\u003eDirected Acyclic Graph\u003c/a\u003e, or\u0026nbsp;DAG. Airflow leverages DAGs to\u0026nbsp;manage complex workflows and scheduling.\u003c/p\u003e\n\u003cp\u003eIn\u0026nbsp;Airflow, you define your workflow as\u0026nbsp;a\u0026nbsp;DAG using Python code. This lets you programmatically create a\u0026nbsp;structure representing your entire data pipeline or\u0026nbsp;process. Within a\u0026nbsp;DAG, you define individual tasks and their dependencies. Tasks can be\u0026nbsp;anything from running a\u0026nbsp;SQL query to\u0026nbsp;executing a\u0026nbsp;Python function or\u0026nbsp;interacting with an\u0026nbsp;external system. Dependencies determine the order in\u0026nbsp;which tasks should run.\u003c/p\u003e"},{"type":"blog-media-block","column":"left","resetPaddings":true,"text":"\u003cp\u003e(Source: \u003ca href=\"https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html\"\u003eApache Airflow\u003c/a\u003e)\u003c/p\u003e","image":"/assets/blog/articles/2024/best-practices-for-data-transformation-as-pipeline-complexity-grows-img-3.png","fullWidth":false}]},{"type":"blog-layout-block","resetPaddings":true,"mobileOrder":"reverse","children":[{"type":"blog-yfm-block","column":"left","resetPaddings":true,"text":"\u003cp\u003eEach DAG can have a\u0026nbsp;schedule associated with\u0026nbsp;it. This could be\u0026nbsp;as\u0026nbsp;simple as\u0026nbsp;\u0026ldquo;run daily at\u0026nbsp;midnight\u0026rdquo; or\u0026nbsp;as\u0026nbsp;complex as\u0026nbsp;\u0026ldquo;run every 15\u0026nbsp;minutes on\u0026nbsp;weekdays\u0026rdquo;. Airflow\u0026rsquo;s scheduler uses these definitions to\u0026nbsp;determine when to\u0026nbsp;trigger DAG runs. When it\u0026rsquo;s time for a\u0026nbsp;DAG to\u0026nbsp;run, Airflow\u0026rsquo;s executor starts processing tasks. It\u0026nbsp;begins with tasks with no\u0026nbsp;dependencies and moves through the graph as\u0026nbsp;tasks are completed, always respecting the defined dependencies.\u003c/p\u003e\n\u003cp\u003eBecause the workflow is\u0026nbsp;defined as\u0026nbsp;a\u0026nbsp;graph, Airflow can quickly identify which tasks can run in\u0026nbsp;parallel. This allows for efficient execution of\u0026nbsp;complex workflows. Airflow keeps track of\u0026nbsp;the state of\u0026nbsp;each task (running, success, failed, etc.). If\u0026nbsp;a\u0026nbsp;task fails, Airflow can automatically retry it\u0026nbsp;based on\u0026nbsp;your configuration.\u003c/p\u003e\n\u003cp\u003eThis approach allows Airflow to\u0026nbsp;handle everything from simple linear workflows to\u0026nbsp;complex branching processes with multiple dependencies and parallel execution paths. It\u0026rsquo;s ideal for the most complex data engineering scenarios where you might need to\u0026nbsp;orchestrate ETL processes, run machine learning pipelines, or\u0026nbsp;manage complex data transformation workflows.\u003c/p\u003e\n\u003ch2 id=\"5-tips-for-every-data-transformation\"\u003e\u003ca href=\"#5-tips-for-every-data-transformation\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003e5 tips for every data transformation\u003c/span\u003e\u003c/a\u003e5 tips for every data transformation\u003c/h2\u003e\n\u003cp\u003eWhile we\u0026rsquo;ve covered specific approaches for different stages of\u0026nbsp;data pipeline complexity, some overarching principles apply universally.\u003c/p\u003e\n\u003ch3 id=\"1-simplicity-and-maintainability-as-key-goals\"\u003e\u003ca href=\"#1-simplicity-and-maintainability-as-key-goals\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003e1. Simplicity and maintainability as\u0026nbsp;key goals\u003c/span\u003e\u003c/a\u003e1. Simplicity and maintainability as\u0026nbsp;key goals\u003c/h3\u003e\n\u003cp\u003eAlways strive for simplicity in\u0026nbsp;your transformations. Complex transformations might seem impressive, but they often lead to\u0026nbsp;headaches down the road. Here\u0026rsquo;s why simplicity matters:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEasier debugging:\u003c/strong\u003e When issues arise (and they will), more straightforward transformations are quicker to\u0026nbsp;troubleshoot.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eImproved maintainability:\u003c/strong\u003e As\u0026nbsp;your team evolves, simpler code is\u0026nbsp;easier for new team members to\u0026nbsp;understand and modify.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eReduced error risk:\u003c/strong\u003e Complex transformations introduce more points of\u0026nbsp;failure. Keeping things simple minimizes this risk.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eImplement this by\u0026nbsp;breaking down complex transformations into smaller, manageable steps. Use clear naming conventions and add comments to\u0026nbsp;explain the logic behind each transformation. Remember, the goal is\u0026nbsp;not just to\u0026nbsp;transform data but to\u0026nbsp;do\u0026nbsp;so\u0026nbsp;in\u0026nbsp;a\u0026nbsp;sustainable and scalable way as\u0026nbsp;your data needs grow.\u003c/p\u003e\n\u003ch3 id=\"2-denormalization-in-data-warehousing\"\u003e\u003ca href=\"#2-denormalization-in-data-warehousing\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003e2. Denormalization in\u0026nbsp;data warehousing\u003c/span\u003e\u003c/a\u003e2. Denormalization in\u0026nbsp;data warehousing\u003c/h3\u003e\n\u003cp\u003eIn\u0026nbsp;traditional database design, normalization is\u0026nbsp;king. But denormalization often takes the crown in\u0026nbsp;the world of\u0026nbsp;data warehousing and analytics. Here\u0026rsquo;s why:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eFaster queries:\u003c/strong\u003e Denormalized data requires fewer joins, significantly speeding up\u0026nbsp;query performance.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSimpler joins:\u003c/strong\u003e When joins are necessary, they\u0026rsquo;re typically simpler in\u0026nbsp;a\u0026nbsp;denormalized model.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eImproved read performance:\u003c/strong\u003e Analytics workloads are often read-heavy, and denormalization optimizes for reads.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eDenormalization isn\u0026rsquo;t without its trade-offs. You\u0026rsquo;re storing redundant data, which increases storage needs, and updating denormalized data requires careful management to\u0026nbsp;maintain consistency.\u003c/p\u003e\n\u003ch3 id=\"3-performance-considerations-for-real-time-analytics\"\u003e\u003ca href=\"#3-performance-considerations-for-real-time-analytics\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003e3. Performance considerations for real-time analytics\u003c/span\u003e\u003c/a\u003e3. Performance considerations for real-time analytics\u003c/h3\u003e\n\u003cp\u003eOptimizing your transformations for low-latency queries becomes crucial when building for real-time insights. As\u0026nbsp;discussed earlier, materialized views can significantly speed up\u0026nbsp;common queries, but that isn\u0026rsquo;t the only performance optimization you can make.\u003c/p\u003e\n\u003cp\u003eFirstly, you can consider your own data transformation strategy. Do\u0026nbsp;you need to\u0026nbsp;reprocess all data? Or\u0026nbsp;can you focus on\u0026nbsp;transforming only the new or\u0026nbsp;changed data?\u003c/p\u003e\n\u003cp\u003eSecond, can you take advantage of\u0026nbsp;tooling for better optimization? This might be\u0026nbsp;in-memory stores for the most time-sensitive operations or\u0026nbsp;properly partitioned and indexed data to\u0026nbsp;improve query performance.\u003c/p\u003e\n\u003cp\u003eRemember, real-time doesn\u0026rsquo;t always mean instantaneous. Understand your actual latency requirements and optimize accordingly. Sometimes, \u0026ldquo;near real-time\u0026rdquo; is\u0026nbsp;sufficient and much easier to\u0026nbsp;achieve.\u003c/p\u003e\n\u003ch3 id=\"4-security-and-compliance-throughout-the-transformation-process\"\u003e\u003ca href=\"#4-security-and-compliance-throughout-the-transformation-process\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003e4. Security and compliance throughout the transformation process\u003c/span\u003e\u003c/a\u003e4. Security and compliance throughout the transformation process\u003c/h3\u003e\n\u003cp\u003eData security isn\u0026rsquo;t just an\u0026nbsp;ingestion-level concern. It\u0026nbsp;needs to\u0026nbsp;be\u0026nbsp;baked into every stage of\u0026nbsp;your data pipeline. Here\u0026rsquo;s how:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eData encryption:\u003c/strong\u003e Ensure data is\u0026nbsp;encrypted at\u0026nbsp;rest and in\u0026nbsp;transit, even during transformation processes.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAccess controls:\u003c/strong\u003e Implement fine-grained access controls. Not everyone needs access to\u0026nbsp;all transformed data.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAudit trails:\u003c/strong\u003e Maintain detailed logs of\u0026nbsp;all data transformations for compliance and troubleshooting.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eData lineage:\u003c/strong\u003e Track the origin and transformations of\u0026nbsp;each data element. This is\u0026nbsp;crucial for both compliance and data governance.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"5-consider-the-end-use\"\u003e\u003ca href=\"#5-consider-the-end-use\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003e5. Consider the end-use\u003c/span\u003e\u003c/a\u003e5. Consider the end-use\u003c/h3\u003e\n\u003cp\u003eFinally, always consider the end-use of\u0026nbsp;your data. Your transformation strategy should be\u0026nbsp;tailored to\u0026nbsp;how the data will ultimately be\u0026nbsp;consumed.\u003c/p\u003e\n\u003cp\u003eDo\u0026nbsp;you have a\u0026nbsp;data contract or\u0026nbsp;SLA with downstream users that decides latency or\u0026nbsp;freshness? Establish clear data contracts with downstream users. These should define expectations for data quality, freshness, and availability. For instance, a\u0026nbsp;real-time dashboard might require 99.9% uptime and data no\u0026nbsp;older than 5\u0026nbsp;minutes, while a\u0026nbsp;monthly report might tolerate 24\u0026nbsp;hours of\u0026nbsp;lag.\u003c/p\u003e\n\u003cp\u003eRemember, the data team\u0026rsquo;s job is\u0026nbsp;to\u0026nbsp;ship insights, not data. Align your transformation strategy with key business metrics and decision-making processes. Understand which transformations directly support critical business KPIs and prioritize these. This approach transforms data engineering from a\u0026nbsp;support function into a\u0026nbsp;strategic driver of\u0026nbsp;business value, directly tying your technical decisions to\u0026nbsp;business outcomes and user satisfaction.\u003c/p\u003e\n\u003ch2 id=\"doubleclouds-approach-to-data-transformation\"\u003e\u003ca href=\"#doubleclouds-approach-to-data-transformation\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eDoubleCloud\u0026rsquo;s approach to\u0026nbsp;data transformation\u003c/span\u003e\u003c/a\u003eDoubleCloud\u0026rsquo;s approach to\u0026nbsp;data transformation\u003c/h2\u003e\n\u003cp\u003eEffective data transformation is\u0026nbsp;the cornerstone of\u0026nbsp;any successful data strategy. At\u0026nbsp;DoubleCloud, our approach is\u0026nbsp;designed around transformation to\u0026nbsp;meet the needs of\u0026nbsp;modern data teams, from simple ETL processes to\u0026nbsp;complex, real-time analytics pipelines.\u003c/p\u003e\n\u003cp\u003eIn\u0026nbsp;particular, \u003ca href=\"https://double.cloud/services/doublecloud-transfer/\"\u003eDoubleCloud Data Transfer\u003c/a\u003e is\u0026nbsp;designed to\u0026nbsp;be\u0026nbsp;a\u0026nbsp;simple data integration solution that excels at\u0026nbsp;extracting, loading, and transforming data. It\u0026nbsp;is\u0026nbsp;nine times faster than Airbyte and ideal for working with ClickHouse.\u003c/p\u003e"},{"type":"blog-media-block","column":"left","resetPaddings":true,"text":"","image":"/assets/blog/articles/2024/best-practices-for-data-transformation-as-pipeline-complexity-grows-img-4.png","fullWidth":false}]},{"type":"blog-layout-block","resetPaddings":true,"mobileOrder":"reverse","children":[{"type":"blog-yfm-block","column":"left","resetPaddings":true,"text":"\u003cp\u003eWith high-speed data ingestion into ClickHouse, Data Transfer can reach up\u0026nbsp;to\u0026nbsp;2\u0026nbsp;GB/s, making it\u0026nbsp;an\u0026nbsp;ideal choice for organizations looking to\u0026nbsp;build customer-facing or\u0026nbsp;real-time analytics solutions.\u003c/p\u003e\n\u003cp\u003eThis lets you move data in\u0026nbsp;any arrangement, giving you complete control over your data flow. This flexibility is\u0026nbsp;crucial for complex data transformation pipelines. DoubleClod Data Transfer natively integrates with dbt, allowing users to\u0026nbsp;quickly transform, clean, and summarize data for analysis. This integration enhances data modeling capabilities and improves team collaboration to\u0026nbsp;build out the data products your organization needs.\u003c/p\u003e"},{"type":"blog-media-block","column":"left","resetPaddings":true,"text":"","image":"/assets/blog/articles/2024/best-practices-for-data-transformation-as-pipeline-complexity-grows-img-5.png","fullWidth":false}]},{"type":"blog-layout-block","resetPaddings":true,"mobileOrder":"reverse","children":[{"type":"blog-yfm-block","column":"left","resetPaddings":true,"text":"\u003cp\u003eDoubleCloud\u0026rsquo;s approach to\u0026nbsp;data transformation aims to\u0026nbsp;provide a\u0026nbsp;comprehensive, efficient, and cost-effective solution for modern data teams. DoubleCloud Data Transfer is\u0026nbsp;designed to\u0026nbsp;meet data transformation needs while optimizing for performance and cost, whether you\u0026rsquo;re dealing with simple ETL processes or\u0026nbsp;complex, real-time analytics pipelines.\u003c/p\u003e\n\u003ch2 id=\"elevate-your-data-transformation-strategy-with-doublecloud\"\u003e\u003ca href=\"#elevate-your-data-transformation-strategy-with-doublecloud\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eElevate your data transformation strategy with DoubleCloud\u003c/span\u003e\u003c/a\u003eElevate your data transformation strategy with DoubleCloud\u003c/h2\u003e\n\u003cp\u003eData transformation is\u0026nbsp;a\u0026nbsp;strategic imperative for any organization looking to\u0026nbsp;harness the full power of\u0026nbsp;its data. From ingestion-level transformations to\u0026nbsp;complex orchestration with tools like Airflow, each stage of\u0026nbsp;your data pipeline requires careful consideration and planning.\u003c/p\u003e\n\u003cp\u003eThe best practices form the backbone of\u0026nbsp;a\u0026nbsp;robust data transformation strategy. By\u0026nbsp;implementing these practices, your data team can produce the insights the rest of\u0026nbsp;your organization requires. Building your data solutions on\u0026nbsp;DoubleCloud infrastructure gives you the ability to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eScale your data pipelines to\u0026nbsp;handle growing volumes and complexity\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eOptimize performance with ClickHouse-native tools and high-speed data ingestion\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eMaintain data quality and security throughout the transformation process\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eStreamline your workflow with built-in dbt integration for advanced transformations\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis is\u0026nbsp;all necessary for robust data transformation. You can get \u003ca href=\"https://auth.double.cloud/s/signup?_gl=1*19qneeh*_ga*NzU0OTMyNzI2LjE3MTk0MTg0OTE.*_ga_3G0X0VK41E*MTcyMzUxODEyMy4xNC4xLjE3MjM1MTg5NzcuNjAuMC4zODkxNTU0MDA.*_gcl_au*ODAxMjQyMTg2LjE3MTk0MTg0OTI.\"\u003estarted right away\u003c/a\u003e with DoubleCloud to\u0026nbsp;move your data, or\u0026nbsp;you can \u003ca href=\"https://double.cloud/#contact-us-form\"\u003ereach out to\u0026nbsp;the DoubleCloud team\u003c/a\u003e to\u0026nbsp;tell us\u0026nbsp;more about your data needs.\u003c/p\u003e"}]},{"type":"content-layout-block","background":{"src":"/assets/doublecloud/doublecloud-cover-6.png","style":{"backgroundColor":"#CA1551"}},"centered":true,"textContent":{"title":"Get started with DoubleCloud","buttons":[{"text":"Start free trial","size":"promo","theme":"accent","url":"https://auth.double.cloud/s/signup"},{"text":"Contact us","theme":"pseudo","url":"#contact-us-form"}]}},{"type":"blog-suggest-block","resetPaddings":true,"fullWidth":false}]},"title":"","noIndex":false,"shareTitle":null,"shareDescription":"","shareImage":"/assets/blog/articles/2024/best-practices-for-data-transformation-as-pipeline-complexity-grow-sharing.png","pageLocaleId":332,"author":"unknown","metaDescription":"Modern data platforms revolve around data transformation. There are different levels of transformation, and you must choose the appropriate tools for each level. Discover our experience and best practices.","keywords":[],"shareGenTitle":null,"canonicalLink":null,"sharingType":"custom","sharingTheme":"light","comment":"meta fix","shareImageUrl":"/assets/blog/articles/2024/best-practices-for-data-transformation-as-pipeline-complexity-grow-sharing.png","pageRegionId":null,"service":null,"solution":null,"locales":[{"locale":"ru","publishedVersionId":null},{"locale":"en","publishedVersionId":2280}],"regions":[],"pageRegions":[{"id":5,"pageId":166,"regionCode":"en","createdAt":"2024-09-03T13:34:04.136Z","updatedAt":"2024-09-03T13:34:04.165Z","publishedVersionId":null,"lastVersionId":2255}]},"post":{"url":"","id":166,"name":"data-transformation-best-practices","isPinned":false,"blogPostId":166,"image":"/assets/blog/articles/2024/best-practices-for-data-transformation-as-pipeline-complexity-grows-small-cover.png","readingTime":15,"date":"2024-08-20T00:00:00Z","likes":0,"hasUserLike":false,"services":[],"slug":"","authors":[],"locale":{"lang":"en"},"textTitle":"Best practices for data transformation as pipeline complexity grows","htmlTitle":"Best practices for data transformation as\u0026nbsp;pipeline complexity grows","title":"Best practices for data transformation as pipeline complexity grows","tags":[{"icon":null,"slug":"insights","name":"Insights","createdAt":"","updatedAt":"","count":0}],"metaTitle":"Best practices for data transformation as pipeline complexity grows","description":"","content":"\u003cp\u003eQuick navigation:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#always-start-with-ingestion-level-transformations\"\u003eAlways start with ingestion-level transformations\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#move-to-materialized-views-as-data-starts-to-scale\"\u003eMove to\u0026nbsp;materialized views as\u0026nbsp;data starts to\u0026nbsp;scale\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#choose-dbt-when-quality-and-complexity-becomes-the-focus\"\u003eChoose dbt when quality and complexity becomes the focus\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#use-airflow-for-the-most-complex-workflows\"\u003eUse airflow for the most complex workflows\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#5-tips-for-every-data-transformation\"\u003e5 tips for every data transformation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#doubleclouds-approach-to-data-transformation\"\u003eDoubleCloud\u0026rsquo;s approach to\u0026nbsp;data transformation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#elevate-your-data-transformation-strategy-with-doublecloud\"\u003eElevate your data transformation strategy with DoubleCloud\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eModern data platforms revolve around data transformation.\u003c/p\u003e\n\u003cp\u003eThat is\u0026nbsp;the entire point of\u0026nbsp;data pipelines\u0026ndash;\u003cem\u003etransforming raw data into actionable insights\u003c/em\u003e. However, data transformation is\u0026nbsp;also a\u0026nbsp;gigantic process, ranging from simple cleaning and formatting to\u0026nbsp;complex aggregations and machine learning models.\u003c/p\u003e\n\u003cp\u003eThe correct transformation approach is\u0026nbsp;crucial for efficiency, scalability, and data integrity. If\u0026nbsp;you get this wrong, you risk creating a\u0026nbsp;bottleneck that can slow down your entire data operation, lead to\u0026nbsp;inconsistent or\u0026nbsp;unreliable results, and make it\u0026nbsp;difficult to\u0026nbsp;adapt to\u0026nbsp;changing business needs or\u0026nbsp;scale your data infrastructure.\u003c/p\u003e\n\u003cp\u003eIf\u0026nbsp;you get it\u0026nbsp;right, though, you can easily handle increasing data volumes and complexity, deliver consistent, high-quality insights across your team, and provide a\u0026nbsp;solid foundation for advanced analytics like AI\u0026nbsp;and machine learning.\u003c/p\u003e\n\u003cp\u003eHere, we\u0026nbsp;want to\u0026nbsp;explain how we\u0026nbsp;think about data transformation best practices at\u0026nbsp;DoubleCloud, concentrating on\u0026nbsp;each level of\u0026nbsp;transformation as\u0026nbsp;your pipeline complexity grows.\u003c/p\u003e\n\u003ch2 id=\"always-start-with-ingestion-level-transformations\"\u003e\u003ca href=\"#always-start-with-ingestion-level-transformations\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eAlways start with ingestion-level transformations\u003c/span\u003e\u003c/a\u003eAlways start with ingestion-level transformations\u003c/h2\u003e\n\u003cp\u003eStarting at\u0026nbsp;the start may seem obvious, but teams can easily skip this step. They ingest vast amounts of\u0026nbsp;data and then pump it\u0026nbsp;down the pipeline without considering the problems this will cause. These problems can be\u0026nbsp;segmented into two categories.\u003c/p\u003e\n\u003ch3 id=\"data-quality\"\u003e\u003ca href=\"#data-quality\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eData quality\u003c/span\u003e\u003c/a\u003eData quality\u003c/h3\u003e\n\u003cp\u003e\u0026ldquo;Garbage In, Garbage Out\u0026rdquo; is\u0026nbsp;a\u0026nbsp;famous aphorism in\u0026nbsp;data analytics that is\u0026nbsp;starting to\u0026nbsp;be\u0026nbsp;forgotten. With the advent of\u0026nbsp;AI\u0026nbsp;and machine learning, teams can get into the habit of\u0026nbsp;thinking that these models can easily work around or\u0026nbsp;take care of\u0026nbsp;any data quality issues.\u003c/p\u003e\n\u003cp\u003eBut the refrain continues to\u0026nbsp;be\u0026nbsp;true. The better the quality of\u0026nbsp;your data at\u0026nbsp;the beginning of\u0026nbsp;the pipeline, the better the quality of\u0026nbsp;your insights at\u0026nbsp;the\u0026nbsp;end. Here are some first-line-of-defense transformations that can make it\u0026nbsp;easier to\u0026nbsp;work with your data downstream:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1. Data type validation:\u003c/strong\u003e Incoming data must adhere to\u0026nbsp;expected data types, such as\u0026nbsp;converting strings to\u0026nbsp;appropriate numeric or\u0026nbsp;date formats, parsing JSON or\u0026nbsp;XML structures, or\u0026nbsp;casting boolean values to\u0026nbsp;a\u0026nbsp;standardized format. This reduces the need for error handling and type conversions in\u0026nbsp;the later stages of\u0026nbsp;the pipeline.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2. Null value handling:\u003c/strong\u003e You can deal with null or\u0026nbsp;missing values using \u003ca href=\"https://scikit-learn.org/stable/modules/impute.html\"\u003eimputation\u003c/a\u003e, \u003ca href=\"https://www.scaler.com/topics/data-science/categorical-missing-values/#:~:text=replace%20missing%20values.-,Flagging,-In%20some%20cases\"\u003eflagging\u003c/a\u003e, or\u0026nbsp;removal. Null value handling is\u0026nbsp;required to\u0026nbsp;ensure that downstream analysis and pipelines don\u0026rsquo;t break due to\u0026nbsp;unexpected null values.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e3. Deduplication:\u003c/strong\u003e Deduplication, removing duplicate records at\u0026nbsp;the ingestion stage, improves data quality and reduces storage costs. Though you can brute-force this at\u0026nbsp;scale, depending on\u0026nbsp;your data characteristics and business rules, you must consider fuzzy matching techniques or\u0026nbsp;libraries such as\u0026nbsp;\u003ca href=\"https://pypi.org/project/dedupe/1.6.5/\"\u003ededupe\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e4. Trimming and cleaning:\u003c/strong\u003e Remove leading/trailing whitespaces, control characters, or\u0026nbsp;other unwanted artifacts from string fields. This seemingly simple step can prevent numerous issues downstream, such as\u0026nbsp;unexpected behavior in\u0026nbsp;string comparisons or\u0026nbsp;joins.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e5. Schema validation:\u003c/strong\u003e This ensures the incoming data structure matches the expected schema, handling any discrepancies or\u0026nbsp;schema evolution. Robust \u003ca href=\"https://python-jsonschema.readthedocs.io/en/latest/validate/\"\u003eschema validation\u003c/a\u003e can prevent data inconsistencies. You might want to\u0026nbsp;implement versioning for your schemas to\u0026nbsp;ensure your pipeline can adapt to\u0026nbsp;evolving data structures without breaking.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e6. Referential integrity checks:\u003c/strong\u003e Verify that foreign key relationships are maintained, especially when ingesting data from multiple sources. This maintains logical consistency of\u0026nbsp;your data across different tables or\u0026nbsp;datasets. Implement checks that validate the existence of\u0026nbsp;referenced keys and handle scenarios where referential integrity might be\u0026nbsp;temporarily broken due to\u0026nbsp;the order of\u0026nbsp;data ingestion (essential in\u0026nbsp;\u003ca href=\"https://neon.tech/blog/postgres-roles#:~:text=Onto%20the%20more%20significant%20issue%3A%20migrations.\"\u003emigrations\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003eAll these are for a\u0026nbsp;single reason: improve data quality now to\u0026nbsp;reduce complexity later.\u003c/p\u003e\n\u003ch3 id=\"data-security\"\u003e\u003ca href=\"#data-security\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eData security\u003c/span\u003e\u003c/a\u003eData security\u003c/h3\u003e\n\u003cp\u003eWe\u0026nbsp;left data security checks out of\u0026nbsp;the list above because they require more discussion. You must deal with personal or\u0026nbsp;sensitive data at\u0026nbsp;the ingestion level. This is\u0026nbsp;not just a\u0026nbsp;best practice but often a\u0026nbsp;legal requirement under regulations like GDPR, CCPA, or\u0026nbsp;industry-specific standards like HIPAA.\u003c/p\u003e\n\u003cp\u003eHere are some key data security transformations to\u0026nbsp;consider at\u0026nbsp;the ingestion stage:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1. Data masking:\u003c/strong\u003e Apply masking techniques to\u0026nbsp;sensitive fields such as\u0026nbsp;personal identifiers, credit card numbers, or\u0026nbsp;health information. This involves replacing sensitive data with fictitious but realistic data maintaining the data\u0026rsquo;s format and consistency while protecting individual privacy.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2. Encryption:\u003c/strong\u003e Implement encryption for highly sensitive data fields. This could involve using robust encryption algorithms to\u0026nbsp;protect data at\u0026nbsp;rest and in\u0026nbsp;transit. Consider using format-preserving encryption for fields where the encrypted data needs to\u0026nbsp;maintain the original format.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e3. Tokenization:\u003c/strong\u003e Replace sensitive data elements with non-sensitive equivalents or\u0026nbsp;tokens. This is\u0026nbsp;particularly useful for fields that must maintain uniqueness without exposing the original data.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e4. Data anonymization:\u003c/strong\u003e For datasets used in\u0026nbsp;analytics or\u0026nbsp;machine learning, consider anonymization techniques that remove or\u0026nbsp;alter personally identifiable information while preserving the data\u0026rsquo;s analytical value.\u003c/p\u003e\n\u003ch2 id=\"move-to-materialized-views-as-data-starts-to-scale\"\u003e\u003ca href=\"#move-to-materialized-views-as-data-starts-to-scale\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eMove to\u0026nbsp;materialized views as\u0026nbsp;data starts to\u0026nbsp;scale\u003c/span\u003e\u003c/a\u003eMove to\u0026nbsp;materialized views as\u0026nbsp;data starts to\u0026nbsp;scale\u003c/h2\u003e\n\u003cp\u003eAs\u0026nbsp;data volume grows and query complexity increases, simple transformations are still necessary but no\u0026nbsp;longer sufficient.\u003c/p\u003e\n\u003cp\u003eAt\u0026nbsp;this point, data engineers start working with \u003ca href=\"https://double.cloud/docs/en/managed-clickhouse/glossary#materialized-view\"\u003ematerialized views\u003c/a\u003e, which are pre-computed result sets stored for faster query performance.\u003c/p\u003e\n\u003cp\u003e(Source: \u003ca href=\"https://clickhouse.com/docs/en/materialized-view\"\u003eClickHouse\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003eImagine running an\u0026nbsp;e-commerce platform that generates billions of\u0026nbsp;clickstream events daily. You\u0026rsquo;ll frequently need to\u0026nbsp;analyze user engagement and sales performance. Instead of\u0026nbsp;running complex queries on\u0026nbsp;this large raw dataset every time, you can create materialized views to\u0026nbsp;pre-compute common aggregations:\u003c/p\u003e\n\n    \u003cdiv class=\"yfm-clipboard\"\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs sql\"\u003e\u003cspan class=\"hljs-keyword\"\u003eCREATE\u003c/span\u003e MATERIALIZED \u003cspan class=\"hljs-keyword\"\u003eVIEW\u003c/span\u003e daily_user_activity\nENGINE \u003cspan class=\"hljs-operator\"\u003e=\u003c/span\u003e SummingMergeTree()\n\u003cspan class=\"hljs-keyword\"\u003eORDER\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eBY\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003edate\u003c/span\u003e, user_id)\nPOPULATE\n\u003cspan class=\"hljs-keyword\"\u003eAS\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eSELECT\u003c/span\u003e\n    toDate(event_time) \u003cspan class=\"hljs-keyword\"\u003eAS\u003c/span\u003e \u003cspan class=\"hljs-type\"\u003edate\u003c/span\u003e,\n    user_id,\n    countIf(event_type \u003cspan class=\"hljs-operator\"\u003e=\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e'pageview'\u003c/span\u003e) \u003cspan class=\"hljs-keyword\"\u003eAS\u003c/span\u003e pageviews,\n    countIf(event_type \u003cspan class=\"hljs-operator\"\u003e=\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e'click'\u003c/span\u003e) \u003cspan class=\"hljs-keyword\"\u003eAS\u003c/span\u003e clicks,\n    countIf(event_type \u003cspan class=\"hljs-operator\"\u003e=\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e'purchase'\u003c/span\u003e) \u003cspan class=\"hljs-keyword\"\u003eAS\u003c/span\u003e purchases,\n    \u003cspan class=\"hljs-built_in\"\u003esum\u003c/span\u003e(purchase_amount) \u003cspan class=\"hljs-keyword\"\u003eAS\u003c/span\u003e total_spent\n\u003cspan class=\"hljs-keyword\"\u003eFROM\u003c/span\u003e raw_events\n\u003cspan class=\"hljs-keyword\"\u003eGROUP\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eBY\u003c/span\u003e \u003cspan class=\"hljs-type\"\u003edate\u003c/span\u003e, user_id;\n\n\u003c/code\u003e\u003c/pre\u003e\n\n    \u003csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\" class=\"yfm-clipboard-button\" data-animation=\"3\"\u003e\n        \u003cpath fill=\"currentColor\" d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\"\u003e\u003c/path\u003e\n        \u003cpath stroke=\"currentColor\" fill=\"transparent\" stroke-width=\"1.5\" d=\"M9.5 13l3 3l5 -5\" visibility=\"hidden\"\u003e\n            \u003canimate id=\"visibileAnimation-3\" attributeName=\"visibility\" from=\"hidden\" to=\"visible\" dur=\"0.2s\" fill=\"freeze\" begin\u003e\u003c/animate\u003e\n            \u003canimate id=\"hideAnimation-3\" attributeName=\"visibility\" from=\"visible\" to=\"hidden\" dur=\"1s\" begin=\"visibileAnimation-3.end+1\" fill=\"freeze\"\u003e\u003c/animate\u003e\n        \u003c/path\u003e\n    \u003c/svg\u003e\n    \u003c/div\u003e\n\u003cp\u003eThe view stores only the aggregated data, which updates automatically as\u0026nbsp;source data changes, significantly reducing the amount of\u0026nbsp;data scanned for common queries.\u003c/p\u003e\n\u003cp\u003eThe golden rule for materialized views is\u0026nbsp;to\u0026nbsp;understand your query performance. Materialized views perform best in\u0026nbsp;situations with frequent, expensive queries. In\u0026nbsp;particular, you want to\u0026nbsp;precompute expensive joins and aggregations. To\u0026nbsp;do\u0026nbsp;that, you can use \u003ca href=\"https://www.postgresql.org/docs/current/using-explain.html#USING-EXPLAIN-ANALYZE\"\u003eEXPLAIN ANALYZE\u003c/a\u003e to\u0026nbsp;understand query costs:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAnalyze execution time: Anything taking several seconds or\u0026nbsp;more could be\u0026nbsp;considered expensive, especially if\u0026nbsp;run frequently.\u003c/li\u003e\n\u003cli\u003eCheck for Seq Scans on\u0026nbsp;large tables: These are often indicators of\u0026nbsp;expensive operations.\u003c/li\u003e\n\u003cli\u003eCompare cost estimates: Higher costs suggest more expensive queries.\u003c/li\u003e\n\u003cli\u003eConsider the frequency of\u0026nbsp;the query: Even a\u0026nbsp;moderately expensive query can be\u0026nbsp;problematic if\u0026nbsp;run very often.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eMaterialized views are a\u0026nbsp;crucial layer in\u0026nbsp;your data transformation pipeline, enabling real-time analytics on\u0026nbsp;massive datasets while maintaining the flexibility to\u0026nbsp;access raw data when needed.\u003c/p\u003e\n\u003ch2 id=\"choose-dbt-when-quality-and-complexity-becomes-the-focus\"\u003e\u003ca href=\"#choose-dbt-when-quality-and-complexity-becomes-the-focus\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eChoose dbt when quality and complexity becomes the focus\u003c/span\u003e\u003c/a\u003eChoose dbt when quality and complexity becomes the focus\u003c/h2\u003e\n\u003cp\u003eAt\u0026nbsp;some point, manual transformation isn\u0026rsquo;t going to\u0026nbsp;cut\u0026nbsp;it. The rule of\u0026nbsp;thumb for when you need to\u0026nbsp;bring in\u0026nbsp;automation to\u0026nbsp;your transformations is\u0026nbsp;when:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eYou have more than five materialized views or\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ewhen data transformations become too complex to\u0026nbsp;manage manually.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThen, data quality tools such as\u0026nbsp;\u003ca href=\"https://www.getdbt.com/\"\u003edbt\u003c/a\u003e are needed. dbt is\u0026nbsp;an\u0026nbsp;open-source tool that enables data analysts and engineers to\u0026nbsp;transform data in\u0026nbsp;their warehouses more effectively. Engineers primarily use it\u0026nbsp;for more complex transformations and ensuring data quality.\u003c/p\u003e\n\u003cp\u003edbt brings a\u0026nbsp;more developer-like workflow to\u0026nbsp;data transformation. Instead of\u0026nbsp;individual SQL queries that need to\u0026nbsp;be\u0026nbsp;run manually for part of\u0026nbsp;an\u0026nbsp;ad-hoc transformation process, dbt brings structured queries and version control into the\u0026nbsp;mix. Engineers can use dbt to\u0026nbsp;create modular, reusable SQL code that defines how raw data should be\u0026nbsp;transformed into analytics-ready models.\u003c/p\u003e\n\u003cp\u003eIn\u0026nbsp;dbt, you break down complex transformations into smaller, more manageable pieces. This modular approach makes understanding, maintaining, and reusing transformation logic easier. Then, dbt includes built-in functionality for testing your data transformations. You can define tests to\u0026nbsp;ensure data quality, such as\u0026nbsp;checking for null values, unique constraints, or\u0026nbsp;custom business logic.\u003c/p\u003e\n\u003cp\u003eThese are the fundamentals we\u0026nbsp;mentioned for the ingestion phase. Even though you might only start using dbt once you have more complexity in\u0026nbsp;your pipelines, you should use it\u0026nbsp;throughout your pipeline once you hit that level of\u0026nbsp;complexity. \u003ca href=\"https://double.cloud/docs/en/managed-clickhouse/integrations/transform-data-in-clickhouse-with-dbt\"\u003edbt integrates with DoubleCloud\u0026rsquo;s Managed ClickHouse cluster\u003c/a\u003e to\u0026nbsp;do\u0026nbsp;precisely that, allowing you to\u0026nbsp;build a\u0026nbsp;seamless workflow from data ingestion to\u0026nbsp;transformation:\u003c/p\u003e\n\n\u003cp\u003eTo\u0026nbsp;get started with dbt in\u0026nbsp;your data transformation workflow:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1. Define your models:\u003c/strong\u003e Create SQL files that represent your data models, from staging tables that closely represent your raw data to\u0026nbsp;intermediate and final models that power your analytics.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2. Set up\u0026nbsp;tests:\u003c/strong\u003e Write simple YAML configurations to\u0026nbsp;test your data for nulls, uniqueness, accepted values, and custom business logic.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e3. Document your models:\u003c/strong\u003e Use YAML files and markdown to\u0026nbsp;document your models, making it\u0026nbsp;easier for others to\u0026nbsp;understand and use your transformed data.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e4. Create a\u0026nbsp;project structure:\u003c/strong\u003e Organize your dbt project with a\u0026nbsp;clear folder structure, separating staging models, intermediate models, and final models.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e5. Implement CI/CD:\u003c/strong\u003e Set up\u0026nbsp;continuous integration to\u0026nbsp;run your dbt models and tests automatically when changes are pushed to\u0026nbsp;your repository.\u003c/p\u003e\n\u003cp\u003eThe transition to\u0026nbsp;using dbt should be\u0026nbsp;gradual. Start with a\u0026nbsp;few key models, get comfortable with the workflow, and expand its use across your data transformation process. By\u0026nbsp;adopting dbt, you\u0026rsquo;re not just automating your transformations but bringing software engineering best practices to\u0026nbsp;your data work. This results in\u0026nbsp;more reliable, maintainable, and scalable data transformations, which is\u0026nbsp;crucial as\u0026nbsp;your data pipeline grows in\u0026nbsp;complexity.\u003c/p\u003e\n\u003ch2 id=\"use-airflow-for-the-most-complex-workflows\"\u003e\u003ca href=\"#use-airflow-for-the-most-complex-workflows\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eUse Airflow for the most complex workflows\u003c/span\u003e\u003c/a\u003eUse Airflow for the most complex workflows\u003c/h2\u003e\n\u003cp\u003eWith dbt, you can build workflows, but dbt itself doesn\u0026rsquo;t handle scheduling. For that, you need an\u0026nbsp;orchestration tool, such as\u0026nbsp;\u003ca href=\"https://airflow.apache.org/\"\u003eAirflow\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAirflow will allow you to\u0026nbsp;manage dependencies between tasks and schedule complex jobs, give you the flexibility to\u0026nbsp;handle diverse workflows, provide scalability for large-scale data processing, and provide robust error handling and monitoring. Airflow\u0026rsquo;s \u0026ldquo;secret sauce\u0026rdquo; is\u0026nbsp;the \u003ca href=\"https://double.cloud/docs/en/managed-airflow/concepts/dag\"\u003eDirected Acyclic Graph\u003c/a\u003e, or\u0026nbsp;DAG. Airflow leverages DAGs to\u0026nbsp;manage complex workflows and scheduling.\u003c/p\u003e\n\u003cp\u003eIn\u0026nbsp;Airflow, you define your workflow as\u0026nbsp;a\u0026nbsp;DAG using Python code. This lets you programmatically create a\u0026nbsp;structure representing your entire data pipeline or\u0026nbsp;process. Within a\u0026nbsp;DAG, you define individual tasks and their dependencies. Tasks can be\u0026nbsp;anything from running a\u0026nbsp;SQL query to\u0026nbsp;executing a\u0026nbsp;Python function or\u0026nbsp;interacting with an\u0026nbsp;external system. Dependencies determine the order in\u0026nbsp;which tasks should run.\u003c/p\u003e\n\u003cp\u003e(Source: \u003ca href=\"https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html\"\u003eApache Airflow\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003eEach DAG can have a\u0026nbsp;schedule associated with\u0026nbsp;it. This could be\u0026nbsp;as\u0026nbsp;simple as\u0026nbsp;\u0026ldquo;run daily at\u0026nbsp;midnight\u0026rdquo; or\u0026nbsp;as\u0026nbsp;complex as\u0026nbsp;\u0026ldquo;run every 15\u0026nbsp;minutes on\u0026nbsp;weekdays\u0026rdquo;. Airflow\u0026rsquo;s scheduler uses these definitions to\u0026nbsp;determine when to\u0026nbsp;trigger DAG runs. When it\u0026rsquo;s time for a\u0026nbsp;DAG to\u0026nbsp;run, Airflow\u0026rsquo;s executor starts processing tasks. It\u0026nbsp;begins with tasks with no\u0026nbsp;dependencies and moves through the graph as\u0026nbsp;tasks are completed, always respecting the defined dependencies.\u003c/p\u003e\n\u003cp\u003eBecause the workflow is\u0026nbsp;defined as\u0026nbsp;a\u0026nbsp;graph, Airflow can quickly identify which tasks can run in\u0026nbsp;parallel. This allows for efficient execution of\u0026nbsp;complex workflows. Airflow keeps track of\u0026nbsp;the state of\u0026nbsp;each task (running, success, failed, etc.). If\u0026nbsp;a\u0026nbsp;task fails, Airflow can automatically retry it\u0026nbsp;based on\u0026nbsp;your configuration.\u003c/p\u003e\n\u003cp\u003eThis approach allows Airflow to\u0026nbsp;handle everything from simple linear workflows to\u0026nbsp;complex branching processes with multiple dependencies and parallel execution paths. It\u0026rsquo;s ideal for the most complex data engineering scenarios where you might need to\u0026nbsp;orchestrate ETL processes, run machine learning pipelines, or\u0026nbsp;manage complex data transformation workflows.\u003c/p\u003e\n\u003ch2 id=\"5-tips-for-every-data-transformation\"\u003e\u003ca href=\"#5-tips-for-every-data-transformation\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003e5 tips for every data transformation\u003c/span\u003e\u003c/a\u003e5 tips for every data transformation\u003c/h2\u003e\n\u003cp\u003eWhile we\u0026rsquo;ve covered specific approaches for different stages of\u0026nbsp;data pipeline complexity, some overarching principles apply universally.\u003c/p\u003e\n\u003ch3 id=\"1-simplicity-and-maintainability-as-key-goals\"\u003e\u003ca href=\"#1-simplicity-and-maintainability-as-key-goals\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003e1. Simplicity and maintainability as\u0026nbsp;key goals\u003c/span\u003e\u003c/a\u003e1. Simplicity and maintainability as\u0026nbsp;key goals\u003c/h3\u003e\n\u003cp\u003eAlways strive for simplicity in\u0026nbsp;your transformations. Complex transformations might seem impressive, but they often lead to\u0026nbsp;headaches down the road. Here\u0026rsquo;s why simplicity matters:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEasier debugging:\u003c/strong\u003e When issues arise (and they will), more straightforward transformations are quicker to\u0026nbsp;troubleshoot.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eImproved maintainability:\u003c/strong\u003e As\u0026nbsp;your team evolves, simpler code is\u0026nbsp;easier for new team members to\u0026nbsp;understand and modify.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eReduced error risk:\u003c/strong\u003e Complex transformations introduce more points of\u0026nbsp;failure. Keeping things simple minimizes this risk.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eImplement this by\u0026nbsp;breaking down complex transformations into smaller, manageable steps. Use clear naming conventions and add comments to\u0026nbsp;explain the logic behind each transformation. Remember, the goal is\u0026nbsp;not just to\u0026nbsp;transform data but to\u0026nbsp;do\u0026nbsp;so\u0026nbsp;in\u0026nbsp;a\u0026nbsp;sustainable and scalable way as\u0026nbsp;your data needs grow.\u003c/p\u003e\n\u003ch3 id=\"2-denormalization-in-data-warehousing\"\u003e\u003ca href=\"#2-denormalization-in-data-warehousing\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003e2. Denormalization in\u0026nbsp;data warehousing\u003c/span\u003e\u003c/a\u003e2. Denormalization in\u0026nbsp;data warehousing\u003c/h3\u003e\n\u003cp\u003eIn\u0026nbsp;traditional database design, normalization is\u0026nbsp;king. But denormalization often takes the crown in\u0026nbsp;the world of\u0026nbsp;data warehousing and analytics. Here\u0026rsquo;s why:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eFaster queries:\u003c/strong\u003e Denormalized data requires fewer joins, significantly speeding up\u0026nbsp;query performance.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSimpler joins:\u003c/strong\u003e When joins are necessary, they\u0026rsquo;re typically simpler in\u0026nbsp;a\u0026nbsp;denormalized model.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eImproved read performance:\u003c/strong\u003e Analytics workloads are often read-heavy, and denormalization optimizes for reads.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eDenormalization isn\u0026rsquo;t without its trade-offs. You\u0026rsquo;re storing redundant data, which increases storage needs, and updating denormalized data requires careful management to\u0026nbsp;maintain consistency.\u003c/p\u003e\n\u003ch3 id=\"3-performance-considerations-for-real-time-analytics\"\u003e\u003ca href=\"#3-performance-considerations-for-real-time-analytics\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003e3. Performance considerations for real-time analytics\u003c/span\u003e\u003c/a\u003e3. Performance considerations for real-time analytics\u003c/h3\u003e\n\u003cp\u003eOptimizing your transformations for low-latency queries becomes crucial when building for real-time insights. As\u0026nbsp;discussed earlier, materialized views can significantly speed up\u0026nbsp;common queries, but that isn\u0026rsquo;t the only performance optimization you can make.\u003c/p\u003e\n\u003cp\u003eFirstly, you can consider your own data transformation strategy. Do\u0026nbsp;you need to\u0026nbsp;reprocess all data? Or\u0026nbsp;can you focus on\u0026nbsp;transforming only the new or\u0026nbsp;changed data?\u003c/p\u003e\n\u003cp\u003eSecond, can you take advantage of\u0026nbsp;tooling for better optimization? This might be\u0026nbsp;in-memory stores for the most time-sensitive operations or\u0026nbsp;properly partitioned and indexed data to\u0026nbsp;improve query performance.\u003c/p\u003e\n\u003cp\u003eRemember, real-time doesn\u0026rsquo;t always mean instantaneous. Understand your actual latency requirements and optimize accordingly. Sometimes, \u0026ldquo;near real-time\u0026rdquo; is\u0026nbsp;sufficient and much easier to\u0026nbsp;achieve.\u003c/p\u003e\n\u003ch3 id=\"4-security-and-compliance-throughout-the-transformation-process\"\u003e\u003ca href=\"#4-security-and-compliance-throughout-the-transformation-process\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003e4. Security and compliance throughout the transformation process\u003c/span\u003e\u003c/a\u003e4. Security and compliance throughout the transformation process\u003c/h3\u003e\n\u003cp\u003eData security isn\u0026rsquo;t just an\u0026nbsp;ingestion-level concern. It\u0026nbsp;needs to\u0026nbsp;be\u0026nbsp;baked into every stage of\u0026nbsp;your data pipeline. Here\u0026rsquo;s how:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eData encryption:\u003c/strong\u003e Ensure data is\u0026nbsp;encrypted at\u0026nbsp;rest and in\u0026nbsp;transit, even during transformation processes.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAccess controls:\u003c/strong\u003e Implement fine-grained access controls. Not everyone needs access to\u0026nbsp;all transformed data.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAudit trails:\u003c/strong\u003e Maintain detailed logs of\u0026nbsp;all data transformations for compliance and troubleshooting.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eData lineage:\u003c/strong\u003e Track the origin and transformations of\u0026nbsp;each data element. This is\u0026nbsp;crucial for both compliance and data governance.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"5-consider-the-end-use\"\u003e\u003ca href=\"#5-consider-the-end-use\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003e5. Consider the end-use\u003c/span\u003e\u003c/a\u003e5. Consider the end-use\u003c/h3\u003e\n\u003cp\u003eFinally, always consider the end-use of\u0026nbsp;your data. Your transformation strategy should be\u0026nbsp;tailored to\u0026nbsp;how the data will ultimately be\u0026nbsp;consumed.\u003c/p\u003e\n\u003cp\u003eDo\u0026nbsp;you have a\u0026nbsp;data contract or\u0026nbsp;SLA with downstream users that decides latency or\u0026nbsp;freshness? Establish clear data contracts with downstream users. These should define expectations for data quality, freshness, and availability. For instance, a\u0026nbsp;real-time dashboard might require 99.9% uptime and data no\u0026nbsp;older than 5\u0026nbsp;minutes, while a\u0026nbsp;monthly report might tolerate 24\u0026nbsp;hours of\u0026nbsp;lag.\u003c/p\u003e\n\u003cp\u003eRemember, the data team\u0026rsquo;s job is\u0026nbsp;to\u0026nbsp;ship insights, not data. Align your transformation strategy with key business metrics and decision-making processes. Understand which transformations directly support critical business KPIs and prioritize these. This approach transforms data engineering from a\u0026nbsp;support function into a\u0026nbsp;strategic driver of\u0026nbsp;business value, directly tying your technical decisions to\u0026nbsp;business outcomes and user satisfaction.\u003c/p\u003e\n\u003ch2 id=\"doubleclouds-approach-to-data-transformation\"\u003e\u003ca href=\"#doubleclouds-approach-to-data-transformation\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eDoubleCloud\u0026rsquo;s approach to\u0026nbsp;data transformation\u003c/span\u003e\u003c/a\u003eDoubleCloud\u0026rsquo;s approach to\u0026nbsp;data transformation\u003c/h2\u003e\n\u003cp\u003eEffective data transformation is\u0026nbsp;the cornerstone of\u0026nbsp;any successful data strategy. At\u0026nbsp;DoubleCloud, our approach is\u0026nbsp;designed around transformation to\u0026nbsp;meet the needs of\u0026nbsp;modern data teams, from simple ETL processes to\u0026nbsp;complex, real-time analytics pipelines.\u003c/p\u003e\n\u003cp\u003eIn\u0026nbsp;particular, \u003ca href=\"https://double.cloud/services/doublecloud-transfer/\"\u003eDoubleCloud Data Transfer\u003c/a\u003e is\u0026nbsp;designed to\u0026nbsp;be\u0026nbsp;a\u0026nbsp;simple data integration solution that excels at\u0026nbsp;extracting, loading, and transforming data. It\u0026nbsp;is\u0026nbsp;nine times faster than Airbyte and ideal for working with ClickHouse.\u003c/p\u003e\n\n\u003cp\u003eWith high-speed data ingestion into ClickHouse, Data Transfer can reach up\u0026nbsp;to\u0026nbsp;2\u0026nbsp;GB/s, making it\u0026nbsp;an\u0026nbsp;ideal choice for organizations looking to\u0026nbsp;build customer-facing or\u0026nbsp;real-time analytics solutions.\u003c/p\u003e\n\u003cp\u003eThis lets you move data in\u0026nbsp;any arrangement, giving you complete control over your data flow. This flexibility is\u0026nbsp;crucial for complex data transformation pipelines. DoubleClod Data Transfer natively integrates with dbt, allowing users to\u0026nbsp;quickly transform, clean, and summarize data for analysis. This integration enhances data modeling capabilities and improves team collaboration to\u0026nbsp;build out the data products your organization needs.\u003c/p\u003e\n\n\u003cp\u003eDoubleCloud\u0026rsquo;s approach to\u0026nbsp;data transformation aims to\u0026nbsp;provide a\u0026nbsp;comprehensive, efficient, and cost-effective solution for modern data teams. DoubleCloud Data Transfer is\u0026nbsp;designed to\u0026nbsp;meet data transformation needs while optimizing for performance and cost, whether you\u0026rsquo;re dealing with simple ETL processes or\u0026nbsp;complex, real-time analytics pipelines.\u003c/p\u003e\n\u003ch2 id=\"elevate-your-data-transformation-strategy-with-doublecloud\"\u003e\u003ca href=\"#elevate-your-data-transformation-strategy-with-doublecloud\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eElevate your data transformation strategy with DoubleCloud\u003c/span\u003e\u003c/a\u003eElevate your data transformation strategy with DoubleCloud\u003c/h2\u003e\n\u003cp\u003eData transformation is\u0026nbsp;a\u0026nbsp;strategic imperative for any organization looking to\u0026nbsp;harness the full power of\u0026nbsp;its data. From ingestion-level transformations to\u0026nbsp;complex orchestration with tools like Airflow, each stage of\u0026nbsp;your data pipeline requires careful consideration and planning.\u003c/p\u003e\n\u003cp\u003eThe best practices form the backbone of\u0026nbsp;a\u0026nbsp;robust data transformation strategy. By\u0026nbsp;implementing these practices, your data team can produce the insights the rest of\u0026nbsp;your organization requires. Building your data solutions on\u0026nbsp;DoubleCloud infrastructure gives you the ability to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eScale your data pipelines to\u0026nbsp;handle growing volumes and complexity\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eOptimize performance with ClickHouse-native tools and high-speed data ingestion\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eMaintain data quality and security throughout the transformation process\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eStreamline your workflow with built-in dbt integration for advanced transformations\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis is\u0026nbsp;all necessary for robust data transformation. You can get \u003ca href=\"https://auth.double.cloud/s/signup?_gl=1*19qneeh*_ga*NzU0OTMyNzI2LjE3MTk0MTg0OTE.*_ga_3G0X0VK41E*MTcyMzUxODEyMy4xNC4xLjE3MjM1MTg5NzcuNjAuMC4zODkxNTU0MDA.*_gcl_au*ODAxMjQyMTg2LjE3MTk0MTg0OTI.\"\u003estarted right away\u003c/a\u003e with DoubleCloud to\u0026nbsp;move your data, or\u0026nbsp;you can \u003ca href=\"https://double.cloud/#contact-us-form\"\u003ereach out to\u0026nbsp;the DoubleCloud team\u003c/a\u003e to\u0026nbsp;tell us\u0026nbsp;more about your data needs.\u003c/p\u003e\n"},"suggestedPosts":[{"url":"/blog/posts/2023/01/why-etl-pipelines-are-essential-for-businesses","id":49,"name":"why-etl-pipelines-are-essential-for-businesses","date":"2023-01-13T00:00:00Z","description":"","readingTime":10,"image":"/assets/blog/articles/why-etl-small-cover.png","blogPostId":49,"likes":0,"hasUserLike":false,"slug":"","title":"Why ETL pipelines are essential for businesses","authors":[],"tags":[],"locale":{"lang":"en"},"textTitle":"Why ETL pipelines are essential for businesses","htmlTitle":"Why ETL pipelines are essential for businesses"},{"url":"/blog/posts/2023/06/etl-vs-datapipelines","id":111,"name":"etl-vs-datapipelines","date":"2023-06-16T00:00:00Z","description":"","readingTime":10,"image":"/assets/blog/articles/etl_vs_data_pipelines_small.png","blogPostId":111,"likes":0,"hasUserLike":false,"slug":"","title":"ETL vs data pipelines: What are they and how do they work?","authors":[],"tags":[],"locale":{"lang":"en"},"textTitle":"ETL vs data pipelines: What are they and how do they work?","htmlTitle":"ETL vs data pipelines: What are they and how do they work?"},{"url":"/blog/posts/2023/05/what-is-data-pipeline","id":95,"name":"what-is-data-pipeline","date":"2023-05-12T00:00:00Z","description":"","readingTime":15,"image":"/assets/blog/articles/what-is-data-pipeline-small-cover.jpg","blogPostId":95,"likes":0,"hasUserLike":false,"slug":"","title":"What is data pipeline: A comprehensive guide","authors":[],"tags":[],"locale":{"lang":"en"},"textTitle":"What is data pipeline: A comprehensive guide","htmlTitle":"What is data pipeline: A comprehensive guide"}]}},"navigationData":{"newMenu":true,"header":{"leftItems":[{"text":"Why DoubleCloud","type":"dc-dropdown","data":{"view":"list","groups":[{"items":[{"text":"Performance","url":"/performance-boost/","description":"Get the best performance with the highest ROI"},{"text":"Security","url":"/security/","description":"Keep your data protected and maintain compliance"},{"text":"DoubleCloud vs. other solutions","url":"/comparison/","description":"Learn how DoubleCloud’s products compare to other solutions"},{"text":"Customer stories","url":"/resources/case-studies/","description":"See our solutions in action"}]},{"image":{"src":"/assets/doublecloud/menu-bar/menu-banner-dc-results.png.webp","style":{"width":300,"height":300}},"text":"\u003ca href='/performance-boost/' target='_self'\u003eGet more and spend less with DoubleCloud  →\u003c/a\u003e"}]}},{"text":"Products","type":"dc-dropdown","metaSchema":{"@graph":[{"@type":"SoftwareApplication","sameAs":["https://twitter.com/getdoublecloud","https://www.youtube.com/@doublecloud2499","https://www.linkedin.com/company/doublecloudplatform/","https://www.facebook.com/GetDoubleCloud/"]},{"@context":"https://schema.org","@type":"Organization","foundingDate":2022,"contactPoint":{"@type":"ContactPoint","contactType":"customer support","telephone":"+1 302-658-7581","email":"info@double.cloud"},"sameAs":["https://twitter.com/getdoublecloud","https://www.youtube.com/@doublecloud2499","https://www.linkedin.com/company/doublecloudplatform/","https://www.facebook.com/GetDoubleCloud/"]}]},"data":{"items":[{"text":"Managed Service for ClickHouse®","url":"/services/managed-clickhouse/","icon":"/assets/icons/dc-clickhouse.svg","description":"The fastest, most resource-efficient OLAP database for real-time analytics"},{"text":"Managed Service for Apache Kafka®","url":"/services/managed-kafka/","icon":"/assets/icons/dc-kafka.svg","description":"A leading data streaming technology for large-scale, data-intensive applications"},{"text":"Managed Service for Apache Airflow®","url":"/services/managed-airflow/","icon":"/assets/icons/dc-airflow.svg","description":"Open-source tool to orchestrate and monitor workflows"},{"text":"Data Transfer","url":"/services/doublecloud-transfer/","icon":"/assets/icons/dc-transfer.svg","description":"No-code ELT tool for aggregating, collecting, and migrating data"},{"text":"Data Visualization","url":"/services/doublecloud-visualization/","icon":"/assets/icons/dc-data-vis.svg","description":"Free tool to create, modify, and share dashboards and charts"}]}},{"text":"Solutions","type":"dc-dropdown","data":{"view":"list","groups":[{"title":"By use case","items":[{"text":"Customer-facing analytics","url":"/solutions/customer-facing-analytics/","description":"Provide business insights for your clients or partners"},{"text":"Real-time analytics","url":"/solutions/real-time-analytics/","description":"Build a data infrastructure to collect, process, and analyze data in real time"},{"text":"Observability and monitoring","url":"/solutions/observability-and-monitoring/","description":"Analyze terabytes of your logs, events, and traces with ease"}]},{"title":"By industry","items":[{"text":"AdTech and MarTech data analytics","url":"/solutions/adtech/","description":"Extract and analyze data from Meta ads, Google ads, LinkedIn ads, and others"},{"text":"Analytics for mobile and gaming apps","url":"/solutions/web-mobile-gaming-apps/","description":"Optimize and scale your mobile and gaming app analytics"},{"text":"EdTech data analytics","url":"/solutions/edtech/","description":"Improve online learning and identify new sales opportunities"},{"text":"FinTech data analytics","url":"/solutions/fintech-real-time-analytics/","description":"Manage and process large amounts of financial data efficiently"}]}]}},{"text":"Resources","type":"dc-dropdown","data":{"view":"list","groups":[{"title":"Using DoubleCloud","items":[{"text":"DoubleCloud API","url":"/docs/en/public-api/","description":"Read up on API tutorials and instructions","target":"_self"},{"text":"Terraform","url":"/docs/en/developer-resources/terraform/create-resources","description":"Deploy and manage cloud resources with the infrastructure-as-code approach"},{"text":"Status updates","url":"https://status.double.cloud/","description":"Check the current operational status of our services"},{"text":"Support","url":"/support/","description":"Learn more about our support tiers"}]},{"title":"Discover","items":[{"text":"Webinars","url":"/webinars/","description":"Sign up for the next webinar or watch previous ones"},{"text":"Blog","url":"/blog/","description":"Get insights from our team and the latest news"}]},{"image":{"src":"/assets/doublecloud/menu-bar/menu-banners-dc-ebook.png.webp","style":{"width":300,"height":300}},"text":"\u003ca href='/resources/clickhouse-ebook/' target='_self'\u003eGrab your ebook  →\u003c/a\u003e"}]}},{"text":"Company","type":"dc-dropdown","data":{"items":[{"text":"About DoubleCloud","url":"/company/about-us/"},{"text":"Careers","url":"/company/careers/"},{"text":"Contact us","url":"/company/contact-us/"}]}},{"text":"Pricing","url":"/pricing/"},{"text":"Documentation","url":"/docs/en/","target":"_self"}]},"logo":{"icon":"/assets/logo/dc-logo-dark.svg","text":""},"footer":{"underline":{"links":[{"text":"Customer Agreement","url":"/legal/en/customer_agreement/","target":"_blank"},{"text":"Privacy Policy","url":"/legal/en/privacy/","target":"_blank"},{"text":"Pricing","url":"/pricing/"},{"text":"Security","url":"/security/","target":"_blank"}],"copyright":"© 2024 DoubleCloud"},"columns":[{"title":"Products","links":[{"text":"Managed Service for ClickHouse®","url":"/services/managed-clickhouse/"},{"text":"Managed Service for Apache Kafka®","url":"/services/managed-kafka/"},{"text":"Managed Service for Apache Airflow®","url":"/services/managed-airflow"},{"text":"Data Transfer","url":"/services/doublecloud-transfer/"},{"text":"Data Visualization","url":"/services/doublecloud-visualization"}]},{"title":"Solutions","links":[{"text":"Case studies","url":"/resources/case-studies/"},{"text":"Customer-facing analytics","url":"/solutions/customer-facing-analytics/"},{"text":"Real-time analytics","url":"/solutions/real-time-analytics/"},{"text":"Observability and monitoring","url":"/solutions/observability-and-monitoring/"},{"text":"AdTech and MarTech data analytics","url":"/solutions/adtech/"},{"text":"Analytics for mobile and gaming Apps","url":"/solutions/web-mobile-gaming-apps/"},{"text":"EdTech data analytics","url":"/solutions/edtech/"},{"text":"FinTech data analytics","url":"/solutions/fintech-real-time-analytics/"}]},{"title":"Resources","links":[{"text":"Documentation","url":"/docs/en/"},{"text":"Webinars","url":"/webinars/"},{"text":"Blog","url":"/blog/"},{"text":"Support","url":"/support/"},{"text":"Status updates","url":"https://status.double.cloud/"},{"text":"Product comparisons","url":"/comparison/"},{"text":"Site map","url":"/sitemap/"}]},{"title":"Company","links":[{"text":"About DoubleCloud","url":"/company/about-us/"},{"text":"Careers","url":"/company/careers"},{"text":"AWS Partnership","url":"/aws-partnership/"}]}]},"forms":{"contact":"11819433.daba96b39df83b7903708cc4842e9dbb9c944cce"},"favicon":{"folder":"/assets/favicon"},"analytics":{"id":"GTM-5M39N8J","ignore":true,"popup":{"text":"\u003cp\u003eBy\u0026nbsp;clicking \u0026ldquo;Accept\u0026rdquo;, you agree to\u0026nbsp;the storing of\u0026nbsp;cookies on\u0026nbsp;your device to\u0026nbsp;help us\u0026nbsp;analyze site usage and assist in\u0026nbsp;our marketing efforts. However, you may \u0026ldquo;Decline\u0026rdquo; that. More details here in\u0026nbsp;\u003ca href=\"/legal/en/privacy/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e","buttons":{"accept":{"size":"xl","text":"Accept"},"decline":{"size":"xl","text":"Decline"}}}},"announcement":{"name":"webinar_announcement","url":"https://doublecloud-archive.github.io/blog/posts/2024/10/doublecloud-final-update/","conditions":{"date":{"start":"2000-01-01T00:00:00Z","end":"2099-12-31T00:00:00Z"}},"text":"\u003cp\u003e\u003cb\u003eDoubleCloud has wound down operations\u003c/b\u003e | This is\u0026nbsp;an\u0026nbsp;archived version of\u0026nbsp;the site. \u003cb\u003eLearn more \u0026rarr; \u003c/b\u003e\u003c/p\u003e"}},"meta":{"title":"Best practices for data transformation as pipeline complexity grows | DoubleCloud","date":"2024-08-20T00:00:00Z","image":"/assets/blog/articles/2024/best-practices-for-data-transformation-as-pipeline-complexity-grow-sharing.png","canonicalUrl":"","organization":{"appTitle":"DoubleCloud","legalName":"DoubleCloud Inc","supportEmail":"","url":"https://double.cloud"},"description":"Modern data platforms revolve around data transformation. There are different levels of transformation, and you must choose the appropriate tools for each level. Discover our experience and best practices.","content":"\u003cp\u003eQuick navigation:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#always-start-with-ingestion-level-transformations\"\u003eAlways start with ingestion-level transformations\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#move-to-materialized-views-as-data-starts-to-scale\"\u003eMove to\u0026nbsp;materialized views as\u0026nbsp;data starts to\u0026nbsp;scale\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#choose-dbt-when-quality-and-complexity-becomes-the-focus\"\u003eChoose dbt when quality and complexity becomes the focus\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#use-airflow-for-the-most-complex-workflows\"\u003eUse airflow for the most complex workflows\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#5-tips-for-every-data-transformation\"\u003e5 tips for every data transformation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#doubleclouds-approach-to-data-transformation\"\u003eDoubleCloud\u0026rsquo;s approach to\u0026nbsp;data transformation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#elevate-your-data-transformation-strategy-with-doublecloud\"\u003eElevate your data transformation strategy with DoubleCloud\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eModern data platforms revolve around data transformation.\u003c/p\u003e\n\u003cp\u003eThat is\u0026nbsp;the entire point of\u0026nbsp;data pipelines\u0026ndash;\u003cem\u003etransforming raw data into actionable insights\u003c/em\u003e. However, data transformation is\u0026nbsp;also a\u0026nbsp;gigantic process, ranging from simple cleaning and formatting to\u0026nbsp;complex aggregations and machine learning models.\u003c/p\u003e\n\u003cp\u003eThe correct transformation approach is\u0026nbsp;crucial for efficiency, scalability, and data integrity. If\u0026nbsp;you get this wrong, you risk creating a\u0026nbsp;bottleneck that can slow down your entire data operation, lead to\u0026nbsp;inconsistent or\u0026nbsp;unreliable results, and make it\u0026nbsp;difficult to\u0026nbsp;adapt to\u0026nbsp;changing business needs or\u0026nbsp;scale your data infrastructure.\u003c/p\u003e\n\u003cp\u003eIf\u0026nbsp;you get it\u0026nbsp;right, though, you can easily handle increasing data volumes and complexity, deliver consistent, high-quality insights across your team, and provide a\u0026nbsp;solid foundation for advanced analytics like AI\u0026nbsp;and machine learning.\u003c/p\u003e\n\u003cp\u003eHere, we\u0026nbsp;want to\u0026nbsp;explain how we\u0026nbsp;think about data transformation best practices at\u0026nbsp;DoubleCloud, concentrating on\u0026nbsp;each level of\u0026nbsp;transformation as\u0026nbsp;your pipeline complexity grows.\u003c/p\u003e\n\u003ch2 id=\"always-start-with-ingestion-level-transformations\"\u003e\u003ca href=\"#always-start-with-ingestion-level-transformations\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eAlways start with ingestion-level transformations\u003c/span\u003e\u003c/a\u003eAlways start with ingestion-level transformations\u003c/h2\u003e\n\u003cp\u003eStarting at\u0026nbsp;the start may seem obvious, but teams can easily skip this step. They ingest vast amounts of\u0026nbsp;data and then pump it\u0026nbsp;down the pipeline without considering the problems this will cause. These problems can be\u0026nbsp;segmented into two categories.\u003c/p\u003e\n\u003ch3 id=\"data-quality\"\u003e\u003ca href=\"#data-quality\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eData quality\u003c/span\u003e\u003c/a\u003eData quality\u003c/h3\u003e\n\u003cp\u003e\u0026ldquo;Garbage In, Garbage Out\u0026rdquo; is\u0026nbsp;a\u0026nbsp;famous aphorism in\u0026nbsp;data analytics that is\u0026nbsp;starting to\u0026nbsp;be\u0026nbsp;forgotten. With the advent of\u0026nbsp;AI\u0026nbsp;and machine learning, teams can get into the habit of\u0026nbsp;thinking that these models can easily work around or\u0026nbsp;take care of\u0026nbsp;any data quality issues.\u003c/p\u003e\n\u003cp\u003eBut the refrain continues to\u0026nbsp;be\u0026nbsp;true. The better the quality of\u0026nbsp;your data at\u0026nbsp;the beginning of\u0026nbsp;the pipeline, the better the quality of\u0026nbsp;your insights at\u0026nbsp;the\u0026nbsp;end. Here are some first-line-of-defense transformations that can make it\u0026nbsp;easier to\u0026nbsp;work with your data downstream:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1. Data type validation:\u003c/strong\u003e Incoming data must adhere to\u0026nbsp;expected data types, such as\u0026nbsp;converting strings to\u0026nbsp;appropriate numeric or\u0026nbsp;date formats, parsing JSON or\u0026nbsp;XML structures, or\u0026nbsp;casting boolean values to\u0026nbsp;a\u0026nbsp;standardized format. This reduces the need for error handling and type conversions in\u0026nbsp;the later stages of\u0026nbsp;the pipeline.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2. Null value handling:\u003c/strong\u003e You can deal with null or\u0026nbsp;missing values using \u003ca href=\"https://scikit-learn.org/stable/modules/impute.html\"\u003eimputation\u003c/a\u003e, \u003ca href=\"https://www.scaler.com/topics/data-science/categorical-missing-values/#:~:text=replace%20missing%20values.-,Flagging,-In%20some%20cases\"\u003eflagging\u003c/a\u003e, or\u0026nbsp;removal. Null value handling is\u0026nbsp;required to\u0026nbsp;ensure that downstream analysis and pipelines don\u0026rsquo;t break due to\u0026nbsp;unexpected null values.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e3. Deduplication:\u003c/strong\u003e Deduplication, removing duplicate records at\u0026nbsp;the ingestion stage, improves data quality and reduces storage costs. Though you can brute-force this at\u0026nbsp;scale, depending on\u0026nbsp;your data characteristics and business rules, you must consider fuzzy matching techniques or\u0026nbsp;libraries such as\u0026nbsp;\u003ca href=\"https://pypi.org/project/dedupe/1.6.5/\"\u003ededupe\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e4. Trimming and cleaning:\u003c/strong\u003e Remove leading/trailing whitespaces, control characters, or\u0026nbsp;other unwanted artifacts from string fields. This seemingly simple step can prevent numerous issues downstream, such as\u0026nbsp;unexpected behavior in\u0026nbsp;string comparisons or\u0026nbsp;joins.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e5. Schema validation:\u003c/strong\u003e This ensures the incoming data structure matches the expected schema, handling any discrepancies or\u0026nbsp;schema evolution. Robust \u003ca href=\"https://python-jsonschema.readthedocs.io/en/latest/validate/\"\u003eschema validation\u003c/a\u003e can prevent data inconsistencies. You might want to\u0026nbsp;implement versioning for your schemas to\u0026nbsp;ensure your pipeline can adapt to\u0026nbsp;evolving data structures without breaking.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e6. Referential integrity checks:\u003c/strong\u003e Verify that foreign key relationships are maintained, especially when ingesting data from multiple sources. This maintains logical consistency of\u0026nbsp;your data across different tables or\u0026nbsp;datasets. Implement checks that validate the existence of\u0026nbsp;referenced keys and handle scenarios where referential integrity might be\u0026nbsp;temporarily broken due to\u0026nbsp;the order of\u0026nbsp;data ingestion (essential in\u0026nbsp;\u003ca href=\"https://neon.tech/blog/postgres-roles#:~:text=Onto%20the%20more%20significant%20issue%3A%20migrations.\"\u003emigrations\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003eAll these are for a\u0026nbsp;single reason: improve data quality now to\u0026nbsp;reduce complexity later.\u003c/p\u003e\n\u003ch3 id=\"data-security\"\u003e\u003ca href=\"#data-security\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eData security\u003c/span\u003e\u003c/a\u003eData security\u003c/h3\u003e\n\u003cp\u003eWe\u0026nbsp;left data security checks out of\u0026nbsp;the list above because they require more discussion. You must deal with personal or\u0026nbsp;sensitive data at\u0026nbsp;the ingestion level. This is\u0026nbsp;not just a\u0026nbsp;best practice but often a\u0026nbsp;legal requirement under regulations like GDPR, CCPA, or\u0026nbsp;industry-specific standards like HIPAA.\u003c/p\u003e\n\u003cp\u003eHere are some key data security transformations to\u0026nbsp;consider at\u0026nbsp;the ingestion stage:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1. Data masking:\u003c/strong\u003e Apply masking techniques to\u0026nbsp;sensitive fields such as\u0026nbsp;personal identifiers, credit card numbers, or\u0026nbsp;health information. This involves replacing sensitive data with fictitious but realistic data maintaining the data\u0026rsquo;s format and consistency while protecting individual privacy.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2. Encryption:\u003c/strong\u003e Implement encryption for highly sensitive data fields. This could involve using robust encryption algorithms to\u0026nbsp;protect data at\u0026nbsp;rest and in\u0026nbsp;transit. Consider using format-preserving encryption for fields where the encrypted data needs to\u0026nbsp;maintain the original format.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e3. Tokenization:\u003c/strong\u003e Replace sensitive data elements with non-sensitive equivalents or\u0026nbsp;tokens. This is\u0026nbsp;particularly useful for fields that must maintain uniqueness without exposing the original data.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e4. Data anonymization:\u003c/strong\u003e For datasets used in\u0026nbsp;analytics or\u0026nbsp;machine learning, consider anonymization techniques that remove or\u0026nbsp;alter personally identifiable information while preserving the data\u0026rsquo;s analytical value.\u003c/p\u003e\n\u003ch2 id=\"move-to-materialized-views-as-data-starts-to-scale\"\u003e\u003ca href=\"#move-to-materialized-views-as-data-starts-to-scale\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eMove to\u0026nbsp;materialized views as\u0026nbsp;data starts to\u0026nbsp;scale\u003c/span\u003e\u003c/a\u003eMove to\u0026nbsp;materialized views as\u0026nbsp;data starts to\u0026nbsp;scale\u003c/h2\u003e\n\u003cp\u003eAs\u0026nbsp;data volume grows and query complexity increases, simple transformations are still necessary but no\u0026nbsp;longer sufficient.\u003c/p\u003e\n\u003cp\u003eAt\u0026nbsp;this point, data engineers start working with \u003ca href=\"https://double.cloud/docs/en/managed-clickhouse/glossary#materialized-view\"\u003ematerialized views\u003c/a\u003e, which are pre-computed result sets stored for faster query performance.\u003c/p\u003e\n\u003cp\u003e(Source: \u003ca href=\"https://clickhouse.com/docs/en/materialized-view\"\u003eClickHouse\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003eImagine running an\u0026nbsp;e-commerce platform that generates billions of\u0026nbsp;clickstream events daily. You\u0026rsquo;ll frequently need to\u0026nbsp;analyze user engagement and sales performance. Instead of\u0026nbsp;running complex queries on\u0026nbsp;this large raw dataset every time, you can create materialized views to\u0026nbsp;pre-compute common aggregations:\u003c/p\u003e\n\n    \u003cdiv class=\"yfm-clipboard\"\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs sql\"\u003e\u003cspan class=\"hljs-keyword\"\u003eCREATE\u003c/span\u003e MATERIALIZED \u003cspan class=\"hljs-keyword\"\u003eVIEW\u003c/span\u003e daily_user_activity\nENGINE \u003cspan class=\"hljs-operator\"\u003e=\u003c/span\u003e SummingMergeTree()\n\u003cspan class=\"hljs-keyword\"\u003eORDER\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eBY\u003c/span\u003e (\u003cspan class=\"hljs-type\"\u003edate\u003c/span\u003e, user_id)\nPOPULATE\n\u003cspan class=\"hljs-keyword\"\u003eAS\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eSELECT\u003c/span\u003e\n    toDate(event_time) \u003cspan class=\"hljs-keyword\"\u003eAS\u003c/span\u003e \u003cspan class=\"hljs-type\"\u003edate\u003c/span\u003e,\n    user_id,\n    countIf(event_type \u003cspan class=\"hljs-operator\"\u003e=\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e'pageview'\u003c/span\u003e) \u003cspan class=\"hljs-keyword\"\u003eAS\u003c/span\u003e pageviews,\n    countIf(event_type \u003cspan class=\"hljs-operator\"\u003e=\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e'click'\u003c/span\u003e) \u003cspan class=\"hljs-keyword\"\u003eAS\u003c/span\u003e clicks,\n    countIf(event_type \u003cspan class=\"hljs-operator\"\u003e=\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e'purchase'\u003c/span\u003e) \u003cspan class=\"hljs-keyword\"\u003eAS\u003c/span\u003e purchases,\n    \u003cspan class=\"hljs-built_in\"\u003esum\u003c/span\u003e(purchase_amount) \u003cspan class=\"hljs-keyword\"\u003eAS\u003c/span\u003e total_spent\n\u003cspan class=\"hljs-keyword\"\u003eFROM\u003c/span\u003e raw_events\n\u003cspan class=\"hljs-keyword\"\u003eGROUP\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eBY\u003c/span\u003e \u003cspan class=\"hljs-type\"\u003edate\u003c/span\u003e, user_id;\n\n\u003c/code\u003e\u003c/pre\u003e\n\n    \u003csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\" class=\"yfm-clipboard-button\" data-animation=\"3\"\u003e\n        \u003cpath fill=\"currentColor\" d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\"\u003e\u003c/path\u003e\n        \u003cpath stroke=\"currentColor\" fill=\"transparent\" stroke-width=\"1.5\" d=\"M9.5 13l3 3l5 -5\" visibility=\"hidden\"\u003e\n            \u003canimate id=\"visibileAnimation-3\" attributeName=\"visibility\" from=\"hidden\" to=\"visible\" dur=\"0.2s\" fill=\"freeze\" begin\u003e\u003c/animate\u003e\n            \u003canimate id=\"hideAnimation-3\" attributeName=\"visibility\" from=\"visible\" to=\"hidden\" dur=\"1s\" begin=\"visibileAnimation-3.end+1\" fill=\"freeze\"\u003e\u003c/animate\u003e\n        \u003c/path\u003e\n    \u003c/svg\u003e\n    \u003c/div\u003e\n\u003cp\u003eThe view stores only the aggregated data, which updates automatically as\u0026nbsp;source data changes, significantly reducing the amount of\u0026nbsp;data scanned for common queries.\u003c/p\u003e\n\u003cp\u003eThe golden rule for materialized views is\u0026nbsp;to\u0026nbsp;understand your query performance. Materialized views perform best in\u0026nbsp;situations with frequent, expensive queries. In\u0026nbsp;particular, you want to\u0026nbsp;precompute expensive joins and aggregations. To\u0026nbsp;do\u0026nbsp;that, you can use \u003ca href=\"https://www.postgresql.org/docs/current/using-explain.html#USING-EXPLAIN-ANALYZE\"\u003eEXPLAIN ANALYZE\u003c/a\u003e to\u0026nbsp;understand query costs:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAnalyze execution time: Anything taking several seconds or\u0026nbsp;more could be\u0026nbsp;considered expensive, especially if\u0026nbsp;run frequently.\u003c/li\u003e\n\u003cli\u003eCheck for Seq Scans on\u0026nbsp;large tables: These are often indicators of\u0026nbsp;expensive operations.\u003c/li\u003e\n\u003cli\u003eCompare cost estimates: Higher costs suggest more expensive queries.\u003c/li\u003e\n\u003cli\u003eConsider the frequency of\u0026nbsp;the query: Even a\u0026nbsp;moderately expensive query can be\u0026nbsp;problematic if\u0026nbsp;run very often.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eMaterialized views are a\u0026nbsp;crucial layer in\u0026nbsp;your data transformation pipeline, enabling real-time analytics on\u0026nbsp;massive datasets while maintaining the flexibility to\u0026nbsp;access raw data when needed.\u003c/p\u003e\n\u003ch2 id=\"choose-dbt-when-quality-and-complexity-becomes-the-focus\"\u003e\u003ca href=\"#choose-dbt-when-quality-and-complexity-becomes-the-focus\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eChoose dbt when quality and complexity becomes the focus\u003c/span\u003e\u003c/a\u003eChoose dbt when quality and complexity becomes the focus\u003c/h2\u003e\n\u003cp\u003eAt\u0026nbsp;some point, manual transformation isn\u0026rsquo;t going to\u0026nbsp;cut\u0026nbsp;it. The rule of\u0026nbsp;thumb for when you need to\u0026nbsp;bring in\u0026nbsp;automation to\u0026nbsp;your transformations is\u0026nbsp;when:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eYou have more than five materialized views or\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ewhen data transformations become too complex to\u0026nbsp;manage manually.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThen, data quality tools such as\u0026nbsp;\u003ca href=\"https://www.getdbt.com/\"\u003edbt\u003c/a\u003e are needed. dbt is\u0026nbsp;an\u0026nbsp;open-source tool that enables data analysts and engineers to\u0026nbsp;transform data in\u0026nbsp;their warehouses more effectively. Engineers primarily use it\u0026nbsp;for more complex transformations and ensuring data quality.\u003c/p\u003e\n\u003cp\u003edbt brings a\u0026nbsp;more developer-like workflow to\u0026nbsp;data transformation. Instead of\u0026nbsp;individual SQL queries that need to\u0026nbsp;be\u0026nbsp;run manually for part of\u0026nbsp;an\u0026nbsp;ad-hoc transformation process, dbt brings structured queries and version control into the\u0026nbsp;mix. Engineers can use dbt to\u0026nbsp;create modular, reusable SQL code that defines how raw data should be\u0026nbsp;transformed into analytics-ready models.\u003c/p\u003e\n\u003cp\u003eIn\u0026nbsp;dbt, you break down complex transformations into smaller, more manageable pieces. This modular approach makes understanding, maintaining, and reusing transformation logic easier. Then, dbt includes built-in functionality for testing your data transformations. You can define tests to\u0026nbsp;ensure data quality, such as\u0026nbsp;checking for null values, unique constraints, or\u0026nbsp;custom business logic.\u003c/p\u003e\n\u003cp\u003eThese are the fundamentals we\u0026nbsp;mentioned for the ingestion phase. Even though you might only start using dbt once you have more complexity in\u0026nbsp;your pipelines, you should use it\u0026nbsp;throughout your pipeline once you hit that level of\u0026nbsp;complexity. \u003ca href=\"https://double.cloud/docs/en/managed-clickhouse/integrations/transform-data-in-clickhouse-with-dbt\"\u003edbt integrates with DoubleCloud\u0026rsquo;s Managed ClickHouse cluster\u003c/a\u003e to\u0026nbsp;do\u0026nbsp;precisely that, allowing you to\u0026nbsp;build a\u0026nbsp;seamless workflow from data ingestion to\u0026nbsp;transformation:\u003c/p\u003e\n\n\u003cp\u003eTo\u0026nbsp;get started with dbt in\u0026nbsp;your data transformation workflow:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1. Define your models:\u003c/strong\u003e Create SQL files that represent your data models, from staging tables that closely represent your raw data to\u0026nbsp;intermediate and final models that power your analytics.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2. Set up\u0026nbsp;tests:\u003c/strong\u003e Write simple YAML configurations to\u0026nbsp;test your data for nulls, uniqueness, accepted values, and custom business logic.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e3. Document your models:\u003c/strong\u003e Use YAML files and markdown to\u0026nbsp;document your models, making it\u0026nbsp;easier for others to\u0026nbsp;understand and use your transformed data.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e4. Create a\u0026nbsp;project structure:\u003c/strong\u003e Organize your dbt project with a\u0026nbsp;clear folder structure, separating staging models, intermediate models, and final models.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e5. Implement CI/CD:\u003c/strong\u003e Set up\u0026nbsp;continuous integration to\u0026nbsp;run your dbt models and tests automatically when changes are pushed to\u0026nbsp;your repository.\u003c/p\u003e\n\u003cp\u003eThe transition to\u0026nbsp;using dbt should be\u0026nbsp;gradual. Start with a\u0026nbsp;few key models, get comfortable with the workflow, and expand its use across your data transformation process. By\u0026nbsp;adopting dbt, you\u0026rsquo;re not just automating your transformations but bringing software engineering best practices to\u0026nbsp;your data work. This results in\u0026nbsp;more reliable, maintainable, and scalable data transformations, which is\u0026nbsp;crucial as\u0026nbsp;your data pipeline grows in\u0026nbsp;complexity.\u003c/p\u003e\n\u003ch2 id=\"use-airflow-for-the-most-complex-workflows\"\u003e\u003ca href=\"#use-airflow-for-the-most-complex-workflows\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eUse Airflow for the most complex workflows\u003c/span\u003e\u003c/a\u003eUse Airflow for the most complex workflows\u003c/h2\u003e\n\u003cp\u003eWith dbt, you can build workflows, but dbt itself doesn\u0026rsquo;t handle scheduling. For that, you need an\u0026nbsp;orchestration tool, such as\u0026nbsp;\u003ca href=\"https://airflow.apache.org/\"\u003eAirflow\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAirflow will allow you to\u0026nbsp;manage dependencies between tasks and schedule complex jobs, give you the flexibility to\u0026nbsp;handle diverse workflows, provide scalability for large-scale data processing, and provide robust error handling and monitoring. Airflow\u0026rsquo;s \u0026ldquo;secret sauce\u0026rdquo; is\u0026nbsp;the \u003ca href=\"https://double.cloud/docs/en/managed-airflow/concepts/dag\"\u003eDirected Acyclic Graph\u003c/a\u003e, or\u0026nbsp;DAG. Airflow leverages DAGs to\u0026nbsp;manage complex workflows and scheduling.\u003c/p\u003e\n\u003cp\u003eIn\u0026nbsp;Airflow, you define your workflow as\u0026nbsp;a\u0026nbsp;DAG using Python code. This lets you programmatically create a\u0026nbsp;structure representing your entire data pipeline or\u0026nbsp;process. Within a\u0026nbsp;DAG, you define individual tasks and their dependencies. Tasks can be\u0026nbsp;anything from running a\u0026nbsp;SQL query to\u0026nbsp;executing a\u0026nbsp;Python function or\u0026nbsp;interacting with an\u0026nbsp;external system. Dependencies determine the order in\u0026nbsp;which tasks should run.\u003c/p\u003e\n\u003cp\u003e(Source: \u003ca href=\"https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html\"\u003eApache Airflow\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003eEach DAG can have a\u0026nbsp;schedule associated with\u0026nbsp;it. This could be\u0026nbsp;as\u0026nbsp;simple as\u0026nbsp;\u0026ldquo;run daily at\u0026nbsp;midnight\u0026rdquo; or\u0026nbsp;as\u0026nbsp;complex as\u0026nbsp;\u0026ldquo;run every 15\u0026nbsp;minutes on\u0026nbsp;weekdays\u0026rdquo;. Airflow\u0026rsquo;s scheduler uses these definitions to\u0026nbsp;determine when to\u0026nbsp;trigger DAG runs. When it\u0026rsquo;s time for a\u0026nbsp;DAG to\u0026nbsp;run, Airflow\u0026rsquo;s executor starts processing tasks. It\u0026nbsp;begins with tasks with no\u0026nbsp;dependencies and moves through the graph as\u0026nbsp;tasks are completed, always respecting the defined dependencies.\u003c/p\u003e\n\u003cp\u003eBecause the workflow is\u0026nbsp;defined as\u0026nbsp;a\u0026nbsp;graph, Airflow can quickly identify which tasks can run in\u0026nbsp;parallel. This allows for efficient execution of\u0026nbsp;complex workflows. Airflow keeps track of\u0026nbsp;the state of\u0026nbsp;each task (running, success, failed, etc.). If\u0026nbsp;a\u0026nbsp;task fails, Airflow can automatically retry it\u0026nbsp;based on\u0026nbsp;your configuration.\u003c/p\u003e\n\u003cp\u003eThis approach allows Airflow to\u0026nbsp;handle everything from simple linear workflows to\u0026nbsp;complex branching processes with multiple dependencies and parallel execution paths. It\u0026rsquo;s ideal for the most complex data engineering scenarios where you might need to\u0026nbsp;orchestrate ETL processes, run machine learning pipelines, or\u0026nbsp;manage complex data transformation workflows.\u003c/p\u003e\n\u003ch2 id=\"5-tips-for-every-data-transformation\"\u003e\u003ca href=\"#5-tips-for-every-data-transformation\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003e5 tips for every data transformation\u003c/span\u003e\u003c/a\u003e5 tips for every data transformation\u003c/h2\u003e\n\u003cp\u003eWhile we\u0026rsquo;ve covered specific approaches for different stages of\u0026nbsp;data pipeline complexity, some overarching principles apply universally.\u003c/p\u003e\n\u003ch3 id=\"1-simplicity-and-maintainability-as-key-goals\"\u003e\u003ca href=\"#1-simplicity-and-maintainability-as-key-goals\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003e1. Simplicity and maintainability as\u0026nbsp;key goals\u003c/span\u003e\u003c/a\u003e1. Simplicity and maintainability as\u0026nbsp;key goals\u003c/h3\u003e\n\u003cp\u003eAlways strive for simplicity in\u0026nbsp;your transformations. Complex transformations might seem impressive, but they often lead to\u0026nbsp;headaches down the road. Here\u0026rsquo;s why simplicity matters:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEasier debugging:\u003c/strong\u003e When issues arise (and they will), more straightforward transformations are quicker to\u0026nbsp;troubleshoot.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eImproved maintainability:\u003c/strong\u003e As\u0026nbsp;your team evolves, simpler code is\u0026nbsp;easier for new team members to\u0026nbsp;understand and modify.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eReduced error risk:\u003c/strong\u003e Complex transformations introduce more points of\u0026nbsp;failure. Keeping things simple minimizes this risk.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eImplement this by\u0026nbsp;breaking down complex transformations into smaller, manageable steps. Use clear naming conventions and add comments to\u0026nbsp;explain the logic behind each transformation. Remember, the goal is\u0026nbsp;not just to\u0026nbsp;transform data but to\u0026nbsp;do\u0026nbsp;so\u0026nbsp;in\u0026nbsp;a\u0026nbsp;sustainable and scalable way as\u0026nbsp;your data needs grow.\u003c/p\u003e\n\u003ch3 id=\"2-denormalization-in-data-warehousing\"\u003e\u003ca href=\"#2-denormalization-in-data-warehousing\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003e2. Denormalization in\u0026nbsp;data warehousing\u003c/span\u003e\u003c/a\u003e2. Denormalization in\u0026nbsp;data warehousing\u003c/h3\u003e\n\u003cp\u003eIn\u0026nbsp;traditional database design, normalization is\u0026nbsp;king. But denormalization often takes the crown in\u0026nbsp;the world of\u0026nbsp;data warehousing and analytics. Here\u0026rsquo;s why:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eFaster queries:\u003c/strong\u003e Denormalized data requires fewer joins, significantly speeding up\u0026nbsp;query performance.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSimpler joins:\u003c/strong\u003e When joins are necessary, they\u0026rsquo;re typically simpler in\u0026nbsp;a\u0026nbsp;denormalized model.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eImproved read performance:\u003c/strong\u003e Analytics workloads are often read-heavy, and denormalization optimizes for reads.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eDenormalization isn\u0026rsquo;t without its trade-offs. You\u0026rsquo;re storing redundant data, which increases storage needs, and updating denormalized data requires careful management to\u0026nbsp;maintain consistency.\u003c/p\u003e\n\u003ch3 id=\"3-performance-considerations-for-real-time-analytics\"\u003e\u003ca href=\"#3-performance-considerations-for-real-time-analytics\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003e3. Performance considerations for real-time analytics\u003c/span\u003e\u003c/a\u003e3. Performance considerations for real-time analytics\u003c/h3\u003e\n\u003cp\u003eOptimizing your transformations for low-latency queries becomes crucial when building for real-time insights. As\u0026nbsp;discussed earlier, materialized views can significantly speed up\u0026nbsp;common queries, but that isn\u0026rsquo;t the only performance optimization you can make.\u003c/p\u003e\n\u003cp\u003eFirstly, you can consider your own data transformation strategy. Do\u0026nbsp;you need to\u0026nbsp;reprocess all data? Or\u0026nbsp;can you focus on\u0026nbsp;transforming only the new or\u0026nbsp;changed data?\u003c/p\u003e\n\u003cp\u003eSecond, can you take advantage of\u0026nbsp;tooling for better optimization? This might be\u0026nbsp;in-memory stores for the most time-sensitive operations or\u0026nbsp;properly partitioned and indexed data to\u0026nbsp;improve query performance.\u003c/p\u003e\n\u003cp\u003eRemember, real-time doesn\u0026rsquo;t always mean instantaneous. Understand your actual latency requirements and optimize accordingly. Sometimes, \u0026ldquo;near real-time\u0026rdquo; is\u0026nbsp;sufficient and much easier to\u0026nbsp;achieve.\u003c/p\u003e\n\u003ch3 id=\"4-security-and-compliance-throughout-the-transformation-process\"\u003e\u003ca href=\"#4-security-and-compliance-throughout-the-transformation-process\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003e4. Security and compliance throughout the transformation process\u003c/span\u003e\u003c/a\u003e4. Security and compliance throughout the transformation process\u003c/h3\u003e\n\u003cp\u003eData security isn\u0026rsquo;t just an\u0026nbsp;ingestion-level concern. It\u0026nbsp;needs to\u0026nbsp;be\u0026nbsp;baked into every stage of\u0026nbsp;your data pipeline. Here\u0026rsquo;s how:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eData encryption:\u003c/strong\u003e Ensure data is\u0026nbsp;encrypted at\u0026nbsp;rest and in\u0026nbsp;transit, even during transformation processes.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAccess controls:\u003c/strong\u003e Implement fine-grained access controls. Not everyone needs access to\u0026nbsp;all transformed data.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAudit trails:\u003c/strong\u003e Maintain detailed logs of\u0026nbsp;all data transformations for compliance and troubleshooting.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eData lineage:\u003c/strong\u003e Track the origin and transformations of\u0026nbsp;each data element. This is\u0026nbsp;crucial for both compliance and data governance.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"5-consider-the-end-use\"\u003e\u003ca href=\"#5-consider-the-end-use\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003e5. Consider the end-use\u003c/span\u003e\u003c/a\u003e5. Consider the end-use\u003c/h3\u003e\n\u003cp\u003eFinally, always consider the end-use of\u0026nbsp;your data. Your transformation strategy should be\u0026nbsp;tailored to\u0026nbsp;how the data will ultimately be\u0026nbsp;consumed.\u003c/p\u003e\n\u003cp\u003eDo\u0026nbsp;you have a\u0026nbsp;data contract or\u0026nbsp;SLA with downstream users that decides latency or\u0026nbsp;freshness? Establish clear data contracts with downstream users. These should define expectations for data quality, freshness, and availability. For instance, a\u0026nbsp;real-time dashboard might require 99.9% uptime and data no\u0026nbsp;older than 5\u0026nbsp;minutes, while a\u0026nbsp;monthly report might tolerate 24\u0026nbsp;hours of\u0026nbsp;lag.\u003c/p\u003e\n\u003cp\u003eRemember, the data team\u0026rsquo;s job is\u0026nbsp;to\u0026nbsp;ship insights, not data. Align your transformation strategy with key business metrics and decision-making processes. Understand which transformations directly support critical business KPIs and prioritize these. This approach transforms data engineering from a\u0026nbsp;support function into a\u0026nbsp;strategic driver of\u0026nbsp;business value, directly tying your technical decisions to\u0026nbsp;business outcomes and user satisfaction.\u003c/p\u003e\n\u003ch2 id=\"doubleclouds-approach-to-data-transformation\"\u003e\u003ca href=\"#doubleclouds-approach-to-data-transformation\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eDoubleCloud\u0026rsquo;s approach to\u0026nbsp;data transformation\u003c/span\u003e\u003c/a\u003eDoubleCloud\u0026rsquo;s approach to\u0026nbsp;data transformation\u003c/h2\u003e\n\u003cp\u003eEffective data transformation is\u0026nbsp;the cornerstone of\u0026nbsp;any successful data strategy. At\u0026nbsp;DoubleCloud, our approach is\u0026nbsp;designed around transformation to\u0026nbsp;meet the needs of\u0026nbsp;modern data teams, from simple ETL processes to\u0026nbsp;complex, real-time analytics pipelines.\u003c/p\u003e\n\u003cp\u003eIn\u0026nbsp;particular, \u003ca href=\"https://double.cloud/services/doublecloud-transfer/\"\u003eDoubleCloud Data Transfer\u003c/a\u003e is\u0026nbsp;designed to\u0026nbsp;be\u0026nbsp;a\u0026nbsp;simple data integration solution that excels at\u0026nbsp;extracting, loading, and transforming data. It\u0026nbsp;is\u0026nbsp;nine times faster than Airbyte and ideal for working with ClickHouse.\u003c/p\u003e\n\n\u003cp\u003eWith high-speed data ingestion into ClickHouse, Data Transfer can reach up\u0026nbsp;to\u0026nbsp;2\u0026nbsp;GB/s, making it\u0026nbsp;an\u0026nbsp;ideal choice for organizations looking to\u0026nbsp;build customer-facing or\u0026nbsp;real-time analytics solutions.\u003c/p\u003e\n\u003cp\u003eThis lets you move data in\u0026nbsp;any arrangement, giving you complete control over your data flow. This flexibility is\u0026nbsp;crucial for complex data transformation pipelines. DoubleClod Data Transfer natively integrates with dbt, allowing users to\u0026nbsp;quickly transform, clean, and summarize data for analysis. This integration enhances data modeling capabilities and improves team collaboration to\u0026nbsp;build out the data products your organization needs.\u003c/p\u003e\n\n\u003cp\u003eDoubleCloud\u0026rsquo;s approach to\u0026nbsp;data transformation aims to\u0026nbsp;provide a\u0026nbsp;comprehensive, efficient, and cost-effective solution for modern data teams. DoubleCloud Data Transfer is\u0026nbsp;designed to\u0026nbsp;meet data transformation needs while optimizing for performance and cost, whether you\u0026rsquo;re dealing with simple ETL processes or\u0026nbsp;complex, real-time analytics pipelines.\u003c/p\u003e\n\u003ch2 id=\"elevate-your-data-transformation-strategy-with-doublecloud\"\u003e\u003ca href=\"#elevate-your-data-transformation-strategy-with-doublecloud\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eElevate your data transformation strategy with DoubleCloud\u003c/span\u003e\u003c/a\u003eElevate your data transformation strategy with DoubleCloud\u003c/h2\u003e\n\u003cp\u003eData transformation is\u0026nbsp;a\u0026nbsp;strategic imperative for any organization looking to\u0026nbsp;harness the full power of\u0026nbsp;its data. From ingestion-level transformations to\u0026nbsp;complex orchestration with tools like Airflow, each stage of\u0026nbsp;your data pipeline requires careful consideration and planning.\u003c/p\u003e\n\u003cp\u003eThe best practices form the backbone of\u0026nbsp;a\u0026nbsp;robust data transformation strategy. By\u0026nbsp;implementing these practices, your data team can produce the insights the rest of\u0026nbsp;your organization requires. Building your data solutions on\u0026nbsp;DoubleCloud infrastructure gives you the ability to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eScale your data pipelines to\u0026nbsp;handle growing volumes and complexity\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eOptimize performance with ClickHouse-native tools and high-speed data ingestion\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eMaintain data quality and security throughout the transformation process\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eStreamline your workflow with built-in dbt integration for advanced transformations\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis is\u0026nbsp;all necessary for robust data transformation. You can get \u003ca href=\"https://auth.double.cloud/s/signup?_gl=1*19qneeh*_ga*NzU0OTMyNzI2LjE3MTk0MTg0OTE.*_ga_3G0X0VK41E*MTcyMzUxODEyMy4xNC4xLjE3MjM1MTg5NzcuNjAuMC4zODkxNTU0MDA.*_gcl_au*ODAxMjQyMTg2LjE3MTk0MTg0OTI.\"\u003estarted right away\u003c/a\u003e with DoubleCloud to\u0026nbsp;move your data, or\u0026nbsp;you can \u003ca href=\"https://double.cloud/#contact-us-form\"\u003ereach out to\u0026nbsp;the DoubleCloud team\u003c/a\u003e to\u0026nbsp;tell us\u0026nbsp;more about your data needs.\u003c/p\u003e\n","sharing":{"title":"Best practices for data transformation as pipeline complexity grows","description":"","image":"/assets/blog/articles/2024/best-practices-for-data-transformation-as-pipeline-complexity-grow-sharing.png","shareGenImage":"","shareGenTitle":"Best practices for data transformation as pipeline complexity grows"},"keywords":[],"noIndex":false,"authors":[],"tags":[{"icon":null,"slug":"insights","name":"Insights","createdAt":"","updatedAt":"","count":0}],"metaSchema":{"@context":"https://schema.org","@graph":[{"@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"item":{"@id":"insights","name":"Insights"}}]},{"@type":"BlogPosting","@id":"https://double.cloud/blog/posts/2024/08/data-transformation-best-practices/","url":"https://double.cloud/blog/posts/2024/08/data-transformation-best-practices/","name":"Best practices for data transformation as pipeline complexity grows","headline":"Best practices for data transformation as pipeline complexity grows","abstract":"","description":"","dateCreated":"2024-08-20T00:00:00Z","datePublished":"2024-08-20T00:00:00Z","dateModified":"2024-08-20T00:00:00Z","author":{"@type":"Organization","name":"DoubleCloud","legalName":"DoubleCloud Inc","url":"https://double.cloud/","logo":{"@type":"ImageObject","url":"","width":32,"height":32}},"creator":{"@type":"Organization","name":"DoubleCloud","legalName":"DoubleCloud Inc","url":"https://double.cloud/","logo":{"@type":"ImageObject","url":"","width":32,"height":32}},"publisher":{"@type":"Organization","name":"DoubleCloud","legalName":"DoubleCloud Inc","url":"https://double.cloud/","logo":{"@type":"ImageObject","url":"","width":32,"height":32}},"copyrightHolder":{"@type":"Organization","name":"DoubleCloud","legalName":"DoubleCloud Inc","url":"https://double.cloud/","logo":{"@type":"ImageObject","url":"","width":32,"height":32}},"copyrightYear":2025,"mainEntityOfPage":{"@type":"WebPage","@id":"https://double.cloud/blog/posts/2024/08/data-transformation-best-practices/"},"inLanguage":{"@type":"Language","name":"English","alternateName":"en"},"keywords":["Insights"],"image":"https://double.cloud/assets/blog/articles/2024/best-practices-for-data-transformation-as-pipeline-complexity-grows-small-cover.png","sharedContent":{"@type":"WebPage","headline":"Best practices for data transformation as pipeline complexity grows","url":"https://double.cloud/blog/posts/2024/08/data-transformation-best-practices/","author":{"@type":"Organization","name":"DoubleCloud","legalName":"DoubleCloud Inc","url":"https://double.cloud/","logo":{"@type":"ImageObject","url":"","width":32,"height":32}}},"wordCount":"","articleBody":""}]}},"routingData":{"hostname":"double.cloud"},"deviceData":{"isRobot":true,"isMobile":false,"isTablet":false},"csrfToken":"GiewrQSn-91HdQSEjSCk3c1r5Zndl6CxuLKk","clientConfig":{"appTitle":"DoubleCloud","legalName":"DoubleCloud Inc","supportEmail":"","hosts":{"site":"https://double.cloud","console":"https://app.double.cloud"}},"ignoreConsent":false,"noNextImg":false,"noSnippet":null},"__N_SSP":true},"page":"/blog/posts/[...slug]","query":{"slug":["2024","08","data-transformation-best-practices"]},"buildId":"HkxA3M0ES7gp3V0n_0ecw","isFallback":false,"gssp":true,"locale":"en","locales":["en"],"defaultLocale":"en","scriptLoader":[]}</script></body></html>