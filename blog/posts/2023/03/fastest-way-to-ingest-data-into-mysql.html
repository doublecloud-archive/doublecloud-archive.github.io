<!DOCTYPE html><html lang="en"><head itemscope=""><script type="application/ld+json">{"@context":"https://schema.org","@graph":[{"@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"item":{"@id":"insights","name":"Insights"}},{"@type":"ListItem","position":2,"item":{"@id":"ClickHouse","name":"ClickHouse"}}]},{"@type":"BlogPosting","@id":"https://double.cloud/blog/posts/2023/03/fastest-way-to-ingest-data-into-mysql/","url":"https://double.cloud/blog/posts/2023/03/fastest-way-to-ingest-data-into-mysql/","name":"The fastest way to ingest data into MySQL is… ClickHouse®","headline":"The fastest way to ingest data into MySQL is… ClickHouse®","abstract":"<p>Written By: Stefan Kaeser, DoubleCloud Senior Solution Architect</p>","description":"<p>Written By: Stefan Kaeser, DoubleCloud Senior Solution Architect</p>","dateCreated":"2023-03-23T00:00:00Z","datePublished":"2023-03-23T00:00:00Z","dateModified":"2023-03-23T00:00:00Z","author":{"@type":"Organization","name":"DoubleCloud","legalName":"DoubleCloud Inc","url":"https://double.cloud/","logo":{"@type":"ImageObject","url":"","width":32,"height":32}},"creator":{"@type":"Organization","name":"DoubleCloud","legalName":"DoubleCloud Inc","url":"https://double.cloud/","logo":{"@type":"ImageObject","url":"","width":32,"height":32}},"publisher":{"@type":"Organization","name":"DoubleCloud","legalName":"DoubleCloud Inc","url":"https://double.cloud/","logo":{"@type":"ImageObject","url":"","width":32,"height":32}},"copyrightHolder":{"@type":"Organization","name":"DoubleCloud","legalName":"DoubleCloud Inc","url":"https://double.cloud/","logo":{"@type":"ImageObject","url":"","width":32,"height":32}},"copyrightYear":2025,"mainEntityOfPage":{"@type":"WebPage","@id":"https://double.cloud/blog/posts/2023/03/fastest-way-to-ingest-data-into-mysql/"},"inLanguage":{"@type":"Language","name":"English","alternateName":"en"},"keywords":["Insights","ClickHouse"],"image":"https://double.cloud/assets/blog/articles/fastest-way-to-ingest-data-into-mysql-small-cover.png","sharedContent":{"@type":"WebPage","headline":"The fastest way to ingest data into MySQL is… ClickHouse®","url":"https://double.cloud/blog/posts/2023/03/fastest-way-to-ingest-data-into-mysql/","author":{"@type":"Organization","name":"DoubleCloud","legalName":"DoubleCloud Inc","url":"https://double.cloud/","logo":{"@type":"ImageObject","url":"","width":32,"height":32}}},"wordCount":"","articleBody":""}]}</script><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Efficient data ingestion into MySQL with ClickHouse: streamlining 1 billion rows and local files | DoubleCloud</title><meta name="description" content="Explore efficient data ingestion into MySQL using ClickHouse. Learn how to streamline the process for 1 billion rows and local files. Discover a faster way to manage data overhead."/><link rel="canonical" href="fastest-way-to-ingest-data-into-mysql.html"/><meta itemProp="name" content="The fastest way to ingest data into MySQL is… ClickHouse®"/><meta itemProp="description" content="&lt;p&gt;Written By: Stefan Kaeser, DoubleCloud Senior Solution Architect&lt;/p&gt;"/><meta itemProp="image" content="https://double.cloud/assets/blog/articles/fastest-way-to-ingest-data-into-mysql-sharing.png"/><meta property="og:type" content="website"/><meta property="og:url" content="https://double.cloud/blog/posts/2023/03/fastest-way-to-ingest-data-into-mysql/"/><meta property="og:title" content="The fastest way to ingest data into MySQL is… ClickHouse®"/><meta property="og:description" content="&lt;p&gt;Written By: Stefan Kaeser, DoubleCloud Senior Solution Architect&lt;/p&gt;"/><meta property="og:image" content="https://double.cloud/assets/blog/articles/fastest-way-to-ingest-data-into-mysql-sharing.png"/><meta property="og:locale" content="en"/><meta property="og:site_name" content="DoubleCloud"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="The fastest way to ingest data into MySQL is… ClickHouse®"/><meta name="twitter:description" content="&lt;p&gt;Written By: Stefan Kaeser, DoubleCloud Senior Solution Architect&lt;/p&gt;"/><meta name="twitter:image" content="https://double.cloud/assets/blog/articles/fastest-way-to-ingest-data-into-mysql-sharing.png"/><meta name="robots" content="follow, index"/><meta property="article:published_time" content="2023-03-23T00:00:00Z"/><meta property="article:author" content=""/><meta property="article:tag" content="Insights"/><meta property="article:tag" content="ClickHouse"/><meta name="next-head-count" content="25"/><link rel="icon" href="../../../../assets/favicon/favicon.ico" sizes="any"/><link type="image/x-icon" rel="shortcut icon" href="../../../../assets/favicon/favicon.ico"/><link type="image/png" sizes="16x16" rel="icon" href="../../../../assets/favicon/favicon-16x16.png"/><link type="image/png" sizes="32x32" rel="icon" href="../../../../assets/favicon/favicon-32x32.png"/><link type="image/png" sizes="120x120" rel="icon" href="../../../../assets/favicon/favicon-120x120.png"/><link type="image/png" sizes="192x192" rel="icon" href="../../../../assets/favicon/favicon-192x192.png"/><link type="image/png" sizes="76x76" rel="apple-touch-icon" href="https://double.cloud/assets/favicon/favicon-76x76.png"/><link type="image/png" sizes="152x152" rel="apple-touch-icon" href="../../../../assets/favicon/favicon-152x152.png"/><link type="image/png" sizes="180x180" rel="apple-touch-icon" href="../../../../assets/favicon/favicon-180x180.png"/><script id="data-google-tag-manager" nonce="WtQiGkAvD9Bf8xAZrtOfhw==" data-nonce="WtQiGkAvD9Bf8xAZrtOfhw==">
                // Define dataLayer and the gtag function.
                window.dataLayer = window.dataLayer || [];
                function gtag(){dataLayer.push(arguments);}

                // Default analytics_storage to 'denied'.
                window.gtag = window.gtag || gtag;

                const hasAnalyticsConsent = window?.localStorage.getItem('hasAnalyticsConsent');
                const consent =  hasAnalyticsConsent === 'true' ? 'granted' : 'denied';

                window.gtag('consent', 'default', {
                    'analytics_storage': consent,
                    'ad_storage': consent,
                    'wait_for_update': hasAnalyticsConsent === 'true' ? 0 : Infinity,
                });

                dataLayer.push({
                    'event': 'default_consent'
                });

                (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
                j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
                'https://www.googletagmanager.com/gtm.js?id='+i+dl;var n=d.querySelector('[nonce]');
                n&&j.setAttribute('nonce',n.nonce||n.getAttribute('nonce'));f.parentNode.insertBefore(j,f);
                })(window,document,'script','dataLayer','GTM-5M39N8J');
            </script><script nonce="WtQiGkAvD9Bf8xAZrtOfhw==">window.__webpack_nonce__ = "WtQiGkAvD9Bf8xAZrtOfhw=="</script><link rel="preconnect" href="https://www.googletagmanager.com"/><link rel="preconnect" href="https://snap.licdn.com"/><link rel="preconnect" href="https://www.google.com"/><link nonce="WtQiGkAvD9Bf8xAZrtOfhw==" rel="preload" href="../../../../_next/static/css/a4c87e381fd61058.css" as="style"/><link nonce="WtQiGkAvD9Bf8xAZrtOfhw==" rel="stylesheet" href="../../../../_next/static/css/a4c87e381fd61058.css" data-n-g=""/><link nonce="WtQiGkAvD9Bf8xAZrtOfhw==" rel="preload" href="../../../../_next/static/css/2facd84af36bff2e.css" as="style"/><link nonce="WtQiGkAvD9Bf8xAZrtOfhw==" rel="stylesheet" href="../../../../_next/static/css/2facd84af36bff2e.css" data-n-p=""/><link nonce="WtQiGkAvD9Bf8xAZrtOfhw==" rel="preload" href="../../../../_next/static/css/c413166e8b0da734.css" as="style"/><link nonce="WtQiGkAvD9Bf8xAZrtOfhw==" rel="stylesheet" href="../../../../_next/static/css/c413166e8b0da734.css" data-n-p=""/><link nonce="WtQiGkAvD9Bf8xAZrtOfhw==" rel="preload" href="../../../../_next/static/css/248e88462928fa2f.css" as="style"/><link nonce="WtQiGkAvD9Bf8xAZrtOfhw==" rel="stylesheet" href="../../../../_next/static/css/248e88462928fa2f.css" data-n-p=""/><link nonce="WtQiGkAvD9Bf8xAZrtOfhw==" rel="preload" href="../../../../_next/static/css/eb8a627e7f585420.css" as="style"/><link nonce="WtQiGkAvD9Bf8xAZrtOfhw==" rel="stylesheet" href="../../../../_next/static/css/eb8a627e7f585420.css" data-n-p=""/><noscript data-n-css="WtQiGkAvD9Bf8xAZrtOfhw=="></noscript><script defer="" nonce="WtQiGkAvD9Bf8xAZrtOfhw==" nomodule="" src="../../../../_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="../../../../_next/static/chunks/webpack-d326a7489defa990.js" nonce="WtQiGkAvD9Bf8xAZrtOfhw==" defer=""></script><script src="../../../../_next/static/chunks/framework-cc7effedd0fd3d95.js" nonce="WtQiGkAvD9Bf8xAZrtOfhw==" defer=""></script><script src="../../../../_next/static/chunks/main-ebfff3515213fa2f.js" nonce="WtQiGkAvD9Bf8xAZrtOfhw==" defer=""></script><script src="../../../../_next/static/chunks/pages/_app-4cd98c5be1eceb26.js" nonce="WtQiGkAvD9Bf8xAZrtOfhw==" defer=""></script><script src="../../../../_next/static/chunks/f69bbb46-eed95df46583a2d8.js" nonce="WtQiGkAvD9Bf8xAZrtOfhw==" defer=""></script><script src="../../../../_next/static/chunks/030d571f-c7510aa4f8d650e7.js" nonce="WtQiGkAvD9Bf8xAZrtOfhw==" defer=""></script><script src="../../../../_next/static/chunks/193-fdb54e47dd6b7c7b.js" nonce="WtQiGkAvD9Bf8xAZrtOfhw==" defer=""></script><script src="../../../../_next/static/chunks/756-04d1c95c632019ed.js" nonce="WtQiGkAvD9Bf8xAZrtOfhw==" defer=""></script><script src="../../../../_next/static/chunks/387-27526d5e8e2a9173.js" nonce="WtQiGkAvD9Bf8xAZrtOfhw==" defer=""></script><script src="../../../../_next/static/chunks/pages/blog/posts/[...slug]-896d627301783262.js" nonce="WtQiGkAvD9Bf8xAZrtOfhw==" defer=""></script><script src="../../../../_next/static/HkxA3M0ES7gp3V0n_0ecw/_buildManifest.js" nonce="WtQiGkAvD9Bf8xAZrtOfhw==" defer=""></script><script src="../../../../_next/static/HkxA3M0ES7gp3V0n_0ecw/_ssgManifest.js" nonce="WtQiGkAvD9Bf8xAZrtOfhw==" defer=""></script></head><body class="dc-root g-root g-root_theme_dark"><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5M39N8J" title="Googletagmanager" height="0" width="0" style="display:none;visibility:hidden" loading="lazy"></iframe></noscript><div id="__next" data-reactroot=""><div class="layout"><a href="https://doublecloud-archive.github.io/blog/posts/2024/10/doublecloud-final-update/"><div class="pc-Grid header-anncouncement"><div class="container-fluid "><div class="row"><div class="col"><div class="yfm yfm_constructor"><p><b>DoubleCloud has wound down operations</b> | This is&nbsp;an&nbsp;archived version of&nbsp;the site. <b>Learn more &rarr; </b></p></div></div></div></div></div></a><div class="layout__content"><div class="g-root g-root_theme_dark pc-page-constructor"><div class="pc-page-constructor__wrapper"><div class="pc-layout"><div class="pc-Grid pc-navigation pc-layout__navigation"><div class="container-fluid "><div class="row"><div class="col"><nav><div class="pc-desktop-navigation__wrapper"><div class="pc-desktop-navigation__left"><div class="link" data-link-type="router"><span class="pc-logo pc-desktop-navigation__logo"><picture><img alt="Logo icon" src="../../../../assets/logo/dc-logo-dark.svg" class="pc-logo__icon"/></picture><span class="pc-logo__text"></span></span></div></div><div class="pc-desktop-navigation__navigation-container"><div class="pc-overflow-scroller__container"><div class="pc-overflow-scroller pc-desktop-navigation__navigation"><div class="pc-overflow-scroller__wrapper" style="left:0"><ul class="pc-desktop-navigation__links"><li class="pc-navigation-item pc-navigation-item_menu-layout_desktop pc-desktop-navigation__item"><button class="dc-dropdown-navigation-item__control"><span class="dc-dropdown-navigation-item__title">Why DoubleCloud</span></button><div style="position:fixed;left:0;top:0" class="g-popup dc-dropdown-navigation-item__dropdown"><div class="g-popup__content dc-dropdown-navigation-item__dropdown-content-wrapper" tabindex="-1"><div class="group-list-content"><div class="row item-list-content"><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Performance" data-link-type="router" href="../../../../performance-boost/index.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">Performance</span><span class="navigation-popup-item__description">Get the best performance with the highest ROI</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Security" data-link-type="router" href="../../../../security.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">Security</span><span class="navigation-popup-item__description">Keep your data protected and maintain compliance</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="DoubleCloud vs. other solutions" data-link-type="router" href="../../../../comparison/index.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">DoubleCloud vs. other solutions</span><span class="navigation-popup-item__description">Learn how DoubleCloud’s products compare to other solutions</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Customer stories" data-link-type="router" href="../../../../resources/case-studies/index.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">Customer stories</span><span class="navigation-popup-item__description">See our solutions in action</span></div></a></div></div><div class="group-list-content__banner"><picture><img alt="" src="../../../../assets/doublecloud/menu-bar/menu-banner-dc-results.png.webp" class="group-list-content__image" style="width:300px;height:300px"/></picture><span class="yfm yfm_constructor"><a href='../../../../performance-boost/index.html' target='_self'>Get more and spend less with DoubleCloud  →</a></span></div></div></div></div></li><li class="pc-navigation-item pc-navigation-item_menu-layout_desktop pc-desktop-navigation__item"><button class="dc-dropdown-navigation-item__control"><span class="dc-dropdown-navigation-item__title">Products</span></button><div style="position:fixed;left:0;top:0" class="g-popup dc-dropdown-navigation-item__dropdown"><div class="g-popup__content dc-dropdown-navigation-item__dropdown-content-wrapper" tabindex="-1"><div class="row item-list-content"><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Managed Service for ClickHouse®" data-link-type="router" href="../../../../services/managed-clickhouse.html"><picture class="navigation-popup-item__icon-container"><img alt="" src="../../../../assets/icons/dc-clickhouse.svg" class="navigation-popup-item__icon"/></picture><div class="navigation-popup-item__container navigation-popup-item__container_with-margin"><span class="navigation-popup-item__title">Managed Service for ClickHouse®</span><span class="navigation-popup-item__description">The fastest, most resource-efficient OLAP database for real-time analytics</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Managed Service for Apache Kafka®" data-link-type="router" href="../../../../services/managed-kafka.html"><picture class="navigation-popup-item__icon-container"><img alt="" src="../../../../assets/icons/dc-kafka.svg" class="navigation-popup-item__icon"/></picture><div class="navigation-popup-item__container navigation-popup-item__container_with-margin"><span class="navigation-popup-item__title">Managed Service for Apache Kafka®</span><span class="navigation-popup-item__description">A leading data streaming technology for large-scale, data-intensive applications</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Managed Service for Apache Airflow®" data-link-type="router" href="../../../../services/managed-airflow/index.html"><picture class="navigation-popup-item__icon-container"><img alt="" src="../../../../assets/icons/dc-airflow.svg" class="navigation-popup-item__icon"/></picture><div class="navigation-popup-item__container navigation-popup-item__container_with-margin"><span class="navigation-popup-item__title">Managed Service for Apache Airflow®</span><span class="navigation-popup-item__description">Open-source tool to orchestrate and monitor workflows</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Data Transfer" data-link-type="router" href="../../../../services/doublecloud-transfer.html"><picture class="navigation-popup-item__icon-container"><img alt="" src="../../../../assets/icons/dc-transfer.svg" class="navigation-popup-item__icon"/></picture><div class="navigation-popup-item__container navigation-popup-item__container_with-margin"><span class="navigation-popup-item__title">Data Transfer</span><span class="navigation-popup-item__description">No-code ELT tool for aggregating, collecting, and migrating data</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Data Visualization" data-link-type="router" href="../../../../services/doublecloud-visualization.html"><picture class="navigation-popup-item__icon-container"><img alt="" src="../../../../assets/icons/dc-data-vis.svg" class="navigation-popup-item__icon"/></picture><div class="navigation-popup-item__container navigation-popup-item__container_with-margin"><span class="navigation-popup-item__title">Data Visualization</span><span class="navigation-popup-item__description">Free tool to create, modify, and share dashboards and charts</span></div></a></div></div></div></div></li><li class="pc-navigation-item pc-navigation-item_menu-layout_desktop pc-desktop-navigation__item"><button class="dc-dropdown-navigation-item__control"><span class="dc-dropdown-navigation-item__title">Solutions</span></button><div style="position:fixed;left:0;top:0" class="g-popup dc-dropdown-navigation-item__dropdown"><div class="g-popup__content dc-dropdown-navigation-item__dropdown-content-wrapper" tabindex="-1"><div class="group-list-content"><div class="row item-list-content"><h4 class="item-list-content__title">By use case</h4><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Customer-facing analytics" data-link-type="router" href="../../../../customer-facing-analytics/index.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">Customer-facing analytics</span><span class="navigation-popup-item__description">Provide business insights for your clients or partners</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Real-time analytics" data-link-type="router" href="../../../../solutions/real-time-analytics/index.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">Real-time analytics</span><span class="navigation-popup-item__description">Build a data infrastructure to collect, process, and analyze data in real time</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Observability and monitoring" data-link-type="router" href="../../../../solutions/observability-and-monitoring/index.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">Observability and monitoring</span><span class="navigation-popup-item__description">Analyze terabytes of your logs, events, and traces with ease</span></div></a></div></div><div class="row item-list-content"><h4 class="item-list-content__title">By industry</h4><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="AdTech and MarTech data analytics" data-link-type="router" href="../../../../solutions/adtech.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">AdTech and MarTech data analytics</span><span class="navigation-popup-item__description">Extract and analyze data from Meta ads, Google ads, LinkedIn ads, and others</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Analytics for mobile and gaming apps" data-link-type="router" href="../../../../solutions/web-mobile-gaming-apps.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">Analytics for mobile and gaming apps</span><span class="navigation-popup-item__description">Optimize and scale your mobile and gaming app analytics</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="EdTech data analytics" data-link-type="router" href="../../../../solutions/edtech/index.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">EdTech data analytics</span><span class="navigation-popup-item__description">Improve online learning and identify new sales opportunities</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="FinTech data analytics" data-link-type="router" href="../../../../solutions/fintech-real-time-analytics/index.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">FinTech data analytics</span><span class="navigation-popup-item__description">Manage and process large amounts of financial data efficiently</span></div></a></div></div></div></div></div></li><li class="pc-navigation-item pc-navigation-item_menu-layout_desktop pc-desktop-navigation__item"><button class="dc-dropdown-navigation-item__control dc-dropdown-navigation-item__control_selected"><span class="dc-dropdown-navigation-item__title">Resources</span></button><div style="position:fixed;left:0;top:0" class="g-popup dc-dropdown-navigation-item__dropdown"><div class="g-popup__content dc-dropdown-navigation-item__dropdown-content-wrapper" tabindex="-1"><div class="group-list-content"><div class="row item-list-content"><h4 class="item-list-content__title">Using DoubleCloud</h4><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="DoubleCloud API" href="../../../../docs/en/public-api/index.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">DoubleCloud API</span><span class="navigation-popup-item__description">Read up on API tutorials and instructions</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Terraform" href="../../../../docs/en/developer-resources/terraform/create-resources.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">Terraform</span><span class="navigation-popup-item__description">Deploy and manage cloud resources with the infrastructure-as-code approach</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Status updates" data-link-type="router" href="https://status.double.cloud/"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">Status updates</span><span class="navigation-popup-item__description">Check the current operational status of our services</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Support" data-link-type="router" href="../../../../support/index.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">Support</span><span class="navigation-popup-item__description">Learn more about our support tiers</span></div></a></div></div><div class="row item-list-content"><h4 class="item-list-content__title">Discover</h4><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Webinars" data-link-type="router" href="../../../../webinars/index.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">Webinars</span><span class="navigation-popup-item__description">Sign up for the next webinar or watch previous ones</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover navigation-popup-item__content_selected" aria-label="Blog" data-link-type="router" href="../../../index.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">Blog</span><span class="navigation-popup-item__description">Get insights from our team and the latest news</span></div></a></div></div><div class="group-list-content__banner"><picture><img alt="" src="../../../../assets/doublecloud/menu-bar/menu-banners-dc-ebook.png.webp" class="group-list-content__image" style="width:300px;height:300px"/></picture><span class="yfm yfm_constructor"><a href='../../../../resources/clickhouse-ebook/index.html' target='_self'>Grab your ebook  →</a></span></div></div></div></div></li><li class="pc-navigation-item pc-navigation-item_menu-layout_desktop pc-desktop-navigation__item"><button class="dc-dropdown-navigation-item__control"><span class="dc-dropdown-navigation-item__title">Company</span></button><div style="position:fixed;left:0;top:0" class="g-popup dc-dropdown-navigation-item__dropdown"><div class="g-popup__content dc-dropdown-navigation-item__dropdown-content-wrapper" tabindex="-1"><div class="row item-list-content"><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="About DoubleCloud" data-link-type="router" href="../../../../company/about-us.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">About DoubleCloud</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Careers" data-link-type="router" href="../../../../company/careers.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">Careers</span></div></a></div><div class="col  col-12 navigation-popup-item item-list-content__item"><a class="navigation-popup-item__content navigation-popup-item__content_hover" aria-label="Contact us" data-link-type="router" href="../../../../company/contact-us.html"><div class="navigation-popup-item__container"><span class="navigation-popup-item__title">Contact us</span></div></a></div></div></div></div></li><li class="pc-navigation-item pc-navigation-item_menu-layout_desktop pc-desktop-navigation__item"><a aria-label="Pricing" class="pc-navigation-item__content pc-navigation-item__content_type_link" data-link-type="router" href="../../../../pricing.html"><div class="navigation-item"><span class="navigation-item__text">Pricing</span></div></a></li><li class="pc-navigation-item pc-navigation-item_menu-layout_desktop pc-desktop-navigation__item"><a href="../../../../docs/index.html" aria-label="Documentation" class="pc-navigation-item__content pc-navigation-item__content_type_link" target="_self"><div class="navigation-item"><span class="navigation-item__text">Documentation</span></div></a></li></ul></div></div></div></div><div class="pc-desktop-navigation__right"><button type="button" aria-label="Button label" class="pc-control pc-control_size_l pc-control_theme_primary pc-mobile-menu-button"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="24" height="24" class="g-icon" fill="currentColor" stroke="none" data-qa="icon-test-id" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 16 16"><path fill="currentColor" fill-rule="evenodd" d="M1.25 3.25A.75.75 0 0 1 2 2.5h12A.75.75 0 0 1 14 4H2a.75.75 0 0 1-.75-.75Zm0 4.75A.75.75 0 0 1 2 7.25h12a.75.75 0 0 1 0 1.5H2A.75.75 0 0 1 1.25 8ZM2 12a.75.75 0 0 0 0 1.5h12a.75.75 0 0 0 0-1.5H2Z" clip-rule="evenodd"></path></svg></svg></button></div></div><div></div></nav></div></div></div></div><main class="pc-layout__content"><section class="bc-wrapper bc-wrapper_padding-top_xs bc-wrapper_padding-bottom_l"><header class="pc-header-block pc-header-block_media-view_full"><div class="pc-header-block__background pc-header-block__background_media" style="background-color:#000000"><div class="pc-Media pc-header-block__background-media" style="background-color:#000000"><div style="transform:"><div class="pc-storage-background-image pc-media-component-image__item pc-header-block__image" data-qa="background-image"><picture data-qa="background-image-image"><img fetchpriority="high" alt="" src="../../../../assets/blog/articles/fastest-way-to-ingest-data-into-mysql-cover.png" class="pc-storage-background-image__img"/></picture></div></div></div></div><div class="pc-Grid"><div class="container-fluid pc-header-block__container-fluid"><div class="row pc-header-block__breadcrumbs"><div class="col"><div class="pc-header-breadcrumbs pc-header-breadcrumbs_theme_light" aria-label="You are here:"><div class="pc-header-breadcrumbs__item"><a href="../../../index.html" class="pc-header-breadcrumbs__text">Blog</a></div><div class="pc-header-breadcrumbs__item"><a href="../../../index.html%3Ftags=insights.html" class="pc-header-breadcrumbs__text">Insights</a></div></div></div></div><div class="row"><div class="col col-reset pc-header-block__content-wrapper"><div class="row"><div class="col pc-header-block__content pc-header-block__content_offset_default pc-header-block__content_theme_light pc-header-block__content_vertical-offset_l"><div class="col  col-lg-6 col-sm-12 col-md-8 col-12 pc-header-block__content-inner"><h1 class="pc-header-block__title" id="g-uniq-242415"><span>The fastest way to ingest data into MySQL is… ClickHouse®</span></h1><div class="pc-header-block__description"><div class="yfm yfm_constructor yfm_constructor_theme_light"><p>Written By: Stefan Kaeser, DoubleCloud Senior Solution Architect</p></div></div><div class="bc-post-info__container bc-post-info__container_theme_light"><div class="bc-post-info__item bc-post-info__item_size_s" data-qa="blog-header-meta-container-date">March 23, 2023</div><div class="bc-post-info__item bc-post-info__item_size_s" data-qa="blog-header-meta-container-reading-time"><span class="bc-post-info__icon"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="16" height="16" class="g-icon bc-post-info__icon-color" fill="currentColor" stroke="none" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 17" fill="currentColor" aria-hidden="true"><path fill-rule="evenodd" clip-rule="evenodd" d="M8 16.004a8 8 0 1 1 0-16 8 8 0 0 1 0 16Zm0-2a6 6 0 1 0 0-12 6 6 0 0 0 0 12Zm3.357-3.736a1 1 0 0 0-.342-1.372L9 7.688V5.004a1 1 0 0 0-2 0v3.25a1 1 0 0 0 .486.857l2.5 1.5a1 1 0 0 0 1.371-.343Z"></path></svg></svg></span>15 mins to read</div><div class="bc-post-info__item"><div class="bc-post-info__icon"><div class="g-popover gc-share-popover bc-post-info__share"><button class="gc-share-popover__container bc-post-info__switcher bc-post-info__switcher_theme_light" aria-expanded="false" aria-controls="g-uniq-242416" aria-describedby="g-uniq-242416"><div class="gc-share-popover__icon-container"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="16" height="16" class="g-icon gc-share-popover__icon bc-post-info__share-icon" fill="currentColor" stroke="none" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" fill="currentColor" aria-hidden="true"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.798 3.16a.5.5 0 0 0 .363.842H7V9a1 1 0 0 0 2 0V4.002h1.839a.5.5 0 0 0 .363-.844L8.363.156a.5.5 0 0 0-.726 0l-2.84 3.002.001.001ZM13 7a1 1 0 0 1 2 0v6.5a1.5 1.5 0 0 1-1.5 1.5h-11A1.5 1.5 0 0 1 1 13.5V7a1 1 0 0 1 2 0v6h10V7Z"></path></svg></svg></div><div class="gc-share-popover__title">Share</div></button></div></div></div></div></div></div></div></div></div></div></div></header></section><div class="pc-Grid"><div class="container-fluid "><div class="row pc-constructor-row"><div class="col"><div class="col col-reset pc-block-base pc-block-base_reset-paddings pc-block-base_indentTop_0 pc-block-base_indentBottom_0 pc-constructor-block pc-constructor-block_type_blog-layout-block"><section class="bc-wrapper bc-wrapper_padding-top_xs bc-wrapper_padding-bottom_xs"><div class="row bc-layout__row no-gutter"><div class="col  col-12 col-lg-8  order-3 order-lg-1 bc-layout__left-col"><div class="bc-layout__item"><div class="col col-reset pc-block-base pc-block-base_reset-paddings pc-block-base_indentTop_0 pc-block-base_indentBottom_0 pc-constructor-block pc-constructor-block_type_blog-yfm-block"><section class="bc-wrapper bc-wrapper_padding-top_xs bc-wrapper_padding-bottom_l"><div class="yfm yfm_blog yfm_reset_paddings yfm_no-list-reset"><p>MySQL.</p>
<p>It&rsquo;s often the first choice for start-up&rsquo;s looking for open-source software to&nbsp;store application data.</p>
<p>Now I&rsquo;ve been working with MySQL myself for over two decades, and although I&rsquo;ve seen and worked with other databases since then, it&nbsp;will always have a&nbsp;special place in&nbsp;my&nbsp;heart as&nbsp;it&nbsp;was the first one to&nbsp;show me&nbsp;how much fun working with SQL databases is.</p>
<p>Of&nbsp;course MySQL is&nbsp;an&nbsp;OLTP database, its strength is&nbsp;in&nbsp;serving multiple thousands of&nbsp;small queries and transactions, but it&rsquo;s still one of&nbsp;the most often used DBMS for general workloads as&nbsp;well.</p>
<p>As&nbsp;I&nbsp;mentioned, a&nbsp;lot of&nbsp;companies start with MySQL, building their app (s) around it, watching it&nbsp;grow, and even though MySQL definitely wasn&rsquo;t intended to&nbsp;be&nbsp;used as&nbsp;an&nbsp;analytical database, there are a&nbsp;lot of&nbsp;companies (and individuals) still out there using it&nbsp;for exactly this purpose.</p>
<p>And whilst there are a&nbsp;lot of&nbsp;good reasons why you shouldn&rsquo;t use MySQL for big analytical workloads, MySQL is&nbsp;still a&nbsp;lot more capable of&nbsp;analytical processes than a&nbsp;lot of&nbsp;people think it&nbsp;is.</p>
<p>I&rsquo;ve seen single MySQL instances having multiple Terabytes of&nbsp;data stored within itself, some with tables handling multiple billions of&nbsp;rows without bigger issues.</p>
<p>The concept of&nbsp;allowing different storage engines within the same environment allows MySQL to&nbsp;stretch over its own limits by&nbsp;several orders of&nbsp;magnitude.</p>
<p>I&rsquo;ve seen quite impressive analytical queries achieved with the use of&nbsp;TokuDB or&nbsp;RocksDB storage engines that I&nbsp;never thought MySQL would be&nbsp;able to&nbsp;fulfill.</p>
<p>Still, MySQL was never meant to&nbsp;be&nbsp;used for big analytical workloads, as&nbsp;it&nbsp;was designed and built back in&nbsp;a&nbsp;time when clouds were meant to&nbsp;be&nbsp;in&nbsp;the sky and formats of&nbsp;data were proprietary instead of&nbsp;being used for big open exchanges.</p>
<p>So&nbsp;one of&nbsp;the biggest problems when doing analytical stuff in&nbsp;MySQL is&nbsp;still&hellip; How do&nbsp;you get the data in&nbsp;it?</p>
<h1 id="1-billion-rows-to-ingest-what-to-do?"><a href="fastest-way-to-ingest-data-into-mysql.html#1-billion-rows-to-ingest-what-to-do?" class="yfm-anchor" aria-hidden="true"><span class="visually-hidden">1 billion rows to&nbsp;ingest&nbsp;&mdash; What to&nbsp;do?</span></a>1 billion rows to&nbsp;ingest&nbsp;&mdash; What to&nbsp;do?</h1>
<p>Lately I&rsquo;ve been running benchmarks on&nbsp;different DBMS.</p>
<p>The data set itself contained around 1&nbsp;billion rows of&nbsp;data, split into yearly chunks of&nbsp;around 10&nbsp;million rows, formatted in&nbsp;one line of&nbsp;JSON data per row, compressed via gzip, and located in&nbsp;a&nbsp;bucket in&nbsp;Amazon&rsquo;s S3.</p>
<p>Each row has 11&nbsp;columns, but for our benchmark we&nbsp;only need to&nbsp;extract 5&nbsp;of&nbsp;those: date, station_id, tempMax, tempMin, tempAvg</p>
<p>For this blog, I&nbsp;will limit our data set to&nbsp;only one year of&nbsp;data, choosing 2020&nbsp;at&nbsp;random.</p>
<p>The gzip compressed JSON data is&nbsp;98MB.</p>
<p>When decompressed the file size grows to&nbsp;2.7 GB, which is&nbsp;expected as&nbsp;the column names contained within the JSON are of&nbsp;course very compressible.</p>
<p>The total number of&nbsp;rows contained within the file is&nbsp;12,095,646.</p>
<p>For our benchmark we&nbsp;chose a&nbsp;small Amazon RDS instance with 2&nbsp;vCPU cores, 8GB of&nbsp;RAM and enough storage space to&nbsp;store the entire billion rows.</p>
<p>As&nbsp;we&nbsp;only want to&nbsp;show ingestion logic within this blog post, we&nbsp;limited the data set to&nbsp;2020, so&nbsp;the whole data set would fit into ram.</p>
<p>The table definition for our ingestion case is&nbsp;quite simple.</p>
<p>We&nbsp;have a&nbsp;primary key with 2&nbsp;columns (date, station_id) and a&nbsp;secondary index on&nbsp;station_id for a&nbsp;somewhat realistic scenario:</p>

    <div class="yfm-clipboard">
    <pre><code class="hljs">CREATE TABLE `sensor_data` (
  `station_id` char(11) CHARACTER SET ascii COLLATE ascii_bin NOT NULL,
  `date` date NOT NULL,
  `tempAvg` int DEFAULT NULL,
  `tempMax` int DEFAULT NULL,
  `tempMin` int DEFAULT NULL,
  PRIMARY KEY (`date`,`station_id`),
  KEY `station` (`station_id`)
) ENGINE=InnoDB;
</code></pre>

    <svg width="16" height="16" viewBox="0 0 24 24" class="yfm-clipboard-button" data-animation="69">
        <path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path>
        <path stroke="currentColor" fill="transparent" stroke-width="1.5" d="M9.5 13l3 3l5 -5" visibility="hidden">
            <animate id="visibileAnimation-69" attributeName="visibility" from="hidden" to="visible" dur="0.2s" fill="freeze" begin></animate>
            <animate id="hideAnimation-69" attributeName="visibility" from="visible" to="hidden" dur="1s" begin="visibileAnimation-69.end+1" fill="freeze"></animate>
        </path>
    </svg>
    </div>
<p>I&nbsp;chose InnoDB as&nbsp;the storage engine, as&nbsp;it&rsquo;s the most commonly used table engine in&nbsp;MySQL.</p>
<h3 id="how-to-start?"><a href="fastest-way-to-ingest-data-into-mysql.html#how-to-start?" class="yfm-anchor" aria-hidden="true"><span class="visually-hidden">How to&nbsp;start?</span></a>How to&nbsp;start?</h3>
<p>When trying to&nbsp;ingest our test data into MySQL is&nbsp;where we&nbsp;hit our first hurdle.</p>
<p>MySQL can&rsquo;t read directly from S3 or&nbsp;unzip the data on&nbsp;the&nbsp;fly. Therefore we&nbsp;have to&nbsp;do&nbsp;some manual steps, before we&nbsp;can even start:</p>
<p>Download the files: Depending on&nbsp;if&nbsp;you need these tasks regularly or&nbsp;just one time; this step could be&nbsp;easy or&nbsp;complex and the amount of&nbsp;time needed to&nbsp;implement that logic cannot be&nbsp;estimated in&nbsp;a&nbsp;general blog. Also the download speed depends on&nbsp;multiple factors, so&nbsp;we&nbsp;won&rsquo;t include it&nbsp;here in&nbsp;our comparison, just mention that it&nbsp;can have an&nbsp;impact.</p>
<p>Decompressing the files: The easiest way to&nbsp;achieve this is&nbsp;by&nbsp;using gzip -d {filename} on&nbsp;the cli.</p>
<p>Decompressing files of&nbsp;that size normally doesn&rsquo;t take much time, but still, we&nbsp;have to&nbsp;write the decompressed data to&nbsp;the disk. In&nbsp;my&nbsp;tests it&nbsp;took around 7&nbsp;seconds per file.</p>
<p>Now that we&nbsp;have the files locally and decompressed, we&nbsp;can really start to&nbsp;ingest the data.</p>
<p>We&nbsp;need to&nbsp;read the JSON data, get rid of&nbsp;the unwanted columns, and &ldquo;only&rdquo; ingest the 12&nbsp;million rows of&nbsp;data into MySQL.</p>
<h1 id="ingesting-local-files-to-mysql"><a href="fastest-way-to-ingest-data-into-mysql.html#ingesting-local-files-to-mysql" class="yfm-anchor" aria-hidden="true"><span class="visually-hidden">Ingesting local files to&nbsp;MySQL</span></a>Ingesting local files to&nbsp;MySQL</h1>
<h3 id="use-scripting-language-like-php"><a href="fastest-way-to-ingest-data-into-mysql.html#use-scripting-language-like-php" class="yfm-anchor" aria-hidden="true"><span class="visually-hidden">Use scripting language like PHP</span></a>Use scripting language like PHP</h3>
<p>A&nbsp;lot of&nbsp;environments which use MySQL as&nbsp;their main DBMS also make use of&nbsp;PHP (remember LAMP stack anyone?).</p>
<p>A&nbsp;simple solution to&nbsp;achieve our goal would be&nbsp;to&nbsp;read the JSON file one line at&nbsp;a&nbsp;time, use PHPs parsing functionality to&nbsp;get the values of&nbsp;the 5&nbsp;needed columns and insert them directly into MySQL.</p>
<p>Something like this:</p>

    <div class="yfm-clipboard">
    <pre><code class="hljs">$mysqli = new mysqli($mysql_host, $mysql_username, $mysql_password, '', $mysql_port);
$filename = $argv[1];

$handle = @fopen($filename, 'r');
if ($handle)
{
  while (($buffer = fgets($handle, 4096)) !== false)
  {
      $row = json_decode($buffer, true);
      $mysqli-&gt;query("
        INSERT IGNORE INTO test.sensor_data
          (station_id, date, tempMax, tempMin, tempAvg)
        VALUES
          ('{$row['station_id']}', '{$row['date']}', " .
              ($row['tempMax'] ?? 'NULL') . ", " . 
              ($row['tempMin'] ?? 'NULL') . ", " . 
              ($row['tempAvg'] ?? 'NULL') . ")  
      ");
  }
  fclose($handle);
}
exit(0);
</code></pre>

    <svg width="16" height="16" viewBox="0 0 24 24" class="yfm-clipboard-button" data-animation="112">
        <path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path>
        <path stroke="currentColor" fill="transparent" stroke-width="1.5" d="M9.5 13l3 3l5 -5" visibility="hidden">
            <animate id="visibileAnimation-112" attributeName="visibility" from="hidden" to="visible" dur="0.2s" fill="freeze" begin></animate>
            <animate id="hideAnimation-112" attributeName="visibility" from="visible" to="hidden" dur="1s" begin="visibileAnimation-112.end+1" fill="freeze"></animate>
        </path>
    </svg>
    </div>
<p>Doing it&nbsp;this way unfortunately has the overhead of&nbsp;executing a&nbsp;single transaction for each row of&nbsp;the dataset, hence only achieving around 60&nbsp;rows/second, or&nbsp;taking approx. 59&nbsp;hours for the 12&nbsp;million rows.</p>
<p>However, it&rsquo;s easy to&nbsp;optimize this logic by&nbsp;introducing the bunching of&nbsp;rows, combining 10k rows per insert query, speeding up&nbsp;the whole process to&nbsp;take 3&nbsp;minutes and 30&nbsp;seconds for the whole data set.</p>
<p>Ingestion speed is&nbsp;okay for the amount of&nbsp;data but it&nbsp;required a&nbsp;lot of&nbsp;manual work and no&nbsp;piece of&nbsp;error handling is&nbsp;written yet.</p>
<p>But there are ways to&nbsp;optimize data ingestion in&nbsp;MySQL.</p>
<h3 id="use-json-functionality-with-mysql"><a href="fastest-way-to-ingest-data-into-mysql.html#use-json-functionality-with-mysql" class="yfm-anchor" aria-hidden="true"><span class="visually-hidden">Use JSON functionality with MySQL</span></a>Use JSON functionality with MySQL</h3>
<p>Unfortunately MySQL has no&nbsp;feature for working with JSON files directly, but we&nbsp;can interpret the whole file as&nbsp;a&nbsp;CSV file, having only a&nbsp;single column containing a&nbsp;JSON string.</p>
<p>That way we&nbsp;can load the JSON file via the LOAD DATA command, and then select the needed columns via the built-in JSON functions of&nbsp;MySQL.</p>

    <div class="yfm-clipboard">
    <pre><code class="hljs">LOAD DATA LOCAL INFILE '~/MySQLBlog/sensors.2020.json'
INTO TABLE test.sensor_data  
(@json)
SET station_id = JSON_UNQUOTE(JSON_EXTRACT(@json, '$.station_id')),
  date = JSON_UNQUOTE(JSON_EXTRACT(@json, '$.date')),
  tempMax = nullif(JSON_UNQUOTE(JSON_EXTRACT(@json, '$.tempMax')), 'null'),
  tempMin = nullif(JSON_UNQUOTE(JSON_EXTRACT(@json, '$.tempMin')), 'null'),
  tempAvg = nullif(JSON_UNQUOTE(JSON_EXTRACT(@json, '$.tempAvg')), 'null')
</code></pre>

    <svg width="16" height="16" viewBox="0 0 24 24" class="yfm-clipboard-button" data-animation="134">
        <path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path>
        <path stroke="currentColor" fill="transparent" stroke-width="1.5" d="M9.5 13l3 3l5 -5" visibility="hidden">
            <animate id="visibileAnimation-134" attributeName="visibility" from="hidden" to="visible" dur="0.2s" fill="freeze" begin></animate>
            <animate id="hideAnimation-134" attributeName="visibility" from="visible" to="hidden" dur="1s" begin="visibileAnimation-134.end+1" fill="freeze"></animate>
        </path>
    </svg>
    </div>
<p>This approach took 4&nbsp;minutes and 55&nbsp;seconds, so&nbsp;even slower than the &lsquo;optimized&rsquo; PHP solution, but at&nbsp;least it&rsquo;s easier to&nbsp;type (At&nbsp;least for a&nbsp;SQL guy like myself).</p>
<p>The main problem with this approach taking so&nbsp;long though is&nbsp;that we&nbsp;still send the data of&nbsp;all the columns to&nbsp;the MySQL server, and then throw all but 5&nbsp;away within the server, hence having unneeded data sent over the wire to&nbsp;MySQL.</p>
<p>Also loading 10&nbsp;million rows in&nbsp;a&nbsp;single transaction issues quite a&nbsp;load to&nbsp;the server.</p>
<h3 id="use-cli-and-csv-functionality-with-mysql"><a href="fastest-way-to-ingest-data-into-mysql.html#use-cli-and-csv-functionality-with-mysql" class="yfm-anchor" aria-hidden="true"><span class="visually-hidden">Use Cli and CSV functionality with MySQL</span></a>Use Cli and CSV functionality with MySQL</h3>
<p>We&nbsp;can avoid using the JSON functionality of&nbsp;MySQL as&nbsp;well as&nbsp;the sending of&nbsp;unwanted data by&nbsp;adding another preparation step before the ingestion to&nbsp;MySQL.</p>
<p>We&nbsp;can use a&nbsp;command line JSON tool like jq&nbsp;to&nbsp;select only the needed columns and create a&nbsp;comma separated file, only containing the 5&nbsp;needed columns:</p>
<p>jq&nbsp;-r '. | [.station_id, .date, .tempMax, .tempMin, .tempAvg] | @csv' sensors.2020.json &gt; sensors_parsed.2020.csv</p>
<p>Parsing JSON to&nbsp;CSV on&nbsp;cli takes around 60&nbsp;seconds in&nbsp;that case, generating a&nbsp;CSV file with 376&nbsp;MB&nbsp;of&nbsp;data.</p>
<p>Now importing the csv file can be&nbsp;done quite easily within the MySQL client again:</p>

    <div class="yfm-clipboard">
    <pre><code class="hljs">LOAD DATA LOCAL INFILE '~/MySQLBlog/sensors_parsed.2020.csv'
INTO TABLE test.sensor_data  
FIELDS TERMINATED by ',' OPTIONALLY ENCLOSED BY '"'
(station_id, date, @tempMax, @tempMin, @tempAvg)
SET tempMax = nullif(@tempMax, ''),
  tempMin = nullif(@tempMin, ''),
  tempAvg = nullif(@tempAvg, '')
</code></pre>

    <svg width="16" height="16" viewBox="0 0 24 24" class="yfm-clipboard-button" data-animation="162">
        <path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path>
        <path stroke="currentColor" fill="transparent" stroke-width="1.5" d="M9.5 13l3 3l5 -5" visibility="hidden">
            <animate id="visibileAnimation-162" attributeName="visibility" from="hidden" to="visible" dur="0.2s" fill="freeze" begin></animate>
            <animate id="hideAnimation-162" attributeName="visibility" from="visible" to="hidden" dur="1s" begin="visibileAnimation-162.end+1" fill="freeze"></animate>
        </path>
    </svg>
    </div>
<p>This approach takes 1&nbsp;minute 52&nbsp;seconds, which is&nbsp;a&nbsp;lot faster than reading directly from JSON to&nbsp;MySQL.</p>
<p>We&nbsp;still, however, have the problem of&nbsp;big transactions. So&nbsp;if&nbsp;we&nbsp;want our MySQL instance to&nbsp;handle other requests without trouble, we&rsquo;d need to&nbsp;split the csv file into smaller chunks of&nbsp;10k rows to&nbsp;reduce the load on&nbsp;the server.</p>
<p>Thus we&nbsp;again need to&nbsp;write simple cli logic to&nbsp;split the csv into chunks and then write a&nbsp;loop to&nbsp;go&nbsp;over the 1,000 chunk files, ingesting one after another.</p>
<p>Overall, regarding the 60&nbsp;sec of&nbsp;cli time and the 1&nbsp;min 52&nbsp;sec for inserting the data, the total time to&nbsp;ingest our 12&nbsp;million JSON rows takes 2&nbsp;min 52&nbsp;sec, therefore being 40% faster than reading JSON directly from MySQL and still being 17% faster than the scripted solution.</p>
<h1 id="let-clickhouse-handle-all-the-overhead"><a href="fastest-way-to-ingest-data-into-mysql.html#let-clickhouse-handle-all-the-overhead" class="yfm-anchor" aria-hidden="true"><span class="visually-hidden">Let ClickHouse handle all the overhead</span></a>Let ClickHouse handle all the overhead</h1>
<p>Now even though we&nbsp;can reach a&nbsp;somewhat acceptable speed with cli tools and the mysql client, it&rsquo;s still not a&nbsp;very nice approach.</p>
<p>Also&hellip; imagine if&nbsp;you had to&nbsp;do&nbsp;this on&nbsp;a&nbsp;regular basis, as&nbsp;your software imports data from external sources on&nbsp;a&nbsp;nightly basis.</p>
<p>You&rsquo;d need to&nbsp;add checks for each step to&nbsp;make sure it&rsquo;s successfully finished and then clean up&nbsp;all the temporary data files afterwards again.</p>
<p>And if&nbsp;you need to&nbsp;integrate data from multiple sources, in&nbsp;multiple formats, it&nbsp;would get very ugly very quickly.</p>
<p>That&rsquo;s where ClickHouse can help you.</p>
<p>ClickHouse has extremely rich integration capabilities, reading from multiple sources (like s3, rest APIs, local files, external DBMS etc).</p>
<p>So&nbsp;even if&nbsp;you don&rsquo;t want to&nbsp;use ClickHouse as&nbsp;your daily driver for your analytical queries, you can still make use of&nbsp;its integration features.</p>
<p>I&nbsp;connected to&nbsp;a&nbsp;managed ClickHouse cluster within the DoubleCloud sitting in&nbsp;the same AWS region as&nbsp;my&nbsp;test MySQL instance, and then just executed this query:</p>

    <div class="yfm-clipboard">
    <pre><code class="hljs">SET external_storage_connect_timeout_sec = 300;
SET mysql_max_rows_to_insert = 10000;
INSERT INTO FUNCTION mysql('{mysqlurl}',
  test, sensor_data,
  '{username}', '{password}') 
(station_id, date, tempMax, tempMin, tempAvg)
SELECT station_id, date, tempMax, tempMin, tempAvg
FROM s3('https://{s3path}/sensors.2020.json.gz', '{aws_access_id}', '{aws_secret_access_id}', JSONEachRow,
  $$station_id String, date Date, tempMax Nullable(Int32), tempMin Nullable(Int32), tempAvg Nullable(Int32)$$)
</code></pre>

    <svg width="16" height="16" viewBox="0 0 24 24" class="yfm-clipboard-button" data-animation="202">
        <path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path>
        <path stroke="currentColor" fill="transparent" stroke-width="1.5" d="M9.5 13l3 3l5 -5" visibility="hidden">
            <animate id="visibileAnimation-202" attributeName="visibility" from="hidden" to="visible" dur="0.2s" fill="freeze" begin></animate>
            <animate id="hideAnimation-202" attributeName="visibility" from="visible" to="hidden" dur="1s" begin="visibileAnimation-202.end+1" fill="freeze"></animate>
        </path>
    </svg>
    </div>
<ul>
<li>The part between the $$ signs can be&nbsp;omitted in&nbsp;most use cases, as&nbsp;ClickHouse can do&nbsp;schema inference, but in&nbsp;my&nbsp;test data this was not possible</li>
</ul>
<p>The whole process of&nbsp;ingesting the data to&nbsp;MySQL now took 2&nbsp;minutes and 11&nbsp;sec, and as&nbsp;the inserts are automatically bunched to&nbsp;10k, the cpu of&nbsp;MySQL barely noticed anything.</p>
<p>But not only is&nbsp;it&nbsp;30% faster on&nbsp;the ingestion time, it&nbsp;also did the downloading and decompression step.</p>
<p>And even better, it&nbsp;saved me&nbsp;a&nbsp;lot of&nbsp;time implementing the whole logic!</p>
<p>ClickHouse does everything for me, I&nbsp;just had to&nbsp;execute a&nbsp;single query and wait for the result, making it&nbsp;a&nbsp;lot easier on&nbsp;the implementation side, to&nbsp;handle possible errors, cleanups (what cleanups?) etc.</p>
<p>And as&nbsp;a&nbsp;bonus, I&nbsp;also have the full tool set of&nbsp;SQL at&nbsp;my&nbsp;hands directly on&nbsp;insert already.</p>
<p>I&nbsp;could add WHERE conditions to&nbsp;filter out rows before inserting, can manipulate or&nbsp;parse columns, or&nbsp;I&nbsp;can even do&nbsp;aggregations before inserting the data to&nbsp;MySQL!</p>
<p>If&nbsp;anyone knows a&nbsp;faster way of&nbsp;getting data into MySQL, please contact me&nbsp;;).</p>
<h1 id="conclusion"><a href="fastest-way-to-ingest-data-into-mysql.html#conclusion" class="yfm-anchor" aria-hidden="true"><span class="visually-hidden">Conclusion</span></a>Conclusion</h1></div></section></div></div><div class="bc-layout__item"><div class="col col-reset pc-block-base pc-block-base_reset-paddings pc-block-base_indentTop_0 pc-block-base_indentBottom_0 pc-constructor-block pc-constructor-block_type_blog-media-block"><section class="bc-wrapper bc-wrapper_padding-top_xs bc-wrapper_padding-bottom_l bc-media__container"><div class="bc-media__border" data-qa="blog-media-content"><div class="pc-Media bc-media__content"><picture><source srcSet="../../../../assets/blog/articles/total-ingestion-time-my-sql.png.webp" type="image/webp"/><img alt="" src="../../../../assets/blog/articles/total-ingestion-time-my-sql.png" class="pc-media-component-image__item bc-media__image"/></picture></div></div></section></div></div></div><div class="col  col-12 col-lg-3  offset-0 offset-lg-1  order-2 order-lg-2 bc-layout__right-col"><div class="bc-layout__item"><div class="col col-reset pc-block-base pc-block-base_reset-paddings pc-block-base_indentTop_0 pc-block-base_indentBottom_0 pc-constructor-block pc-constructor-block_type_blog-yfm-block"><section class="bc-wrapper bc-wrapper_padding-top_xs bc-wrapper_padding-bottom_l"><div class="yfm yfm_blog yfm_reset_paddings yfm_no-list-reset"><p>In&nbsp;this article, we&rsquo;ll talk about:</p>
<ul>
<li><a href="fastest-way-to-ingest-data-into-mysql.html#1-billion-rows-to-ingest-what-to-do?">1 billion rows to&nbsp;ingest&nbsp;&mdash; What to&nbsp;do?</a></li>
<li><a href="fastest-way-to-ingest-data-into-mysql.html#ingesting-local-files-to-mysql">Ingesting local files to&nbsp;MySQL</a></li>
<li><a href="fastest-way-to-ingest-data-into-mysql.html#let-clickhouse-handle-all-the-overhead">Let ClickHouse handle all the overhead</a></li>
<li><a href="fastest-way-to-ingest-data-into-mysql.html#conclusion">Conclusion</a></li>
</ul></div></section></div></div></div></div></section></div><div class="col col-reset pc-block-base pc-block-base_reset-paddings pc-block-base_indentTop_0 pc-block-base_indentBottom_0 pc-constructor-block pc-constructor-block_type_blog-layout-block"><section class="bc-wrapper bc-wrapper_padding-top_xs bc-wrapper_padding-bottom_xs"><div class="row bc-layout__row no-gutter"><div class="col  col-12 col-lg-8  order-3 order-lg-1 bc-layout__left-col"><div class="bc-layout__item"><div class="col col-reset pc-block-base pc-block-base_reset-paddings pc-block-base_indentTop_0 pc-block-base_indentBottom_0 pc-constructor-block pc-constructor-block_type_blog-yfm-block"><section class="bc-wrapper bc-wrapper_padding-top_xs bc-wrapper_padding-bottom_l"><div class="yfm yfm_blog yfm_reset_paddings yfm_no-list-reset"><p>As&nbsp;we&nbsp;can see, utilizing ClickHouse for ingesting data to&nbsp;MySQL has been the fastest way to&nbsp;do&nbsp;so.</p>
<p>ClickHouse can not only spare you time if&nbsp;you use it&nbsp;as&nbsp;DBMS for your analytical queries but it&nbsp;can also spare you a&nbsp;lot of&nbsp;implementation time.</p>
<p>With ClickHouse it&rsquo;s extremely easy to&nbsp;convert data into different formats, reading from multiple sources (s3 was just an&nbsp;example here, you can utilize RestAPIs, other DBMS, local files, Kafka, etc.), and doing data manipulation on&nbsp;the fly.</p>
<p>So&nbsp;especially if&nbsp;you have multiple sources of&nbsp;data, it&nbsp;makes the whole process of&nbsp;collecting the data, downloading, converting etc. a&nbsp;lot easier.</p>
<p>You just need to&nbsp;rewrite a&nbsp;single query for echo of&nbsp;your ingestion process, instead of&nbsp;rewriting a&nbsp;whole bunch of&nbsp;commands for downloading, preparing inserting etc.</p>
<p>In&nbsp;the end you can still utilize your MySQL instances for all your preferred workloads, and still let ClickHouse help you be&nbsp;faster overall!</p>
<p>And this is&nbsp;the reason why as&nbsp;a&nbsp;tech guy, I&nbsp;fell in&nbsp;love with ClickHouse.</p>
<p>It&rsquo;s not because it&rsquo;s fast as&nbsp;hell in&nbsp;analytical workloads (although it&nbsp;is). It&rsquo;s because it&rsquo;s so&nbsp;extremely versatile to&nbsp;use, from prototypes to&nbsp;production workloads or&nbsp;just for having fun with.</p>
<p>ClickHouse is&nbsp;a&nbsp;trademark of&nbsp;ClickHouse, Inc. https://clickhouse.com</p></div></section></div></div></div><div class="col  col-12 col-lg-3  offset-0 offset-lg-1  order-2 order-lg-2 bc-layout__right-col"></div></div></section></div><div class="col col-reset pc-block-base pc-block-base_indentTop_l pc-block-base_indentBottom_l pc-constructor-block pc-constructor-block_type_content-layout-block"><div class="pc-content-layout-block pc-content-layout-block_size_l pc-content-layout-block_theme_default pc-content-layout-block_background"><div class="col  col-12 col-md-8 col-reset pc-content pc-content_size_l pc-content_centered pc-content_theme_default pc-content-layout-block__content"><div class="pc-title pc-content__title" id="g-uniq-242418"><div class="col  col-12 col-reset"><h2 class="pc-title-item pc-title-item_size_m pc-title-item_reset-margin" data-qa="undefined-header"><span class="pc-title-item__text">Start your trial today</span></h2></div></div><div class="pc-buttons pc-buttons_size_l pc-content__buttons pc-content__buttons_size_l"><a aria-describedby="g-uniq-242418" class="g-button g-button_view_action g-button_size_xl g-button_pin_round-round pc-button-block pc-button-block_size_xl pc-button-block_theme_accent pc-buttons__button" href="https://auth.double.cloud/s/signup" aria-disabled="false"><span class="g-button__text"><span class="pc-button-block__content"><span class="pc-button-block__text">Start free trial</span></span></span></a><a aria-describedby="g-uniq-242418" class="g-button g-button_view_outlined g-button_size_xl g-button_pin_round-round pc-button-block pc-button-block_size_xl pc-button-block_theme_pseudo pc-buttons__button" href="fastest-way-to-ingest-data-into-mysql.html#contact-us-form" aria-disabled="false"><span class="g-button__text"><span class="pc-button-block__content"><span class="pc-button-block__text">Contact us</span></span></span></a></div></div><div class="pc-content-layout-block__background"><div class="pc-storage-background-image pc-content-layout-block__background-item" style="background-color:#CA1551" data-qa="background-image"><picture data-qa="background-image-image"><source srcSet="../../../../assets/doublecloud/doublecloud-cover-1.png.webp" type="image/webp" data-qa="background-image-image-desktop-source-compressed"/><img alt="" src="../../../../assets/doublecloud/doublecloud-cover-1.png" class="pc-storage-background-image__img" style="background-color:#CA1551"/></picture></div></div></div></div><div class="col col-reset pc-block-base pc-block-base_reset-paddings pc-block-base_indentTop_0 pc-block-base_indentBottom_0 pc-constructor-block pc-constructor-block_type_blog-suggest-block"><section class="bc-wrapper bc-wrapper_padding-top_l bc-wrapper_padding-bottom_l"><div class="pc-SliderBlock"><div class="pc-title pc-SliderBlock__header pc-SliderBlock__header_no-description"><div class="col  col-12 col-sm-8 col-reset"><h2 class="pc-title-item pc-title-item_size_m pc-title-item_reset-margin" data-qa="undefined-header"><span class="pc-title-item__text">See also</span></h2></div></div><div class="pc-SliderBlock__animate-slides"><span style="font-size:0"></span><div><div class="slick-slider pc-slick-origin slick-initialized"><div class="slick-list"><div class="slick-track" style="width:100%;left:0%"><div data-index="0" class="slick-slide slick-active slick-current" tabindex="-1" aria-hidden="false" style="outline:none;width:33.333333333333336%"><div><div class="link" data-link-type="router"><a draggable="false" aria-labelledby="g-uniq-242419" aria-describedby="g-uniq-242421 g-uniq-242423" class="g-link g-link_view_normal pc-card-base-block pc-card-base-block_border_shadow bc-post-card__card" href="../../2022/12/advantages-of-integrating-clickhouse-with-hybrid-s3-storage.html"><div class="pc-storage-background-image pc-card-base-block__header bc-post-card__header" data-qa="background-image"><picture data-qa="background-image-image"><source srcSet="../../../../assets/blog/articles/advantages-of-integrating-clickhouse-with-hybrid-s3-storage-small-cover.png.webp" type="image/webp" data-qa="background-image-image-desktop-source-compressed"/><img alt="" src="../../../../assets/blog/articles/advantages-of-integrating-clickhouse-with-hybrid-s3-storage-small-cover.png" class="pc-storage-background-image__img"/></picture><div class="pc-storage-background-image__container"><div class="pc-card-base-block__header-content"><div class="bc-post-card__image-container" data-qa="blog-suggest-header"></div></div></div></div><div class="pc-card-base-block__body"><div class="pc-card-base-block__content"><h3 class="bc-post-card__title bc-post-card__title_size_s"><span><span id="g-uniq-242419">Advantages of integrating ClickHouse® with Hybrid S3 Storage</span></span></h3></div><div class="pc-card-base-block__footer"><div class="bc-post-info__container"><div class="bc-post-info__suggest-container"><div class="bc-post-info__item bc-post-info__item_size_s" id="g-uniq-242421">December 26, 2022</div><div class="bc-post-info__item bc-post-info__item_size_s" id="g-uniq-242423"><span class="bc-post-info__icon"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="16" height="16" class="g-icon bc-post-info__icon-color" fill="currentColor" stroke="none" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 17" fill="currentColor" aria-hidden="true"><path fill-rule="evenodd" clip-rule="evenodd" d="M8 16.004a8 8 0 1 1 0-16 8 8 0 0 1 0 16Zm0-2a6 6 0 1 0 0-12 6 6 0 0 0 0 12Zm3.357-3.736a1 1 0 0 0-.342-1.372L9 7.688V5.004a1 1 0 0 0-2 0v3.25a1 1 0 0 0 .486.857l2.5 1.5a1 1 0 0 0 1.371-.343Z"></path></svg></svg></span>15 mins to read</div></div></div></div></div></a></div></div></div><div data-index="1" class="slick-slide slick-active" tabindex="-1" aria-hidden="false" style="outline:none;width:33.333333333333336%"><div><div class="link" data-link-type="router"><a draggable="false" aria-labelledby="g-uniq-242424" aria-describedby="g-uniq-242425 g-uniq-242426 g-uniq-242428" class="g-link g-link_view_normal pc-card-base-block pc-card-base-block_border_shadow bc-post-card__card" href="../../2022/12/how-to-leverage-jupyter-with-doublecloud-managed-clickhouse.html"><div class="pc-storage-background-image pc-card-base-block__header bc-post-card__header" data-qa="background-image"><picture data-qa="background-image-image"><source srcSet="../../../../assets/blog/articles/jupyter-small-cover.png.webp" type="image/webp" data-qa="background-image-image-desktop-source-compressed"/><img alt="" src="../../../../assets/blog/articles/jupyter-small-cover.png" class="pc-storage-background-image__img"/></picture><div class="pc-storage-background-image__container"><div class="pc-card-base-block__header-content"><div class="bc-post-card__image-container" data-qa="blog-suggest-header"></div></div></div></div><div class="pc-card-base-block__body"><div class="pc-card-base-block__content"><h3 class="bc-post-card__title bc-post-card__title_size_s"><span><span id="g-uniq-242424">How to leverage Jupyter with DoubleCloud’s Managed ClickHouse®</span></span></h3><span class="yfm yfm_blog_card bc-post-card__description" id="g-uniq-242425">Written by: Adam Jennings, Senior Solutions Architect, DoubleCloud</span></div><div class="pc-card-base-block__footer"><div class="bc-post-info__container"><div class="bc-post-info__suggest-container"><div class="bc-post-info__item bc-post-info__item_size_s" id="g-uniq-242426">December 8, 2022</div><div class="bc-post-info__item bc-post-info__item_size_s" id="g-uniq-242428"><span class="bc-post-info__icon"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="16" height="16" class="g-icon bc-post-info__icon-color" fill="currentColor" stroke="none" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 17" fill="currentColor" aria-hidden="true"><path fill-rule="evenodd" clip-rule="evenodd" d="M8 16.004a8 8 0 1 1 0-16 8 8 0 0 1 0 16Zm0-2a6 6 0 1 0 0-12 6 6 0 0 0 0 12Zm3.357-3.736a1 1 0 0 0-.342-1.372L9 7.688V5.004a1 1 0 0 0-2 0v3.25a1 1 0 0 0 .486.857l2.5 1.5a1 1 0 0 0 1.371-.343Z"></path></svg></svg></span>10 mins to read</div></div></div></div></div></a></div></div></div><div data-index="2" class="slick-slide slick-active" tabindex="-1" aria-hidden="false" style="outline:none;width:33.333333333333336%"><div><div class="link" data-link-type="router"><a draggable="false" aria-labelledby="g-uniq-242429" aria-describedby="g-uniq-242430 g-uniq-242431 g-uniq-242433" class="g-link g-link_view_normal pc-card-base-block pc-card-base-block_border_shadow bc-post-card__card" href="machine-learning-with-clickhouse-and-doublecloud.html"><div class="pc-storage-background-image pc-card-base-block__header bc-post-card__header" data-qa="background-image"><picture data-qa="background-image-image"><source srcSet="../../../../assets/blog/articles/machine-learning-with-clickhouse-and-doublecloud-small-cover.png.webp" type="image/webp" data-qa="background-image-image-desktop-source-compressed"/><img alt="" src="../../../../assets/blog/articles/machine-learning-with-clickhouse-and-doublecloud-small-cover.png" class="pc-storage-background-image__img"/></picture><div class="pc-storage-background-image__container"><div class="pc-card-base-block__header-content"><div class="bc-post-card__image-container" data-qa="blog-suggest-header"></div></div></div></div><div class="pc-card-base-block__body"><div class="pc-card-base-block__content"><h3 class="bc-post-card__title bc-post-card__title_size_s"><span><span id="g-uniq-242429">Machine learning with ClickHouse and DoubleCloud</span></span></h3><span class="yfm yfm_blog_card bc-post-card__description" id="g-uniq-242430">Written by: Adam Jennings, DoubleCloud Solution Architect</span></div><div class="pc-card-base-block__footer"><div class="bc-post-info__container"><div class="bc-post-info__suggest-container"><div class="bc-post-info__item bc-post-info__item_size_s" id="g-uniq-242431">March 14, 2023</div><div class="bc-post-info__item bc-post-info__item_size_s" id="g-uniq-242433"><span class="bc-post-info__icon"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="16" height="16" class="g-icon bc-post-info__icon-color" fill="currentColor" stroke="none" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 17" fill="currentColor" aria-hidden="true"><path fill-rule="evenodd" clip-rule="evenodd" d="M8 16.004a8 8 0 1 1 0-16 8 8 0 0 1 0 16Zm0-2a6 6 0 1 0 0-12 6 6 0 0 0 0 12Zm3.357-3.736a1 1 0 0 0-.342-1.372L9 7.688V5.004a1 1 0 0 0-2 0v3.25a1 1 0 0 0 .486.857l2.5 1.5a1 1 0 0 0 1.371-.343Z"></path></svg></svg></span>15 mins to read</div></div></div></div></div></a></div></div></div></div></div></div><div class="pc-SliderBlock__footer"></div></div></div></div></section></div></div></div></div></div></main></div></div></div><div class="bc-prompt bc-prompt_close"><div class="bc-prompt__content"><span class="bc-prompt__text">Sign in to save this post</span><div class="bc-prompt__actions"><button class="g-button g-button_view_action g-button_size_l g-button_pin_round-round bc-prompt__action" type="button"><span class="g-button__text">Sign In</span></button></div></div></div></div><footer class="footer"><div class="pc-Grid"><div class="container-fluid "><div class="row"><div class="col  col-12 col-md-4 footer__column"><div class="link" data-link-type="router"><div class="logo footer__logo"><img alt="Logo Icon" src="../../../../assets/logo/dc-logo-dark.svg" width="178" height="36" decoding="async" data-nimg="future" class="logo__icon" loading="lazy" style="color:transparent"/><span class="logo__text"></span></div></div></div><div class="col  col-6 col-sm-3 col-md-2 footer__column"><div class="footer__column-title">Products</div><a aria-label="Managed Service for ClickHouse®" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../services/managed-clickhouse.html"><div class="navigation-item"><span class="navigation-item__text">Managed Service for ClickHouse®</span></div></a><a aria-label="Managed Service for Apache Kafka®" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../services/managed-kafka.html"><div class="navigation-item"><span class="navigation-item__text">Managed Service for Apache Kafka®</span></div></a><a aria-label="Managed Service for Apache Airflow®" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../services/managed-airflow/index.html"><div class="navigation-item"><span class="navigation-item__text">Managed Service for Apache Airflow®</span></div></a><a aria-label="Data Transfer" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../services/doublecloud-transfer.html"><div class="navigation-item"><span class="navigation-item__text">Data Transfer</span></div></a><a aria-label="Data Visualization" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../services/doublecloud-visualization.html"><div class="navigation-item"><span class="navigation-item__text">Data Visualization</span></div></a></div><div class="col  col-6 col-sm-3 col-md-2 footer__column"><div class="footer__column-title">Solutions</div><a aria-label="Case studies" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../resources/case-studies/index.html"><div class="navigation-item"><span class="navigation-item__text">Case studies</span></div></a><a aria-label="Customer-facing analytics" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../customer-facing-analytics/index.html"><div class="navigation-item"><span class="navigation-item__text">Customer-facing analytics</span></div></a><a aria-label="Real-time analytics" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../solutions/real-time-analytics/index.html"><div class="navigation-item"><span class="navigation-item__text">Real-time analytics</span></div></a><a aria-label="Observability and monitoring" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../solutions/observability-and-monitoring/index.html"><div class="navigation-item"><span class="navigation-item__text">Observability and monitoring</span></div></a><a aria-label="AdTech and MarTech data analytics" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../solutions/adtech.html"><div class="navigation-item"><span class="navigation-item__text">AdTech and MarTech data analytics</span></div></a><a aria-label="Analytics for mobile and gaming Apps" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../solutions/web-mobile-gaming-apps.html"><div class="navigation-item"><span class="navigation-item__text">Analytics for mobile and gaming Apps</span></div></a><a aria-label="EdTech data analytics" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../solutions/edtech/index.html"><div class="navigation-item"><span class="navigation-item__text">EdTech data analytics</span></div></a><a aria-label="FinTech data analytics" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../solutions/fintech-real-time-analytics/index.html"><div class="navigation-item"><span class="navigation-item__text">FinTech data analytics</span></div></a></div><div class="col  col-6 col-sm-3 col-md-2 footer__column"><div class="footer__column-title">Resources</div><a aria-label="Documentation" class="navigation-item navigation-item_type_link footer__column-link" href="../../../../docs/index.html"><div class="navigation-item"><span class="navigation-item__text">Documentation</span></div></a><a aria-label="Webinars" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../webinars/index.html"><div class="navigation-item"><span class="navigation-item__text">Webinars</span></div></a><a aria-label="Blog" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../index.html"><div class="navigation-item navigation-item_selected"><span class="navigation-item__text">Blog</span></div></a><a aria-label="Support" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../support/index.html"><div class="navigation-item"><span class="navigation-item__text">Support</span></div></a><a href="https://status.double.cloud/" aria-label="Status updates" class="navigation-item navigation-item_type_link footer__column-link" target="_blank" rel="noopener noreferrer"><div class="navigation-item"><span class="navigation-item__text">Status updates</span></div></a><a aria-label="Product comparisons" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../comparison/index.html"><div class="navigation-item"><span class="navigation-item__text">Product comparisons</span></div></a><a aria-label="Site map" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../sitemap/index.html"><div class="navigation-item"><span class="navigation-item__text">Site map</span></div></a></div><div class="col  col-6 col-sm-3 col-md-2 footer__column"><div class="footer__column-title">Company</div><a aria-label="About DoubleCloud" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../company/about-us.html"><div class="navigation-item"><span class="navigation-item__text">About DoubleCloud</span></div></a><a aria-label="Careers" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../company/careers.html"><div class="navigation-item"><span class="navigation-item__text">Careers</span></div></a><a aria-label="AWS Partnership" class="navigation-item navigation-item_type_link footer__column-link" data-link-type="router" href="../../../../aws-partnership/index.html"><div class="navigation-item"><span class="navigation-item__text">AWS Partnership</span></div></a></div></div><div class="row"><div class="col  col-12 footer__underline"><div class="footer__underline-links"><a href="../../../../legal/customer_agreement/index.html" aria-label="Customer Agreement" class="navigation-item navigation-item_type_link footer__underline-link" target="_blank"><div class="navigation-item"><span class="navigation-item__text">Customer Agreement</span></div></a><a href="../../../../legal/privacy.html" aria-label="Privacy Policy" class="navigation-item navigation-item_type_link footer__underline-link" target="_blank"><div class="navigation-item"><span class="navigation-item__text">Privacy Policy</span></div></a><a aria-label="Pricing" class="navigation-item navigation-item_type_link footer__underline-link" data-link-type="router" href="../../../../pricing.html"><div class="navigation-item"><span class="navigation-item__text">Pricing</span></div></a><a href="../../../../security.html" aria-label="Security" class="navigation-item navigation-item_type_link footer__underline-link" target="_blank"><div class="navigation-item"><span class="navigation-item__text">Security</span></div></a></div><div class="footer__underline-copyright">© 2024 DoubleCloud</div></div></div></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json" nonce="WtQiGkAvD9Bf8xAZrtOfhw==">{"props":{"pageProps":{"data":{"status":"fulfilled","pageContent":{"page":{"id":81,"name":"blog/posts/2023/03/fastest-way-to-ingest-data-into-mysql","createdAt":"2024-08-21T14:19:09.330Z","updatedAt":"2024-08-21T14:19:09.330Z","type":"default","isDeleted":false,"versionOnTranslationId":null,"pageId":81,"locale":"en","publishedVersionId":2245,"lastVersionId":2245,"content":{"blocks":[{"type":"blog-header-block","resetPaddings":true,"paddingBottom":"l","width":"s","verticalOffset":"l","background":{"image":{"src":"/assets/blog/articles/fastest-way-to-ingest-data-into-mysql-cover.png","disableCompress":true,"fetchPriority":"high"},"color":"#000000","fullWidth":false}},{"type":"blog-layout-block","resetPaddings":true,"mobileOrder":"reverse","children":[{"type":"blog-yfm-block","column":"right","resetPaddings":true,"text":"\u003cp\u003eIn\u0026nbsp;this article, we\u0026rsquo;ll talk about:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#1-billion-rows-to-ingest-what-to-do?\"\u003e1 billion rows to\u0026nbsp;ingest\u0026nbsp;\u0026mdash; What to\u0026nbsp;do?\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#ingesting-local-files-to-mysql\"\u003eIngesting local files to\u0026nbsp;MySQL\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#let-clickhouse-handle-all-the-overhead\"\u003eLet ClickHouse handle all the overhead\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#conclusion\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e"},{"type":"blog-yfm-block","column":"left","resetPaddings":true,"text":"\u003cp\u003eMySQL.\u003c/p\u003e\n\u003cp\u003eIt\u0026rsquo;s often the first choice for start-up\u0026rsquo;s looking for open-source software to\u0026nbsp;store application data.\u003c/p\u003e\n\u003cp\u003eNow I\u0026rsquo;ve been working with MySQL myself for over two decades, and although I\u0026rsquo;ve seen and worked with other databases since then, it\u0026nbsp;will always have a\u0026nbsp;special place in\u0026nbsp;my\u0026nbsp;heart as\u0026nbsp;it\u0026nbsp;was the first one to\u0026nbsp;show me\u0026nbsp;how much fun working with SQL databases is.\u003c/p\u003e\n\u003cp\u003eOf\u0026nbsp;course MySQL is\u0026nbsp;an\u0026nbsp;OLTP database, its strength is\u0026nbsp;in\u0026nbsp;serving multiple thousands of\u0026nbsp;small queries and transactions, but it\u0026rsquo;s still one of\u0026nbsp;the most often used DBMS for general workloads as\u0026nbsp;well.\u003c/p\u003e\n\u003cp\u003eAs\u0026nbsp;I\u0026nbsp;mentioned, a\u0026nbsp;lot of\u0026nbsp;companies start with MySQL, building their app (s) around it, watching it\u0026nbsp;grow, and even though MySQL definitely wasn\u0026rsquo;t intended to\u0026nbsp;be\u0026nbsp;used as\u0026nbsp;an\u0026nbsp;analytical database, there are a\u0026nbsp;lot of\u0026nbsp;companies (and individuals) still out there using it\u0026nbsp;for exactly this purpose.\u003c/p\u003e\n\u003cp\u003eAnd whilst there are a\u0026nbsp;lot of\u0026nbsp;good reasons why you shouldn\u0026rsquo;t use MySQL for big analytical workloads, MySQL is\u0026nbsp;still a\u0026nbsp;lot more capable of\u0026nbsp;analytical processes than a\u0026nbsp;lot of\u0026nbsp;people think it\u0026nbsp;is.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ve seen single MySQL instances having multiple Terabytes of\u0026nbsp;data stored within itself, some with tables handling multiple billions of\u0026nbsp;rows without bigger issues.\u003c/p\u003e\n\u003cp\u003eThe concept of\u0026nbsp;allowing different storage engines within the same environment allows MySQL to\u0026nbsp;stretch over its own limits by\u0026nbsp;several orders of\u0026nbsp;magnitude.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ve seen quite impressive analytical queries achieved with the use of\u0026nbsp;TokuDB or\u0026nbsp;RocksDB storage engines that I\u0026nbsp;never thought MySQL would be\u0026nbsp;able to\u0026nbsp;fulfill.\u003c/p\u003e\n\u003cp\u003eStill, MySQL was never meant to\u0026nbsp;be\u0026nbsp;used for big analytical workloads, as\u0026nbsp;it\u0026nbsp;was designed and built back in\u0026nbsp;a\u0026nbsp;time when clouds were meant to\u0026nbsp;be\u0026nbsp;in\u0026nbsp;the sky and formats of\u0026nbsp;data were proprietary instead of\u0026nbsp;being used for big open exchanges.\u003c/p\u003e\n\u003cp\u003eSo\u0026nbsp;one of\u0026nbsp;the biggest problems when doing analytical stuff in\u0026nbsp;MySQL is\u0026nbsp;still\u0026hellip; How do\u0026nbsp;you get the data in\u0026nbsp;it?\u003c/p\u003e\n\u003ch1 id=\"1-billion-rows-to-ingest-what-to-do?\"\u003e\u003ca href=\"#1-billion-rows-to-ingest-what-to-do?\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003e1 billion rows to\u0026nbsp;ingest\u0026nbsp;\u0026mdash; What to\u0026nbsp;do?\u003c/span\u003e\u003c/a\u003e1 billion rows to\u0026nbsp;ingest\u0026nbsp;\u0026mdash; What to\u0026nbsp;do?\u003c/h1\u003e\n\u003cp\u003eLately I\u0026rsquo;ve been running benchmarks on\u0026nbsp;different DBMS.\u003c/p\u003e\n\u003cp\u003eThe data set itself contained around 1\u0026nbsp;billion rows of\u0026nbsp;data, split into yearly chunks of\u0026nbsp;around 10\u0026nbsp;million rows, formatted in\u0026nbsp;one line of\u0026nbsp;JSON data per row, compressed via gzip, and located in\u0026nbsp;a\u0026nbsp;bucket in\u0026nbsp;Amazon\u0026rsquo;s S3.\u003c/p\u003e\n\u003cp\u003eEach row has 11\u0026nbsp;columns, but for our benchmark we\u0026nbsp;only need to\u0026nbsp;extract 5\u0026nbsp;of\u0026nbsp;those: date, station_id, tempMax, tempMin, tempAvg\u003c/p\u003e\n\u003cp\u003eFor this blog, I\u0026nbsp;will limit our data set to\u0026nbsp;only one year of\u0026nbsp;data, choosing 2020\u0026nbsp;at\u0026nbsp;random.\u003c/p\u003e\n\u003cp\u003eThe gzip compressed JSON data is\u0026nbsp;98MB.\u003c/p\u003e\n\u003cp\u003eWhen decompressed the file size grows to\u0026nbsp;2.7 GB, which is\u0026nbsp;expected as\u0026nbsp;the column names contained within the JSON are of\u0026nbsp;course very compressible.\u003c/p\u003e\n\u003cp\u003eThe total number of\u0026nbsp;rows contained within the file is\u0026nbsp;12,095,646.\u003c/p\u003e\n\u003cp\u003eFor our benchmark we\u0026nbsp;chose a\u0026nbsp;small Amazon RDS instance with 2\u0026nbsp;vCPU cores, 8GB of\u0026nbsp;RAM and enough storage space to\u0026nbsp;store the entire billion rows.\u003c/p\u003e\n\u003cp\u003eAs\u0026nbsp;we\u0026nbsp;only want to\u0026nbsp;show ingestion logic within this blog post, we\u0026nbsp;limited the data set to\u0026nbsp;2020, so\u0026nbsp;the whole data set would fit into ram.\u003c/p\u003e\n\u003cp\u003eThe table definition for our ingestion case is\u0026nbsp;quite simple.\u003c/p\u003e\n\u003cp\u003eWe\u0026nbsp;have a\u0026nbsp;primary key with 2\u0026nbsp;columns (date, station_id) and a\u0026nbsp;secondary index on\u0026nbsp;station_id for a\u0026nbsp;somewhat realistic scenario:\u003c/p\u003e\n\n    \u003cdiv class=\"yfm-clipboard\"\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs\"\u003eCREATE TABLE `sensor_data` (\n  `station_id` char(11) CHARACTER SET ascii COLLATE ascii_bin NOT NULL,\n  `date` date NOT NULL,\n  `tempAvg` int DEFAULT NULL,\n  `tempMax` int DEFAULT NULL,\n  `tempMin` int DEFAULT NULL,\n  PRIMARY KEY (`date`,`station_id`),\n  KEY `station` (`station_id`)\n) ENGINE=InnoDB;\n\u003c/code\u003e\u003c/pre\u003e\n\n    \u003csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\" class=\"yfm-clipboard-button\" data-animation=\"69\"\u003e\n        \u003cpath fill=\"currentColor\" d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\"\u003e\u003c/path\u003e\n        \u003cpath stroke=\"currentColor\" fill=\"transparent\" stroke-width=\"1.5\" d=\"M9.5 13l3 3l5 -5\" visibility=\"hidden\"\u003e\n            \u003canimate id=\"visibileAnimation-69\" attributeName=\"visibility\" from=\"hidden\" to=\"visible\" dur=\"0.2s\" fill=\"freeze\" begin\u003e\u003c/animate\u003e\n            \u003canimate id=\"hideAnimation-69\" attributeName=\"visibility\" from=\"visible\" to=\"hidden\" dur=\"1s\" begin=\"visibileAnimation-69.end+1\" fill=\"freeze\"\u003e\u003c/animate\u003e\n        \u003c/path\u003e\n    \u003c/svg\u003e\n    \u003c/div\u003e\n\u003cp\u003eI\u0026nbsp;chose InnoDB as\u0026nbsp;the storage engine, as\u0026nbsp;it\u0026rsquo;s the most commonly used table engine in\u0026nbsp;MySQL.\u003c/p\u003e\n\u003ch3 id=\"how-to-start?\"\u003e\u003ca href=\"#how-to-start?\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eHow to\u0026nbsp;start?\u003c/span\u003e\u003c/a\u003eHow to\u0026nbsp;start?\u003c/h3\u003e\n\u003cp\u003eWhen trying to\u0026nbsp;ingest our test data into MySQL is\u0026nbsp;where we\u0026nbsp;hit our first hurdle.\u003c/p\u003e\n\u003cp\u003eMySQL can\u0026rsquo;t read directly from S3 or\u0026nbsp;unzip the data on\u0026nbsp;the\u0026nbsp;fly. Therefore we\u0026nbsp;have to\u0026nbsp;do\u0026nbsp;some manual steps, before we\u0026nbsp;can even start:\u003c/p\u003e\n\u003cp\u003eDownload the files: Depending on\u0026nbsp;if\u0026nbsp;you need these tasks regularly or\u0026nbsp;just one time; this step could be\u0026nbsp;easy or\u0026nbsp;complex and the amount of\u0026nbsp;time needed to\u0026nbsp;implement that logic cannot be\u0026nbsp;estimated in\u0026nbsp;a\u0026nbsp;general blog. Also the download speed depends on\u0026nbsp;multiple factors, so\u0026nbsp;we\u0026nbsp;won\u0026rsquo;t include it\u0026nbsp;here in\u0026nbsp;our comparison, just mention that it\u0026nbsp;can have an\u0026nbsp;impact.\u003c/p\u003e\n\u003cp\u003eDecompressing the files: The easiest way to\u0026nbsp;achieve this is\u0026nbsp;by\u0026nbsp;using gzip -d {filename} on\u0026nbsp;the cli.\u003c/p\u003e\n\u003cp\u003eDecompressing files of\u0026nbsp;that size normally doesn\u0026rsquo;t take much time, but still, we\u0026nbsp;have to\u0026nbsp;write the decompressed data to\u0026nbsp;the disk. In\u0026nbsp;my\u0026nbsp;tests it\u0026nbsp;took around 7\u0026nbsp;seconds per file.\u003c/p\u003e\n\u003cp\u003eNow that we\u0026nbsp;have the files locally and decompressed, we\u0026nbsp;can really start to\u0026nbsp;ingest the data.\u003c/p\u003e\n\u003cp\u003eWe\u0026nbsp;need to\u0026nbsp;read the JSON data, get rid of\u0026nbsp;the unwanted columns, and \u0026ldquo;only\u0026rdquo; ingest the 12\u0026nbsp;million rows of\u0026nbsp;data into MySQL.\u003c/p\u003e\n\u003ch1 id=\"ingesting-local-files-to-mysql\"\u003e\u003ca href=\"#ingesting-local-files-to-mysql\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eIngesting local files to\u0026nbsp;MySQL\u003c/span\u003e\u003c/a\u003eIngesting local files to\u0026nbsp;MySQL\u003c/h1\u003e\n\u003ch3 id=\"use-scripting-language-like-php\"\u003e\u003ca href=\"#use-scripting-language-like-php\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eUse scripting language like PHP\u003c/span\u003e\u003c/a\u003eUse scripting language like PHP\u003c/h3\u003e\n\u003cp\u003eA\u0026nbsp;lot of\u0026nbsp;environments which use MySQL as\u0026nbsp;their main DBMS also make use of\u0026nbsp;PHP (remember LAMP stack anyone?).\u003c/p\u003e\n\u003cp\u003eA\u0026nbsp;simple solution to\u0026nbsp;achieve our goal would be\u0026nbsp;to\u0026nbsp;read the JSON file one line at\u0026nbsp;a\u0026nbsp;time, use PHPs parsing functionality to\u0026nbsp;get the values of\u0026nbsp;the 5\u0026nbsp;needed columns and insert them directly into MySQL.\u003c/p\u003e\n\u003cp\u003eSomething like this:\u003c/p\u003e\n\n    \u003cdiv class=\"yfm-clipboard\"\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs\"\u003e$mysqli = new mysqli($mysql_host, $mysql_username, $mysql_password, '', $mysql_port);\n$filename = $argv[1];\n\n$handle = @fopen($filename, 'r');\nif ($handle)\n{\n  while (($buffer = fgets($handle, 4096)) !== false)\n  {\n      $row = json_decode($buffer, true);\n      $mysqli-\u0026gt;query(\"\n        INSERT IGNORE INTO test.sensor_data\n          (station_id, date, tempMax, tempMin, tempAvg)\n        VALUES\n          ('{$row['station_id']}', '{$row['date']}', \" .\n              ($row['tempMax'] ?? 'NULL') . \", \" . \n              ($row['tempMin'] ?? 'NULL') . \", \" . \n              ($row['tempAvg'] ?? 'NULL') . \")  \n      \");\n  }\n  fclose($handle);\n}\nexit(0);\n\u003c/code\u003e\u003c/pre\u003e\n\n    \u003csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\" class=\"yfm-clipboard-button\" data-animation=\"112\"\u003e\n        \u003cpath fill=\"currentColor\" d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\"\u003e\u003c/path\u003e\n        \u003cpath stroke=\"currentColor\" fill=\"transparent\" stroke-width=\"1.5\" d=\"M9.5 13l3 3l5 -5\" visibility=\"hidden\"\u003e\n            \u003canimate id=\"visibileAnimation-112\" attributeName=\"visibility\" from=\"hidden\" to=\"visible\" dur=\"0.2s\" fill=\"freeze\" begin\u003e\u003c/animate\u003e\n            \u003canimate id=\"hideAnimation-112\" attributeName=\"visibility\" from=\"visible\" to=\"hidden\" dur=\"1s\" begin=\"visibileAnimation-112.end+1\" fill=\"freeze\"\u003e\u003c/animate\u003e\n        \u003c/path\u003e\n    \u003c/svg\u003e\n    \u003c/div\u003e\n\u003cp\u003eDoing it\u0026nbsp;this way unfortunately has the overhead of\u0026nbsp;executing a\u0026nbsp;single transaction for each row of\u0026nbsp;the dataset, hence only achieving around 60\u0026nbsp;rows/second, or\u0026nbsp;taking approx. 59\u0026nbsp;hours for the 12\u0026nbsp;million rows.\u003c/p\u003e\n\u003cp\u003eHowever, it\u0026rsquo;s easy to\u0026nbsp;optimize this logic by\u0026nbsp;introducing the bunching of\u0026nbsp;rows, combining 10k rows per insert query, speeding up\u0026nbsp;the whole process to\u0026nbsp;take 3\u0026nbsp;minutes and 30\u0026nbsp;seconds for the whole data set.\u003c/p\u003e\n\u003cp\u003eIngestion speed is\u0026nbsp;okay for the amount of\u0026nbsp;data but it\u0026nbsp;required a\u0026nbsp;lot of\u0026nbsp;manual work and no\u0026nbsp;piece of\u0026nbsp;error handling is\u0026nbsp;written yet.\u003c/p\u003e\n\u003cp\u003eBut there are ways to\u0026nbsp;optimize data ingestion in\u0026nbsp;MySQL.\u003c/p\u003e\n\u003ch3 id=\"use-json-functionality-with-mysql\"\u003e\u003ca href=\"#use-json-functionality-with-mysql\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eUse JSON functionality with MySQL\u003c/span\u003e\u003c/a\u003eUse JSON functionality with MySQL\u003c/h3\u003e\n\u003cp\u003eUnfortunately MySQL has no\u0026nbsp;feature for working with JSON files directly, but we\u0026nbsp;can interpret the whole file as\u0026nbsp;a\u0026nbsp;CSV file, having only a\u0026nbsp;single column containing a\u0026nbsp;JSON string.\u003c/p\u003e\n\u003cp\u003eThat way we\u0026nbsp;can load the JSON file via the LOAD DATA command, and then select the needed columns via the built-in JSON functions of\u0026nbsp;MySQL.\u003c/p\u003e\n\n    \u003cdiv class=\"yfm-clipboard\"\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs\"\u003eLOAD DATA LOCAL INFILE '~/MySQLBlog/sensors.2020.json'\nINTO TABLE test.sensor_data  \n(@json)\nSET station_id = JSON_UNQUOTE(JSON_EXTRACT(@json, '$.station_id')),\n  date = JSON_UNQUOTE(JSON_EXTRACT(@json, '$.date')),\n  tempMax = nullif(JSON_UNQUOTE(JSON_EXTRACT(@json, '$.tempMax')), 'null'),\n  tempMin = nullif(JSON_UNQUOTE(JSON_EXTRACT(@json, '$.tempMin')), 'null'),\n  tempAvg = nullif(JSON_UNQUOTE(JSON_EXTRACT(@json, '$.tempAvg')), 'null')\n\u003c/code\u003e\u003c/pre\u003e\n\n    \u003csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\" class=\"yfm-clipboard-button\" data-animation=\"134\"\u003e\n        \u003cpath fill=\"currentColor\" d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\"\u003e\u003c/path\u003e\n        \u003cpath stroke=\"currentColor\" fill=\"transparent\" stroke-width=\"1.5\" d=\"M9.5 13l3 3l5 -5\" visibility=\"hidden\"\u003e\n            \u003canimate id=\"visibileAnimation-134\" attributeName=\"visibility\" from=\"hidden\" to=\"visible\" dur=\"0.2s\" fill=\"freeze\" begin\u003e\u003c/animate\u003e\n            \u003canimate id=\"hideAnimation-134\" attributeName=\"visibility\" from=\"visible\" to=\"hidden\" dur=\"1s\" begin=\"visibileAnimation-134.end+1\" fill=\"freeze\"\u003e\u003c/animate\u003e\n        \u003c/path\u003e\n    \u003c/svg\u003e\n    \u003c/div\u003e\n\u003cp\u003eThis approach took 4\u0026nbsp;minutes and 55\u0026nbsp;seconds, so\u0026nbsp;even slower than the \u0026lsquo;optimized\u0026rsquo; PHP solution, but at\u0026nbsp;least it\u0026rsquo;s easier to\u0026nbsp;type (At\u0026nbsp;least for a\u0026nbsp;SQL guy like myself).\u003c/p\u003e\n\u003cp\u003eThe main problem with this approach taking so\u0026nbsp;long though is\u0026nbsp;that we\u0026nbsp;still send the data of\u0026nbsp;all the columns to\u0026nbsp;the MySQL server, and then throw all but 5\u0026nbsp;away within the server, hence having unneeded data sent over the wire to\u0026nbsp;MySQL.\u003c/p\u003e\n\u003cp\u003eAlso loading 10\u0026nbsp;million rows in\u0026nbsp;a\u0026nbsp;single transaction issues quite a\u0026nbsp;load to\u0026nbsp;the server.\u003c/p\u003e\n\u003ch3 id=\"use-cli-and-csv-functionality-with-mysql\"\u003e\u003ca href=\"#use-cli-and-csv-functionality-with-mysql\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eUse Cli and CSV functionality with MySQL\u003c/span\u003e\u003c/a\u003eUse Cli and CSV functionality with MySQL\u003c/h3\u003e\n\u003cp\u003eWe\u0026nbsp;can avoid using the JSON functionality of\u0026nbsp;MySQL as\u0026nbsp;well as\u0026nbsp;the sending of\u0026nbsp;unwanted data by\u0026nbsp;adding another preparation step before the ingestion to\u0026nbsp;MySQL.\u003c/p\u003e\n\u003cp\u003eWe\u0026nbsp;can use a\u0026nbsp;command line JSON tool like jq\u0026nbsp;to\u0026nbsp;select only the needed columns and create a\u0026nbsp;comma separated file, only containing the 5\u0026nbsp;needed columns:\u003c/p\u003e\n\u003cp\u003ejq\u0026nbsp;-r '. | [.station_id, .date, .tempMax, .tempMin, .tempAvg] | @csv' sensors.2020.json \u0026gt; sensors_parsed.2020.csv\u003c/p\u003e\n\u003cp\u003eParsing JSON to\u0026nbsp;CSV on\u0026nbsp;cli takes around 60\u0026nbsp;seconds in\u0026nbsp;that case, generating a\u0026nbsp;CSV file with 376\u0026nbsp;MB\u0026nbsp;of\u0026nbsp;data.\u003c/p\u003e\n\u003cp\u003eNow importing the csv file can be\u0026nbsp;done quite easily within the MySQL client again:\u003c/p\u003e\n\n    \u003cdiv class=\"yfm-clipboard\"\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs\"\u003eLOAD DATA LOCAL INFILE '~/MySQLBlog/sensors_parsed.2020.csv'\nINTO TABLE test.sensor_data  \nFIELDS TERMINATED by ',' OPTIONALLY ENCLOSED BY '\"'\n(station_id, date, @tempMax, @tempMin, @tempAvg)\nSET tempMax = nullif(@tempMax, ''),\n  tempMin = nullif(@tempMin, ''),\n  tempAvg = nullif(@tempAvg, '')\n\u003c/code\u003e\u003c/pre\u003e\n\n    \u003csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\" class=\"yfm-clipboard-button\" data-animation=\"162\"\u003e\n        \u003cpath fill=\"currentColor\" d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\"\u003e\u003c/path\u003e\n        \u003cpath stroke=\"currentColor\" fill=\"transparent\" stroke-width=\"1.5\" d=\"M9.5 13l3 3l5 -5\" visibility=\"hidden\"\u003e\n            \u003canimate id=\"visibileAnimation-162\" attributeName=\"visibility\" from=\"hidden\" to=\"visible\" dur=\"0.2s\" fill=\"freeze\" begin\u003e\u003c/animate\u003e\n            \u003canimate id=\"hideAnimation-162\" attributeName=\"visibility\" from=\"visible\" to=\"hidden\" dur=\"1s\" begin=\"visibileAnimation-162.end+1\" fill=\"freeze\"\u003e\u003c/animate\u003e\n        \u003c/path\u003e\n    \u003c/svg\u003e\n    \u003c/div\u003e\n\u003cp\u003eThis approach takes 1\u0026nbsp;minute 52\u0026nbsp;seconds, which is\u0026nbsp;a\u0026nbsp;lot faster than reading directly from JSON to\u0026nbsp;MySQL.\u003c/p\u003e\n\u003cp\u003eWe\u0026nbsp;still, however, have the problem of\u0026nbsp;big transactions. So\u0026nbsp;if\u0026nbsp;we\u0026nbsp;want our MySQL instance to\u0026nbsp;handle other requests without trouble, we\u0026rsquo;d need to\u0026nbsp;split the csv file into smaller chunks of\u0026nbsp;10k rows to\u0026nbsp;reduce the load on\u0026nbsp;the server.\u003c/p\u003e\n\u003cp\u003eThus we\u0026nbsp;again need to\u0026nbsp;write simple cli logic to\u0026nbsp;split the csv into chunks and then write a\u0026nbsp;loop to\u0026nbsp;go\u0026nbsp;over the 1,000 chunk files, ingesting one after another.\u003c/p\u003e\n\u003cp\u003eOverall, regarding the 60\u0026nbsp;sec of\u0026nbsp;cli time and the 1\u0026nbsp;min 52\u0026nbsp;sec for inserting the data, the total time to\u0026nbsp;ingest our 12\u0026nbsp;million JSON rows takes 2\u0026nbsp;min 52\u0026nbsp;sec, therefore being 40% faster than reading JSON directly from MySQL and still being 17% faster than the scripted solution.\u003c/p\u003e\n\u003ch1 id=\"let-clickhouse-handle-all-the-overhead\"\u003e\u003ca href=\"#let-clickhouse-handle-all-the-overhead\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eLet ClickHouse handle all the overhead\u003c/span\u003e\u003c/a\u003eLet ClickHouse handle all the overhead\u003c/h1\u003e\n\u003cp\u003eNow even though we\u0026nbsp;can reach a\u0026nbsp;somewhat acceptable speed with cli tools and the mysql client, it\u0026rsquo;s still not a\u0026nbsp;very nice approach.\u003c/p\u003e\n\u003cp\u003eAlso\u0026hellip; imagine if\u0026nbsp;you had to\u0026nbsp;do\u0026nbsp;this on\u0026nbsp;a\u0026nbsp;regular basis, as\u0026nbsp;your software imports data from external sources on\u0026nbsp;a\u0026nbsp;nightly basis.\u003c/p\u003e\n\u003cp\u003eYou\u0026rsquo;d need to\u0026nbsp;add checks for each step to\u0026nbsp;make sure it\u0026rsquo;s successfully finished and then clean up\u0026nbsp;all the temporary data files afterwards again.\u003c/p\u003e\n\u003cp\u003eAnd if\u0026nbsp;you need to\u0026nbsp;integrate data from multiple sources, in\u0026nbsp;multiple formats, it\u0026nbsp;would get very ugly very quickly.\u003c/p\u003e\n\u003cp\u003eThat\u0026rsquo;s where ClickHouse can help you.\u003c/p\u003e\n\u003cp\u003eClickHouse has extremely rich integration capabilities, reading from multiple sources (like s3, rest APIs, local files, external DBMS etc).\u003c/p\u003e\n\u003cp\u003eSo\u0026nbsp;even if\u0026nbsp;you don\u0026rsquo;t want to\u0026nbsp;use ClickHouse as\u0026nbsp;your daily driver for your analytical queries, you can still make use of\u0026nbsp;its integration features.\u003c/p\u003e\n\u003cp\u003eI\u0026nbsp;connected to\u0026nbsp;a\u0026nbsp;managed ClickHouse cluster within the DoubleCloud sitting in\u0026nbsp;the same AWS region as\u0026nbsp;my\u0026nbsp;test MySQL instance, and then just executed this query:\u003c/p\u003e\n\n    \u003cdiv class=\"yfm-clipboard\"\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs\"\u003eSET external_storage_connect_timeout_sec = 300;\nSET mysql_max_rows_to_insert = 10000;\nINSERT INTO FUNCTION mysql('{mysqlurl}',\n  test, sensor_data,\n  '{username}', '{password}') \n(station_id, date, tempMax, tempMin, tempAvg)\nSELECT station_id, date, tempMax, tempMin, tempAvg\nFROM s3('https://{s3path}/sensors.2020.json.gz', '{aws_access_id}', '{aws_secret_access_id}', JSONEachRow,\n  $$station_id String, date Date, tempMax Nullable(Int32), tempMin Nullable(Int32), tempAvg Nullable(Int32)$$)\n\u003c/code\u003e\u003c/pre\u003e\n\n    \u003csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\" class=\"yfm-clipboard-button\" data-animation=\"202\"\u003e\n        \u003cpath fill=\"currentColor\" d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\"\u003e\u003c/path\u003e\n        \u003cpath stroke=\"currentColor\" fill=\"transparent\" stroke-width=\"1.5\" d=\"M9.5 13l3 3l5 -5\" visibility=\"hidden\"\u003e\n            \u003canimate id=\"visibileAnimation-202\" attributeName=\"visibility\" from=\"hidden\" to=\"visible\" dur=\"0.2s\" fill=\"freeze\" begin\u003e\u003c/animate\u003e\n            \u003canimate id=\"hideAnimation-202\" attributeName=\"visibility\" from=\"visible\" to=\"hidden\" dur=\"1s\" begin=\"visibileAnimation-202.end+1\" fill=\"freeze\"\u003e\u003c/animate\u003e\n        \u003c/path\u003e\n    \u003c/svg\u003e\n    \u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003eThe part between the $$ signs can be\u0026nbsp;omitted in\u0026nbsp;most use cases, as\u0026nbsp;ClickHouse can do\u0026nbsp;schema inference, but in\u0026nbsp;my\u0026nbsp;test data this was not possible\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe whole process of\u0026nbsp;ingesting the data to\u0026nbsp;MySQL now took 2\u0026nbsp;minutes and 11\u0026nbsp;sec, and as\u0026nbsp;the inserts are automatically bunched to\u0026nbsp;10k, the cpu of\u0026nbsp;MySQL barely noticed anything.\u003c/p\u003e\n\u003cp\u003eBut not only is\u0026nbsp;it\u0026nbsp;30% faster on\u0026nbsp;the ingestion time, it\u0026nbsp;also did the downloading and decompression step.\u003c/p\u003e\n\u003cp\u003eAnd even better, it\u0026nbsp;saved me\u0026nbsp;a\u0026nbsp;lot of\u0026nbsp;time implementing the whole logic!\u003c/p\u003e\n\u003cp\u003eClickHouse does everything for me, I\u0026nbsp;just had to\u0026nbsp;execute a\u0026nbsp;single query and wait for the result, making it\u0026nbsp;a\u0026nbsp;lot easier on\u0026nbsp;the implementation side, to\u0026nbsp;handle possible errors, cleanups (what cleanups?) etc.\u003c/p\u003e\n\u003cp\u003eAnd as\u0026nbsp;a\u0026nbsp;bonus, I\u0026nbsp;also have the full tool set of\u0026nbsp;SQL at\u0026nbsp;my\u0026nbsp;hands directly on\u0026nbsp;insert already.\u003c/p\u003e\n\u003cp\u003eI\u0026nbsp;could add WHERE conditions to\u0026nbsp;filter out rows before inserting, can manipulate or\u0026nbsp;parse columns, or\u0026nbsp;I\u0026nbsp;can even do\u0026nbsp;aggregations before inserting the data to\u0026nbsp;MySQL!\u003c/p\u003e\n\u003cp\u003eIf\u0026nbsp;anyone knows a\u0026nbsp;faster way of\u0026nbsp;getting data into MySQL, please contact me\u0026nbsp;;).\u003c/p\u003e\n\u003ch1 id=\"conclusion\"\u003e\u003ca href=\"#conclusion\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eConclusion\u003c/span\u003e\u003c/a\u003eConclusion\u003c/h1\u003e"},{"type":"blog-media-block","column":"left","resetPaddings":true,"text":"","image":"/assets/blog/articles/total-ingestion-time-my-sql.png","fullWidth":false}]},{"type":"blog-layout-block","resetPaddings":true,"mobileOrder":"reverse","children":[{"type":"blog-yfm-block","column":"left","resetPaddings":true,"text":"\u003cp\u003eAs\u0026nbsp;we\u0026nbsp;can see, utilizing ClickHouse for ingesting data to\u0026nbsp;MySQL has been the fastest way to\u0026nbsp;do\u0026nbsp;so.\u003c/p\u003e\n\u003cp\u003eClickHouse can not only spare you time if\u0026nbsp;you use it\u0026nbsp;as\u0026nbsp;DBMS for your analytical queries but it\u0026nbsp;can also spare you a\u0026nbsp;lot of\u0026nbsp;implementation time.\u003c/p\u003e\n\u003cp\u003eWith ClickHouse it\u0026rsquo;s extremely easy to\u0026nbsp;convert data into different formats, reading from multiple sources (s3 was just an\u0026nbsp;example here, you can utilize RestAPIs, other DBMS, local files, Kafka, etc.), and doing data manipulation on\u0026nbsp;the fly.\u003c/p\u003e\n\u003cp\u003eSo\u0026nbsp;especially if\u0026nbsp;you have multiple sources of\u0026nbsp;data, it\u0026nbsp;makes the whole process of\u0026nbsp;collecting the data, downloading, converting etc. a\u0026nbsp;lot easier.\u003c/p\u003e\n\u003cp\u003eYou just need to\u0026nbsp;rewrite a\u0026nbsp;single query for echo of\u0026nbsp;your ingestion process, instead of\u0026nbsp;rewriting a\u0026nbsp;whole bunch of\u0026nbsp;commands for downloading, preparing inserting etc.\u003c/p\u003e\n\u003cp\u003eIn\u0026nbsp;the end you can still utilize your MySQL instances for all your preferred workloads, and still let ClickHouse help you be\u0026nbsp;faster overall!\u003c/p\u003e\n\u003cp\u003eAnd this is\u0026nbsp;the reason why as\u0026nbsp;a\u0026nbsp;tech guy, I\u0026nbsp;fell in\u0026nbsp;love with ClickHouse.\u003c/p\u003e\n\u003cp\u003eIt\u0026rsquo;s not because it\u0026rsquo;s fast as\u0026nbsp;hell in\u0026nbsp;analytical workloads (although it\u0026nbsp;is). It\u0026rsquo;s because it\u0026rsquo;s so\u0026nbsp;extremely versatile to\u0026nbsp;use, from prototypes to\u0026nbsp;production workloads or\u0026nbsp;just for having fun with.\u003c/p\u003e\n\u003cp\u003eClickHouse is\u0026nbsp;a\u0026nbsp;trademark of\u0026nbsp;ClickHouse, Inc. https://clickhouse.com\u003c/p\u003e"}]},{"type":"content-layout-block","background":{"src":"/assets/doublecloud/doublecloud-cover-1.png","style":{"backgroundColor":"#CA1551"}},"centered":true,"textContent":{"title":"Start your trial today","buttons":[{"text":"Start free trial","size":"promo","theme":"accent","url":"https://auth.double.cloud/s/signup"},{"text":"Contact us","theme":"pseudo","url":"#contact-us-form"}]}},{"type":"blog-suggest-block","resetPaddings":true,"fullWidth":false}]},"title":"Efficient data ingestion into MySQL with ClickHouse: streamlining 1 billion rows and local files","noIndex":false,"shareTitle":null,"shareDescription":null,"shareImage":"/assets/blog/articles/fastest-way-to-ingest-data-into-mysql-sharing.png","pageLocaleId":162,"author":"unknown","metaDescription":"Explore efficient data ingestion into MySQL using ClickHouse. Learn how to streamline the process for 1 billion rows and local files. Discover a faster way to manage data overhead.","keywords":[],"shareGenTitle":null,"canonicalLink":null,"sharingType":"custom","sharingTheme":"dark","comment":"meta update reversed","shareImageUrl":"/assets/blog/articles/fastest-way-to-ingest-data-into-mysql-sharing.png","pageRegionId":null,"service":null,"solution":null,"locales":[{"locale":"en","publishedVersionId":2245},{"locale":"ru","publishedVersionId":null}],"regions":[],"pageRegions":[]},"post":{"url":"","id":81,"name":"fastest-way-to-ingest-data-into-mysql","isPinned":false,"blogPostId":81,"image":"/assets/blog/articles/fastest-way-to-ingest-data-into-mysql-small-cover.png","readingTime":15,"date":"2023-03-23T00:00:00Z","likes":0,"hasUserLike":false,"services":[],"slug":"","authors":[],"locale":{"lang":"en"},"textTitle":"The fastest way to ingest data into MySQL is… ClickHouse®","htmlTitle":"The fastest way to\u0026nbsp;ingest data into MySQL is\u0026hellip; ClickHouse\u0026reg;","title":"The fastest way to ingest data into MySQL is… ClickHouse®","tags":[{"icon":null,"slug":"insights","name":"Insights","createdAt":"","updatedAt":"","count":0},{"icon":null,"slug":"ClickHouse","name":"ClickHouse","createdAt":"","updatedAt":"","count":0}],"metaTitle":"The fastest way to ingest data into MySQL is… ClickHouse®","description":"\u003cp\u003eWritten By: Stefan Kaeser, DoubleCloud Senior Solution Architect\u003c/p\u003e","content":"\u003cp\u003eIn\u0026nbsp;this article, we\u0026rsquo;ll talk about:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#1-billion-rows-to-ingest-what-to-do?\"\u003e1 billion rows to\u0026nbsp;ingest\u0026nbsp;\u0026mdash; What to\u0026nbsp;do?\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#ingesting-local-files-to-mysql\"\u003eIngesting local files to\u0026nbsp;MySQL\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#let-clickhouse-handle-all-the-overhead\"\u003eLet ClickHouse handle all the overhead\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#conclusion\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eMySQL.\u003c/p\u003e\n\u003cp\u003eIt\u0026rsquo;s often the first choice for start-up\u0026rsquo;s looking for open-source software to\u0026nbsp;store application data.\u003c/p\u003e\n\u003cp\u003eNow I\u0026rsquo;ve been working with MySQL myself for over two decades, and although I\u0026rsquo;ve seen and worked with other databases since then, it\u0026nbsp;will always have a\u0026nbsp;special place in\u0026nbsp;my\u0026nbsp;heart as\u0026nbsp;it\u0026nbsp;was the first one to\u0026nbsp;show me\u0026nbsp;how much fun working with SQL databases is.\u003c/p\u003e\n\u003cp\u003eOf\u0026nbsp;course MySQL is\u0026nbsp;an\u0026nbsp;OLTP database, its strength is\u0026nbsp;in\u0026nbsp;serving multiple thousands of\u0026nbsp;small queries and transactions, but it\u0026rsquo;s still one of\u0026nbsp;the most often used DBMS for general workloads as\u0026nbsp;well.\u003c/p\u003e\n\u003cp\u003eAs\u0026nbsp;I\u0026nbsp;mentioned, a\u0026nbsp;lot of\u0026nbsp;companies start with MySQL, building their app (s) around it, watching it\u0026nbsp;grow, and even though MySQL definitely wasn\u0026rsquo;t intended to\u0026nbsp;be\u0026nbsp;used as\u0026nbsp;an\u0026nbsp;analytical database, there are a\u0026nbsp;lot of\u0026nbsp;companies (and individuals) still out there using it\u0026nbsp;for exactly this purpose.\u003c/p\u003e\n\u003cp\u003eAnd whilst there are a\u0026nbsp;lot of\u0026nbsp;good reasons why you shouldn\u0026rsquo;t use MySQL for big analytical workloads, MySQL is\u0026nbsp;still a\u0026nbsp;lot more capable of\u0026nbsp;analytical processes than a\u0026nbsp;lot of\u0026nbsp;people think it\u0026nbsp;is.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ve seen single MySQL instances having multiple Terabytes of\u0026nbsp;data stored within itself, some with tables handling multiple billions of\u0026nbsp;rows without bigger issues.\u003c/p\u003e\n\u003cp\u003eThe concept of\u0026nbsp;allowing different storage engines within the same environment allows MySQL to\u0026nbsp;stretch over its own limits by\u0026nbsp;several orders of\u0026nbsp;magnitude.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ve seen quite impressive analytical queries achieved with the use of\u0026nbsp;TokuDB or\u0026nbsp;RocksDB storage engines that I\u0026nbsp;never thought MySQL would be\u0026nbsp;able to\u0026nbsp;fulfill.\u003c/p\u003e\n\u003cp\u003eStill, MySQL was never meant to\u0026nbsp;be\u0026nbsp;used for big analytical workloads, as\u0026nbsp;it\u0026nbsp;was designed and built back in\u0026nbsp;a\u0026nbsp;time when clouds were meant to\u0026nbsp;be\u0026nbsp;in\u0026nbsp;the sky and formats of\u0026nbsp;data were proprietary instead of\u0026nbsp;being used for big open exchanges.\u003c/p\u003e\n\u003cp\u003eSo\u0026nbsp;one of\u0026nbsp;the biggest problems when doing analytical stuff in\u0026nbsp;MySQL is\u0026nbsp;still\u0026hellip; How do\u0026nbsp;you get the data in\u0026nbsp;it?\u003c/p\u003e\n\u003ch1 id=\"1-billion-rows-to-ingest-what-to-do?\"\u003e\u003ca href=\"#1-billion-rows-to-ingest-what-to-do?\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003e1 billion rows to\u0026nbsp;ingest\u0026nbsp;\u0026mdash; What to\u0026nbsp;do?\u003c/span\u003e\u003c/a\u003e1 billion rows to\u0026nbsp;ingest\u0026nbsp;\u0026mdash; What to\u0026nbsp;do?\u003c/h1\u003e\n\u003cp\u003eLately I\u0026rsquo;ve been running benchmarks on\u0026nbsp;different DBMS.\u003c/p\u003e\n\u003cp\u003eThe data set itself contained around 1\u0026nbsp;billion rows of\u0026nbsp;data, split into yearly chunks of\u0026nbsp;around 10\u0026nbsp;million rows, formatted in\u0026nbsp;one line of\u0026nbsp;JSON data per row, compressed via gzip, and located in\u0026nbsp;a\u0026nbsp;bucket in\u0026nbsp;Amazon\u0026rsquo;s S3.\u003c/p\u003e\n\u003cp\u003eEach row has 11\u0026nbsp;columns, but for our benchmark we\u0026nbsp;only need to\u0026nbsp;extract 5\u0026nbsp;of\u0026nbsp;those: date, station_id, tempMax, tempMin, tempAvg\u003c/p\u003e\n\u003cp\u003eFor this blog, I\u0026nbsp;will limit our data set to\u0026nbsp;only one year of\u0026nbsp;data, choosing 2020\u0026nbsp;at\u0026nbsp;random.\u003c/p\u003e\n\u003cp\u003eThe gzip compressed JSON data is\u0026nbsp;98MB.\u003c/p\u003e\n\u003cp\u003eWhen decompressed the file size grows to\u0026nbsp;2.7 GB, which is\u0026nbsp;expected as\u0026nbsp;the column names contained within the JSON are of\u0026nbsp;course very compressible.\u003c/p\u003e\n\u003cp\u003eThe total number of\u0026nbsp;rows contained within the file is\u0026nbsp;12,095,646.\u003c/p\u003e\n\u003cp\u003eFor our benchmark we\u0026nbsp;chose a\u0026nbsp;small Amazon RDS instance with 2\u0026nbsp;vCPU cores, 8GB of\u0026nbsp;RAM and enough storage space to\u0026nbsp;store the entire billion rows.\u003c/p\u003e\n\u003cp\u003eAs\u0026nbsp;we\u0026nbsp;only want to\u0026nbsp;show ingestion logic within this blog post, we\u0026nbsp;limited the data set to\u0026nbsp;2020, so\u0026nbsp;the whole data set would fit into ram.\u003c/p\u003e\n\u003cp\u003eThe table definition for our ingestion case is\u0026nbsp;quite simple.\u003c/p\u003e\n\u003cp\u003eWe\u0026nbsp;have a\u0026nbsp;primary key with 2\u0026nbsp;columns (date, station_id) and a\u0026nbsp;secondary index on\u0026nbsp;station_id for a\u0026nbsp;somewhat realistic scenario:\u003c/p\u003e\n\n    \u003cdiv class=\"yfm-clipboard\"\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs\"\u003eCREATE TABLE `sensor_data` (\n  `station_id` char(11) CHARACTER SET ascii COLLATE ascii_bin NOT NULL,\n  `date` date NOT NULL,\n  `tempAvg` int DEFAULT NULL,\n  `tempMax` int DEFAULT NULL,\n  `tempMin` int DEFAULT NULL,\n  PRIMARY KEY (`date`,`station_id`),\n  KEY `station` (`station_id`)\n) ENGINE=InnoDB;\n\u003c/code\u003e\u003c/pre\u003e\n\n    \u003csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\" class=\"yfm-clipboard-button\" data-animation=\"69\"\u003e\n        \u003cpath fill=\"currentColor\" d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\"\u003e\u003c/path\u003e\n        \u003cpath stroke=\"currentColor\" fill=\"transparent\" stroke-width=\"1.5\" d=\"M9.5 13l3 3l5 -5\" visibility=\"hidden\"\u003e\n            \u003canimate id=\"visibileAnimation-69\" attributeName=\"visibility\" from=\"hidden\" to=\"visible\" dur=\"0.2s\" fill=\"freeze\" begin\u003e\u003c/animate\u003e\n            \u003canimate id=\"hideAnimation-69\" attributeName=\"visibility\" from=\"visible\" to=\"hidden\" dur=\"1s\" begin=\"visibileAnimation-69.end+1\" fill=\"freeze\"\u003e\u003c/animate\u003e\n        \u003c/path\u003e\n    \u003c/svg\u003e\n    \u003c/div\u003e\n\u003cp\u003eI\u0026nbsp;chose InnoDB as\u0026nbsp;the storage engine, as\u0026nbsp;it\u0026rsquo;s the most commonly used table engine in\u0026nbsp;MySQL.\u003c/p\u003e\n\u003ch3 id=\"how-to-start?\"\u003e\u003ca href=\"#how-to-start?\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eHow to\u0026nbsp;start?\u003c/span\u003e\u003c/a\u003eHow to\u0026nbsp;start?\u003c/h3\u003e\n\u003cp\u003eWhen trying to\u0026nbsp;ingest our test data into MySQL is\u0026nbsp;where we\u0026nbsp;hit our first hurdle.\u003c/p\u003e\n\u003cp\u003eMySQL can\u0026rsquo;t read directly from S3 or\u0026nbsp;unzip the data on\u0026nbsp;the\u0026nbsp;fly. Therefore we\u0026nbsp;have to\u0026nbsp;do\u0026nbsp;some manual steps, before we\u0026nbsp;can even start:\u003c/p\u003e\n\u003cp\u003eDownload the files: Depending on\u0026nbsp;if\u0026nbsp;you need these tasks regularly or\u0026nbsp;just one time; this step could be\u0026nbsp;easy or\u0026nbsp;complex and the amount of\u0026nbsp;time needed to\u0026nbsp;implement that logic cannot be\u0026nbsp;estimated in\u0026nbsp;a\u0026nbsp;general blog. Also the download speed depends on\u0026nbsp;multiple factors, so\u0026nbsp;we\u0026nbsp;won\u0026rsquo;t include it\u0026nbsp;here in\u0026nbsp;our comparison, just mention that it\u0026nbsp;can have an\u0026nbsp;impact.\u003c/p\u003e\n\u003cp\u003eDecompressing the files: The easiest way to\u0026nbsp;achieve this is\u0026nbsp;by\u0026nbsp;using gzip -d {filename} on\u0026nbsp;the cli.\u003c/p\u003e\n\u003cp\u003eDecompressing files of\u0026nbsp;that size normally doesn\u0026rsquo;t take much time, but still, we\u0026nbsp;have to\u0026nbsp;write the decompressed data to\u0026nbsp;the disk. In\u0026nbsp;my\u0026nbsp;tests it\u0026nbsp;took around 7\u0026nbsp;seconds per file.\u003c/p\u003e\n\u003cp\u003eNow that we\u0026nbsp;have the files locally and decompressed, we\u0026nbsp;can really start to\u0026nbsp;ingest the data.\u003c/p\u003e\n\u003cp\u003eWe\u0026nbsp;need to\u0026nbsp;read the JSON data, get rid of\u0026nbsp;the unwanted columns, and \u0026ldquo;only\u0026rdquo; ingest the 12\u0026nbsp;million rows of\u0026nbsp;data into MySQL.\u003c/p\u003e\n\u003ch1 id=\"ingesting-local-files-to-mysql\"\u003e\u003ca href=\"#ingesting-local-files-to-mysql\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eIngesting local files to\u0026nbsp;MySQL\u003c/span\u003e\u003c/a\u003eIngesting local files to\u0026nbsp;MySQL\u003c/h1\u003e\n\u003ch3 id=\"use-scripting-language-like-php\"\u003e\u003ca href=\"#use-scripting-language-like-php\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eUse scripting language like PHP\u003c/span\u003e\u003c/a\u003eUse scripting language like PHP\u003c/h3\u003e\n\u003cp\u003eA\u0026nbsp;lot of\u0026nbsp;environments which use MySQL as\u0026nbsp;their main DBMS also make use of\u0026nbsp;PHP (remember LAMP stack anyone?).\u003c/p\u003e\n\u003cp\u003eA\u0026nbsp;simple solution to\u0026nbsp;achieve our goal would be\u0026nbsp;to\u0026nbsp;read the JSON file one line at\u0026nbsp;a\u0026nbsp;time, use PHPs parsing functionality to\u0026nbsp;get the values of\u0026nbsp;the 5\u0026nbsp;needed columns and insert them directly into MySQL.\u003c/p\u003e\n\u003cp\u003eSomething like this:\u003c/p\u003e\n\n    \u003cdiv class=\"yfm-clipboard\"\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs\"\u003e$mysqli = new mysqli($mysql_host, $mysql_username, $mysql_password, '', $mysql_port);\n$filename = $argv[1];\n\n$handle = @fopen($filename, 'r');\nif ($handle)\n{\n  while (($buffer = fgets($handle, 4096)) !== false)\n  {\n      $row = json_decode($buffer, true);\n      $mysqli-\u0026gt;query(\"\n        INSERT IGNORE INTO test.sensor_data\n          (station_id, date, tempMax, tempMin, tempAvg)\n        VALUES\n          ('{$row['station_id']}', '{$row['date']}', \" .\n              ($row['tempMax'] ?? 'NULL') . \", \" . \n              ($row['tempMin'] ?? 'NULL') . \", \" . \n              ($row['tempAvg'] ?? 'NULL') . \")  \n      \");\n  }\n  fclose($handle);\n}\nexit(0);\n\u003c/code\u003e\u003c/pre\u003e\n\n    \u003csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\" class=\"yfm-clipboard-button\" data-animation=\"112\"\u003e\n        \u003cpath fill=\"currentColor\" d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\"\u003e\u003c/path\u003e\n        \u003cpath stroke=\"currentColor\" fill=\"transparent\" stroke-width=\"1.5\" d=\"M9.5 13l3 3l5 -5\" visibility=\"hidden\"\u003e\n            \u003canimate id=\"visibileAnimation-112\" attributeName=\"visibility\" from=\"hidden\" to=\"visible\" dur=\"0.2s\" fill=\"freeze\" begin\u003e\u003c/animate\u003e\n            \u003canimate id=\"hideAnimation-112\" attributeName=\"visibility\" from=\"visible\" to=\"hidden\" dur=\"1s\" begin=\"visibileAnimation-112.end+1\" fill=\"freeze\"\u003e\u003c/animate\u003e\n        \u003c/path\u003e\n    \u003c/svg\u003e\n    \u003c/div\u003e\n\u003cp\u003eDoing it\u0026nbsp;this way unfortunately has the overhead of\u0026nbsp;executing a\u0026nbsp;single transaction for each row of\u0026nbsp;the dataset, hence only achieving around 60\u0026nbsp;rows/second, or\u0026nbsp;taking approx. 59\u0026nbsp;hours for the 12\u0026nbsp;million rows.\u003c/p\u003e\n\u003cp\u003eHowever, it\u0026rsquo;s easy to\u0026nbsp;optimize this logic by\u0026nbsp;introducing the bunching of\u0026nbsp;rows, combining 10k rows per insert query, speeding up\u0026nbsp;the whole process to\u0026nbsp;take 3\u0026nbsp;minutes and 30\u0026nbsp;seconds for the whole data set.\u003c/p\u003e\n\u003cp\u003eIngestion speed is\u0026nbsp;okay for the amount of\u0026nbsp;data but it\u0026nbsp;required a\u0026nbsp;lot of\u0026nbsp;manual work and no\u0026nbsp;piece of\u0026nbsp;error handling is\u0026nbsp;written yet.\u003c/p\u003e\n\u003cp\u003eBut there are ways to\u0026nbsp;optimize data ingestion in\u0026nbsp;MySQL.\u003c/p\u003e\n\u003ch3 id=\"use-json-functionality-with-mysql\"\u003e\u003ca href=\"#use-json-functionality-with-mysql\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eUse JSON functionality with MySQL\u003c/span\u003e\u003c/a\u003eUse JSON functionality with MySQL\u003c/h3\u003e\n\u003cp\u003eUnfortunately MySQL has no\u0026nbsp;feature for working with JSON files directly, but we\u0026nbsp;can interpret the whole file as\u0026nbsp;a\u0026nbsp;CSV file, having only a\u0026nbsp;single column containing a\u0026nbsp;JSON string.\u003c/p\u003e\n\u003cp\u003eThat way we\u0026nbsp;can load the JSON file via the LOAD DATA command, and then select the needed columns via the built-in JSON functions of\u0026nbsp;MySQL.\u003c/p\u003e\n\n    \u003cdiv class=\"yfm-clipboard\"\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs\"\u003eLOAD DATA LOCAL INFILE '~/MySQLBlog/sensors.2020.json'\nINTO TABLE test.sensor_data  \n(@json)\nSET station_id = JSON_UNQUOTE(JSON_EXTRACT(@json, '$.station_id')),\n  date = JSON_UNQUOTE(JSON_EXTRACT(@json, '$.date')),\n  tempMax = nullif(JSON_UNQUOTE(JSON_EXTRACT(@json, '$.tempMax')), 'null'),\n  tempMin = nullif(JSON_UNQUOTE(JSON_EXTRACT(@json, '$.tempMin')), 'null'),\n  tempAvg = nullif(JSON_UNQUOTE(JSON_EXTRACT(@json, '$.tempAvg')), 'null')\n\u003c/code\u003e\u003c/pre\u003e\n\n    \u003csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\" class=\"yfm-clipboard-button\" data-animation=\"134\"\u003e\n        \u003cpath fill=\"currentColor\" d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\"\u003e\u003c/path\u003e\n        \u003cpath stroke=\"currentColor\" fill=\"transparent\" stroke-width=\"1.5\" d=\"M9.5 13l3 3l5 -5\" visibility=\"hidden\"\u003e\n            \u003canimate id=\"visibileAnimation-134\" attributeName=\"visibility\" from=\"hidden\" to=\"visible\" dur=\"0.2s\" fill=\"freeze\" begin\u003e\u003c/animate\u003e\n            \u003canimate id=\"hideAnimation-134\" attributeName=\"visibility\" from=\"visible\" to=\"hidden\" dur=\"1s\" begin=\"visibileAnimation-134.end+1\" fill=\"freeze\"\u003e\u003c/animate\u003e\n        \u003c/path\u003e\n    \u003c/svg\u003e\n    \u003c/div\u003e\n\u003cp\u003eThis approach took 4\u0026nbsp;minutes and 55\u0026nbsp;seconds, so\u0026nbsp;even slower than the \u0026lsquo;optimized\u0026rsquo; PHP solution, but at\u0026nbsp;least it\u0026rsquo;s easier to\u0026nbsp;type (At\u0026nbsp;least for a\u0026nbsp;SQL guy like myself).\u003c/p\u003e\n\u003cp\u003eThe main problem with this approach taking so\u0026nbsp;long though is\u0026nbsp;that we\u0026nbsp;still send the data of\u0026nbsp;all the columns to\u0026nbsp;the MySQL server, and then throw all but 5\u0026nbsp;away within the server, hence having unneeded data sent over the wire to\u0026nbsp;MySQL.\u003c/p\u003e\n\u003cp\u003eAlso loading 10\u0026nbsp;million rows in\u0026nbsp;a\u0026nbsp;single transaction issues quite a\u0026nbsp;load to\u0026nbsp;the server.\u003c/p\u003e\n\u003ch3 id=\"use-cli-and-csv-functionality-with-mysql\"\u003e\u003ca href=\"#use-cli-and-csv-functionality-with-mysql\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eUse Cli and CSV functionality with MySQL\u003c/span\u003e\u003c/a\u003eUse Cli and CSV functionality with MySQL\u003c/h3\u003e\n\u003cp\u003eWe\u0026nbsp;can avoid using the JSON functionality of\u0026nbsp;MySQL as\u0026nbsp;well as\u0026nbsp;the sending of\u0026nbsp;unwanted data by\u0026nbsp;adding another preparation step before the ingestion to\u0026nbsp;MySQL.\u003c/p\u003e\n\u003cp\u003eWe\u0026nbsp;can use a\u0026nbsp;command line JSON tool like jq\u0026nbsp;to\u0026nbsp;select only the needed columns and create a\u0026nbsp;comma separated file, only containing the 5\u0026nbsp;needed columns:\u003c/p\u003e\n\u003cp\u003ejq\u0026nbsp;-r '. | [.station_id, .date, .tempMax, .tempMin, .tempAvg] | @csv' sensors.2020.json \u0026gt; sensors_parsed.2020.csv\u003c/p\u003e\n\u003cp\u003eParsing JSON to\u0026nbsp;CSV on\u0026nbsp;cli takes around 60\u0026nbsp;seconds in\u0026nbsp;that case, generating a\u0026nbsp;CSV file with 376\u0026nbsp;MB\u0026nbsp;of\u0026nbsp;data.\u003c/p\u003e\n\u003cp\u003eNow importing the csv file can be\u0026nbsp;done quite easily within the MySQL client again:\u003c/p\u003e\n\n    \u003cdiv class=\"yfm-clipboard\"\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs\"\u003eLOAD DATA LOCAL INFILE '~/MySQLBlog/sensors_parsed.2020.csv'\nINTO TABLE test.sensor_data  \nFIELDS TERMINATED by ',' OPTIONALLY ENCLOSED BY '\"'\n(station_id, date, @tempMax, @tempMin, @tempAvg)\nSET tempMax = nullif(@tempMax, ''),\n  tempMin = nullif(@tempMin, ''),\n  tempAvg = nullif(@tempAvg, '')\n\u003c/code\u003e\u003c/pre\u003e\n\n    \u003csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\" class=\"yfm-clipboard-button\" data-animation=\"162\"\u003e\n        \u003cpath fill=\"currentColor\" d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\"\u003e\u003c/path\u003e\n        \u003cpath stroke=\"currentColor\" fill=\"transparent\" stroke-width=\"1.5\" d=\"M9.5 13l3 3l5 -5\" visibility=\"hidden\"\u003e\n            \u003canimate id=\"visibileAnimation-162\" attributeName=\"visibility\" from=\"hidden\" to=\"visible\" dur=\"0.2s\" fill=\"freeze\" begin\u003e\u003c/animate\u003e\n            \u003canimate id=\"hideAnimation-162\" attributeName=\"visibility\" from=\"visible\" to=\"hidden\" dur=\"1s\" begin=\"visibileAnimation-162.end+1\" fill=\"freeze\"\u003e\u003c/animate\u003e\n        \u003c/path\u003e\n    \u003c/svg\u003e\n    \u003c/div\u003e\n\u003cp\u003eThis approach takes 1\u0026nbsp;minute 52\u0026nbsp;seconds, which is\u0026nbsp;a\u0026nbsp;lot faster than reading directly from JSON to\u0026nbsp;MySQL.\u003c/p\u003e\n\u003cp\u003eWe\u0026nbsp;still, however, have the problem of\u0026nbsp;big transactions. So\u0026nbsp;if\u0026nbsp;we\u0026nbsp;want our MySQL instance to\u0026nbsp;handle other requests without trouble, we\u0026rsquo;d need to\u0026nbsp;split the csv file into smaller chunks of\u0026nbsp;10k rows to\u0026nbsp;reduce the load on\u0026nbsp;the server.\u003c/p\u003e\n\u003cp\u003eThus we\u0026nbsp;again need to\u0026nbsp;write simple cli logic to\u0026nbsp;split the csv into chunks and then write a\u0026nbsp;loop to\u0026nbsp;go\u0026nbsp;over the 1,000 chunk files, ingesting one after another.\u003c/p\u003e\n\u003cp\u003eOverall, regarding the 60\u0026nbsp;sec of\u0026nbsp;cli time and the 1\u0026nbsp;min 52\u0026nbsp;sec for inserting the data, the total time to\u0026nbsp;ingest our 12\u0026nbsp;million JSON rows takes 2\u0026nbsp;min 52\u0026nbsp;sec, therefore being 40% faster than reading JSON directly from MySQL and still being 17% faster than the scripted solution.\u003c/p\u003e\n\u003ch1 id=\"let-clickhouse-handle-all-the-overhead\"\u003e\u003ca href=\"#let-clickhouse-handle-all-the-overhead\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eLet ClickHouse handle all the overhead\u003c/span\u003e\u003c/a\u003eLet ClickHouse handle all the overhead\u003c/h1\u003e\n\u003cp\u003eNow even though we\u0026nbsp;can reach a\u0026nbsp;somewhat acceptable speed with cli tools and the mysql client, it\u0026rsquo;s still not a\u0026nbsp;very nice approach.\u003c/p\u003e\n\u003cp\u003eAlso\u0026hellip; imagine if\u0026nbsp;you had to\u0026nbsp;do\u0026nbsp;this on\u0026nbsp;a\u0026nbsp;regular basis, as\u0026nbsp;your software imports data from external sources on\u0026nbsp;a\u0026nbsp;nightly basis.\u003c/p\u003e\n\u003cp\u003eYou\u0026rsquo;d need to\u0026nbsp;add checks for each step to\u0026nbsp;make sure it\u0026rsquo;s successfully finished and then clean up\u0026nbsp;all the temporary data files afterwards again.\u003c/p\u003e\n\u003cp\u003eAnd if\u0026nbsp;you need to\u0026nbsp;integrate data from multiple sources, in\u0026nbsp;multiple formats, it\u0026nbsp;would get very ugly very quickly.\u003c/p\u003e\n\u003cp\u003eThat\u0026rsquo;s where ClickHouse can help you.\u003c/p\u003e\n\u003cp\u003eClickHouse has extremely rich integration capabilities, reading from multiple sources (like s3, rest APIs, local files, external DBMS etc).\u003c/p\u003e\n\u003cp\u003eSo\u0026nbsp;even if\u0026nbsp;you don\u0026rsquo;t want to\u0026nbsp;use ClickHouse as\u0026nbsp;your daily driver for your analytical queries, you can still make use of\u0026nbsp;its integration features.\u003c/p\u003e\n\u003cp\u003eI\u0026nbsp;connected to\u0026nbsp;a\u0026nbsp;managed ClickHouse cluster within the DoubleCloud sitting in\u0026nbsp;the same AWS region as\u0026nbsp;my\u0026nbsp;test MySQL instance, and then just executed this query:\u003c/p\u003e\n\n    \u003cdiv class=\"yfm-clipboard\"\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs\"\u003eSET external_storage_connect_timeout_sec = 300;\nSET mysql_max_rows_to_insert = 10000;\nINSERT INTO FUNCTION mysql('{mysqlurl}',\n  test, sensor_data,\n  '{username}', '{password}') \n(station_id, date, tempMax, tempMin, tempAvg)\nSELECT station_id, date, tempMax, tempMin, tempAvg\nFROM s3('https://{s3path}/sensors.2020.json.gz', '{aws_access_id}', '{aws_secret_access_id}', JSONEachRow,\n  $$station_id String, date Date, tempMax Nullable(Int32), tempMin Nullable(Int32), tempAvg Nullable(Int32)$$)\n\u003c/code\u003e\u003c/pre\u003e\n\n    \u003csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\" class=\"yfm-clipboard-button\" data-animation=\"202\"\u003e\n        \u003cpath fill=\"currentColor\" d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\"\u003e\u003c/path\u003e\n        \u003cpath stroke=\"currentColor\" fill=\"transparent\" stroke-width=\"1.5\" d=\"M9.5 13l3 3l5 -5\" visibility=\"hidden\"\u003e\n            \u003canimate id=\"visibileAnimation-202\" attributeName=\"visibility\" from=\"hidden\" to=\"visible\" dur=\"0.2s\" fill=\"freeze\" begin\u003e\u003c/animate\u003e\n            \u003canimate id=\"hideAnimation-202\" attributeName=\"visibility\" from=\"visible\" to=\"hidden\" dur=\"1s\" begin=\"visibileAnimation-202.end+1\" fill=\"freeze\"\u003e\u003c/animate\u003e\n        \u003c/path\u003e\n    \u003c/svg\u003e\n    \u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003eThe part between the $$ signs can be\u0026nbsp;omitted in\u0026nbsp;most use cases, as\u0026nbsp;ClickHouse can do\u0026nbsp;schema inference, but in\u0026nbsp;my\u0026nbsp;test data this was not possible\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe whole process of\u0026nbsp;ingesting the data to\u0026nbsp;MySQL now took 2\u0026nbsp;minutes and 11\u0026nbsp;sec, and as\u0026nbsp;the inserts are automatically bunched to\u0026nbsp;10k, the cpu of\u0026nbsp;MySQL barely noticed anything.\u003c/p\u003e\n\u003cp\u003eBut not only is\u0026nbsp;it\u0026nbsp;30% faster on\u0026nbsp;the ingestion time, it\u0026nbsp;also did the downloading and decompression step.\u003c/p\u003e\n\u003cp\u003eAnd even better, it\u0026nbsp;saved me\u0026nbsp;a\u0026nbsp;lot of\u0026nbsp;time implementing the whole logic!\u003c/p\u003e\n\u003cp\u003eClickHouse does everything for me, I\u0026nbsp;just had to\u0026nbsp;execute a\u0026nbsp;single query and wait for the result, making it\u0026nbsp;a\u0026nbsp;lot easier on\u0026nbsp;the implementation side, to\u0026nbsp;handle possible errors, cleanups (what cleanups?) etc.\u003c/p\u003e\n\u003cp\u003eAnd as\u0026nbsp;a\u0026nbsp;bonus, I\u0026nbsp;also have the full tool set of\u0026nbsp;SQL at\u0026nbsp;my\u0026nbsp;hands directly on\u0026nbsp;insert already.\u003c/p\u003e\n\u003cp\u003eI\u0026nbsp;could add WHERE conditions to\u0026nbsp;filter out rows before inserting, can manipulate or\u0026nbsp;parse columns, or\u0026nbsp;I\u0026nbsp;can even do\u0026nbsp;aggregations before inserting the data to\u0026nbsp;MySQL!\u003c/p\u003e\n\u003cp\u003eIf\u0026nbsp;anyone knows a\u0026nbsp;faster way of\u0026nbsp;getting data into MySQL, please contact me\u0026nbsp;;).\u003c/p\u003e\n\u003ch1 id=\"conclusion\"\u003e\u003ca href=\"#conclusion\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eConclusion\u003c/span\u003e\u003c/a\u003eConclusion\u003c/h1\u003e\n\n\u003cp\u003eAs\u0026nbsp;we\u0026nbsp;can see, utilizing ClickHouse for ingesting data to\u0026nbsp;MySQL has been the fastest way to\u0026nbsp;do\u0026nbsp;so.\u003c/p\u003e\n\u003cp\u003eClickHouse can not only spare you time if\u0026nbsp;you use it\u0026nbsp;as\u0026nbsp;DBMS for your analytical queries but it\u0026nbsp;can also spare you a\u0026nbsp;lot of\u0026nbsp;implementation time.\u003c/p\u003e\n\u003cp\u003eWith ClickHouse it\u0026rsquo;s extremely easy to\u0026nbsp;convert data into different formats, reading from multiple sources (s3 was just an\u0026nbsp;example here, you can utilize RestAPIs, other DBMS, local files, Kafka, etc.), and doing data manipulation on\u0026nbsp;the fly.\u003c/p\u003e\n\u003cp\u003eSo\u0026nbsp;especially if\u0026nbsp;you have multiple sources of\u0026nbsp;data, it\u0026nbsp;makes the whole process of\u0026nbsp;collecting the data, downloading, converting etc. a\u0026nbsp;lot easier.\u003c/p\u003e\n\u003cp\u003eYou just need to\u0026nbsp;rewrite a\u0026nbsp;single query for echo of\u0026nbsp;your ingestion process, instead of\u0026nbsp;rewriting a\u0026nbsp;whole bunch of\u0026nbsp;commands for downloading, preparing inserting etc.\u003c/p\u003e\n\u003cp\u003eIn\u0026nbsp;the end you can still utilize your MySQL instances for all your preferred workloads, and still let ClickHouse help you be\u0026nbsp;faster overall!\u003c/p\u003e\n\u003cp\u003eAnd this is\u0026nbsp;the reason why as\u0026nbsp;a\u0026nbsp;tech guy, I\u0026nbsp;fell in\u0026nbsp;love with ClickHouse.\u003c/p\u003e\n\u003cp\u003eIt\u0026rsquo;s not because it\u0026rsquo;s fast as\u0026nbsp;hell in\u0026nbsp;analytical workloads (although it\u0026nbsp;is). It\u0026rsquo;s because it\u0026rsquo;s so\u0026nbsp;extremely versatile to\u0026nbsp;use, from prototypes to\u0026nbsp;production workloads or\u0026nbsp;just for having fun with.\u003c/p\u003e\n\u003cp\u003eClickHouse is\u0026nbsp;a\u0026nbsp;trademark of\u0026nbsp;ClickHouse, Inc. https://clickhouse.com\u003c/p\u003e\n"},"suggestedPosts":[{"url":"/blog/posts/2022/12/advantages-of-integrating-clickhouse-with-hybrid-s3-storage","id":45,"name":"advantages-of-integrating-clickhouse-with-hybrid-s3-storage","date":"2022-12-26T00:00:00Z","description":"","readingTime":15,"image":"/assets/blog/articles/advantages-of-integrating-clickhouse-with-hybrid-s3-storage-small-cover.png","blogPostId":45,"likes":0,"hasUserLike":false,"slug":"","title":"Advantages of integrating ClickHouse® with Hybrid S3 Storage","authors":[],"tags":[],"locale":{"lang":"en"},"textTitle":"Advantages of integrating ClickHouse® with Hybrid S3 Storage","htmlTitle":"Advantages of integrating ClickHouse® with Hybrid S3 Storage"},{"url":"/blog/posts/2022/12/how-to-leverage-jupyter-with-doublecloud-managed-clickhouse","id":39,"name":"how-to-leverage-jupyter-with-doublecloud-managed-clickhouse","date":"2022-12-08T00:00:00Z","description":"Written by: Adam Jennings, Senior Solutions Architect, DoubleCloud","readingTime":10,"image":"/assets/blog/articles/jupyter-small-cover.png","blogPostId":39,"likes":0,"hasUserLike":false,"slug":"","title":"How to leverage Jupyter with DoubleCloud’s Managed ClickHouse®","authors":[],"tags":[],"locale":{"lang":"en"},"textTitle":"How to leverage Jupyter with DoubleCloud’s Managed ClickHouse®","htmlTitle":"How to leverage Jupyter with DoubleCloud’s Managed ClickHouse®"},{"url":"/blog/posts/2023/03/machine-learning-with-clickhouse-and-doublecloud","id":74,"name":"machine-learning-with-clickhouse-and-doublecloud","date":"2023-03-14T00:00:00Z","description":"Written by: Adam Jennings, DoubleCloud Solution Architect","readingTime":15,"image":"/assets/blog/articles/machine-learning-with-clickhouse-and-doublecloud-small-cover.png","blogPostId":74,"likes":0,"hasUserLike":false,"slug":"","title":"Machine learning with ClickHouse and DoubleCloud","authors":[],"tags":[],"locale":{"lang":"en"},"textTitle":"Machine learning with ClickHouse and DoubleCloud","htmlTitle":"Machine learning with ClickHouse and DoubleCloud"}]}},"navigationData":{"newMenu":true,"header":{"leftItems":[{"text":"Why DoubleCloud","type":"dc-dropdown","data":{"view":"list","groups":[{"items":[{"text":"Performance","url":"/performance-boost/","description":"Get the best performance with the highest ROI"},{"text":"Security","url":"/security/","description":"Keep your data protected and maintain compliance"},{"text":"DoubleCloud vs. other solutions","url":"/comparison/","description":"Learn how DoubleCloud’s products compare to other solutions"},{"text":"Customer stories","url":"/resources/case-studies/","description":"See our solutions in action"}]},{"image":{"src":"/assets/doublecloud/menu-bar/menu-banner-dc-results.png.webp","style":{"width":300,"height":300}},"text":"\u003ca href='/performance-boost/' target='_self'\u003eGet more and spend less with DoubleCloud  →\u003c/a\u003e"}]}},{"text":"Products","type":"dc-dropdown","metaSchema":{"@graph":[{"@type":"SoftwareApplication","sameAs":["https://twitter.com/getdoublecloud","https://www.youtube.com/@doublecloud2499","https://www.linkedin.com/company/doublecloudplatform/","https://www.facebook.com/GetDoubleCloud/"]},{"@context":"https://schema.org","@type":"Organization","foundingDate":2022,"contactPoint":{"@type":"ContactPoint","contactType":"customer support","telephone":"+1 302-658-7581","email":"info@double.cloud"},"sameAs":["https://twitter.com/getdoublecloud","https://www.youtube.com/@doublecloud2499","https://www.linkedin.com/company/doublecloudplatform/","https://www.facebook.com/GetDoubleCloud/"]}]},"data":{"items":[{"text":"Managed Service for ClickHouse®","url":"/services/managed-clickhouse/","icon":"/assets/icons/dc-clickhouse.svg","description":"The fastest, most resource-efficient OLAP database for real-time analytics"},{"text":"Managed Service for Apache Kafka®","url":"/services/managed-kafka/","icon":"/assets/icons/dc-kafka.svg","description":"A leading data streaming technology for large-scale, data-intensive applications"},{"text":"Managed Service for Apache Airflow®","url":"/services/managed-airflow/","icon":"/assets/icons/dc-airflow.svg","description":"Open-source tool to orchestrate and monitor workflows"},{"text":"Data Transfer","url":"/services/doublecloud-transfer/","icon":"/assets/icons/dc-transfer.svg","description":"No-code ELT tool for aggregating, collecting, and migrating data"},{"text":"Data Visualization","url":"/services/doublecloud-visualization/","icon":"/assets/icons/dc-data-vis.svg","description":"Free tool to create, modify, and share dashboards and charts"}]}},{"text":"Solutions","type":"dc-dropdown","data":{"view":"list","groups":[{"title":"By use case","items":[{"text":"Customer-facing analytics","url":"/solutions/customer-facing-analytics/","description":"Provide business insights for your clients or partners"},{"text":"Real-time analytics","url":"/solutions/real-time-analytics/","description":"Build a data infrastructure to collect, process, and analyze data in real time"},{"text":"Observability and monitoring","url":"/solutions/observability-and-monitoring/","description":"Analyze terabytes of your logs, events, and traces with ease"}]},{"title":"By industry","items":[{"text":"AdTech and MarTech data analytics","url":"/solutions/adtech/","description":"Extract and analyze data from Meta ads, Google ads, LinkedIn ads, and others"},{"text":"Analytics for mobile and gaming apps","url":"/solutions/web-mobile-gaming-apps/","description":"Optimize and scale your mobile and gaming app analytics"},{"text":"EdTech data analytics","url":"/solutions/edtech/","description":"Improve online learning and identify new sales opportunities"},{"text":"FinTech data analytics","url":"/solutions/fintech-real-time-analytics/","description":"Manage and process large amounts of financial data efficiently"}]}]}},{"text":"Resources","type":"dc-dropdown","data":{"view":"list","groups":[{"title":"Using DoubleCloud","items":[{"text":"DoubleCloud API","url":"/docs/en/public-api/","description":"Read up on API tutorials and instructions","target":"_self"},{"text":"Terraform","url":"/docs/en/developer-resources/terraform/create-resources","description":"Deploy and manage cloud resources with the infrastructure-as-code approach"},{"text":"Status updates","url":"https://status.double.cloud/","description":"Check the current operational status of our services"},{"text":"Support","url":"/support/","description":"Learn more about our support tiers"}]},{"title":"Discover","items":[{"text":"Webinars","url":"/webinars/","description":"Sign up for the next webinar or watch previous ones"},{"text":"Blog","url":"/blog/","description":"Get insights from our team and the latest news"}]},{"image":{"src":"/assets/doublecloud/menu-bar/menu-banners-dc-ebook.png.webp","style":{"width":300,"height":300}},"text":"\u003ca href='/resources/clickhouse-ebook/' target='_self'\u003eGrab your ebook  →\u003c/a\u003e"}]}},{"text":"Company","type":"dc-dropdown","data":{"items":[{"text":"About DoubleCloud","url":"/company/about-us/"},{"text":"Careers","url":"/company/careers/"},{"text":"Contact us","url":"/company/contact-us/"}]}},{"text":"Pricing","url":"/pricing/"},{"text":"Documentation","url":"/docs/en/","target":"_self"}]},"logo":{"icon":"/assets/logo/dc-logo-dark.svg","text":""},"footer":{"underline":{"links":[{"text":"Customer Agreement","url":"/legal/en/customer_agreement/","target":"_blank"},{"text":"Privacy Policy","url":"/legal/en/privacy/","target":"_blank"},{"text":"Pricing","url":"/pricing/"},{"text":"Security","url":"/security/","target":"_blank"}],"copyright":"© 2024 DoubleCloud"},"columns":[{"title":"Products","links":[{"text":"Managed Service for ClickHouse®","url":"/services/managed-clickhouse/"},{"text":"Managed Service for Apache Kafka®","url":"/services/managed-kafka/"},{"text":"Managed Service for Apache Airflow®","url":"/services/managed-airflow"},{"text":"Data Transfer","url":"/services/doublecloud-transfer/"},{"text":"Data Visualization","url":"/services/doublecloud-visualization"}]},{"title":"Solutions","links":[{"text":"Case studies","url":"/resources/case-studies/"},{"text":"Customer-facing analytics","url":"/solutions/customer-facing-analytics/"},{"text":"Real-time analytics","url":"/solutions/real-time-analytics/"},{"text":"Observability and monitoring","url":"/solutions/observability-and-monitoring/"},{"text":"AdTech and MarTech data analytics","url":"/solutions/adtech/"},{"text":"Analytics for mobile and gaming Apps","url":"/solutions/web-mobile-gaming-apps/"},{"text":"EdTech data analytics","url":"/solutions/edtech/"},{"text":"FinTech data analytics","url":"/solutions/fintech-real-time-analytics/"}]},{"title":"Resources","links":[{"text":"Documentation","url":"/docs/en/"},{"text":"Webinars","url":"/webinars/"},{"text":"Blog","url":"/blog/"},{"text":"Support","url":"/support/"},{"text":"Status updates","url":"https://status.double.cloud/"},{"text":"Product comparisons","url":"/comparison/"},{"text":"Site map","url":"/sitemap/"}]},{"title":"Company","links":[{"text":"About DoubleCloud","url":"/company/about-us/"},{"text":"Careers","url":"/company/careers"},{"text":"AWS Partnership","url":"/aws-partnership/"}]}]},"forms":{"contact":"11819433.daba96b39df83b7903708cc4842e9dbb9c944cce"},"favicon":{"folder":"/assets/favicon"},"analytics":{"id":"GTM-5M39N8J","ignore":true,"popup":{"text":"\u003cp\u003eBy\u0026nbsp;clicking \u0026ldquo;Accept\u0026rdquo;, you agree to\u0026nbsp;the storing of\u0026nbsp;cookies on\u0026nbsp;your device to\u0026nbsp;help us\u0026nbsp;analyze site usage and assist in\u0026nbsp;our marketing efforts. However, you may \u0026ldquo;Decline\u0026rdquo; that. More details here in\u0026nbsp;\u003ca href=\"/legal/en/privacy/\"\u003ePrivacy Policy\u003c/a\u003e\u003c/p\u003e","buttons":{"accept":{"size":"xl","text":"Accept"},"decline":{"size":"xl","text":"Decline"}}}},"announcement":{"name":"webinar_announcement","url":"https://doublecloud-archive.github.io/blog/posts/2024/10/doublecloud-final-update/","conditions":{"date":{"start":"2000-01-01T00:00:00Z","end":"2099-12-31T00:00:00Z"}},"text":"\u003cp\u003e\u003cb\u003eDoubleCloud has wound down operations\u003c/b\u003e | This is\u0026nbsp;an\u0026nbsp;archived version of\u0026nbsp;the site. \u003cb\u003eLearn more \u0026rarr; \u003c/b\u003e\u003c/p\u003e"}},"meta":{"title":"Efficient data ingestion into MySQL with ClickHouse: streamlining 1 billion rows and local files | DoubleCloud","date":"2023-03-23T00:00:00Z","image":"/assets/blog/articles/fastest-way-to-ingest-data-into-mysql-sharing.png","canonicalUrl":"","organization":{"appTitle":"DoubleCloud","legalName":"DoubleCloud Inc","supportEmail":"","url":"https://double.cloud"},"description":"Explore efficient data ingestion into MySQL using ClickHouse. Learn how to streamline the process for 1 billion rows and local files. Discover a faster way to manage data overhead.","content":"\u003cp\u003eIn\u0026nbsp;this article, we\u0026rsquo;ll talk about:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#1-billion-rows-to-ingest-what-to-do?\"\u003e1 billion rows to\u0026nbsp;ingest\u0026nbsp;\u0026mdash; What to\u0026nbsp;do?\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#ingesting-local-files-to-mysql\"\u003eIngesting local files to\u0026nbsp;MySQL\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#let-clickhouse-handle-all-the-overhead\"\u003eLet ClickHouse handle all the overhead\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#conclusion\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eMySQL.\u003c/p\u003e\n\u003cp\u003eIt\u0026rsquo;s often the first choice for start-up\u0026rsquo;s looking for open-source software to\u0026nbsp;store application data.\u003c/p\u003e\n\u003cp\u003eNow I\u0026rsquo;ve been working with MySQL myself for over two decades, and although I\u0026rsquo;ve seen and worked with other databases since then, it\u0026nbsp;will always have a\u0026nbsp;special place in\u0026nbsp;my\u0026nbsp;heart as\u0026nbsp;it\u0026nbsp;was the first one to\u0026nbsp;show me\u0026nbsp;how much fun working with SQL databases is.\u003c/p\u003e\n\u003cp\u003eOf\u0026nbsp;course MySQL is\u0026nbsp;an\u0026nbsp;OLTP database, its strength is\u0026nbsp;in\u0026nbsp;serving multiple thousands of\u0026nbsp;small queries and transactions, but it\u0026rsquo;s still one of\u0026nbsp;the most often used DBMS for general workloads as\u0026nbsp;well.\u003c/p\u003e\n\u003cp\u003eAs\u0026nbsp;I\u0026nbsp;mentioned, a\u0026nbsp;lot of\u0026nbsp;companies start with MySQL, building their app (s) around it, watching it\u0026nbsp;grow, and even though MySQL definitely wasn\u0026rsquo;t intended to\u0026nbsp;be\u0026nbsp;used as\u0026nbsp;an\u0026nbsp;analytical database, there are a\u0026nbsp;lot of\u0026nbsp;companies (and individuals) still out there using it\u0026nbsp;for exactly this purpose.\u003c/p\u003e\n\u003cp\u003eAnd whilst there are a\u0026nbsp;lot of\u0026nbsp;good reasons why you shouldn\u0026rsquo;t use MySQL for big analytical workloads, MySQL is\u0026nbsp;still a\u0026nbsp;lot more capable of\u0026nbsp;analytical processes than a\u0026nbsp;lot of\u0026nbsp;people think it\u0026nbsp;is.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ve seen single MySQL instances having multiple Terabytes of\u0026nbsp;data stored within itself, some with tables handling multiple billions of\u0026nbsp;rows without bigger issues.\u003c/p\u003e\n\u003cp\u003eThe concept of\u0026nbsp;allowing different storage engines within the same environment allows MySQL to\u0026nbsp;stretch over its own limits by\u0026nbsp;several orders of\u0026nbsp;magnitude.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ve seen quite impressive analytical queries achieved with the use of\u0026nbsp;TokuDB or\u0026nbsp;RocksDB storage engines that I\u0026nbsp;never thought MySQL would be\u0026nbsp;able to\u0026nbsp;fulfill.\u003c/p\u003e\n\u003cp\u003eStill, MySQL was never meant to\u0026nbsp;be\u0026nbsp;used for big analytical workloads, as\u0026nbsp;it\u0026nbsp;was designed and built back in\u0026nbsp;a\u0026nbsp;time when clouds were meant to\u0026nbsp;be\u0026nbsp;in\u0026nbsp;the sky and formats of\u0026nbsp;data were proprietary instead of\u0026nbsp;being used for big open exchanges.\u003c/p\u003e\n\u003cp\u003eSo\u0026nbsp;one of\u0026nbsp;the biggest problems when doing analytical stuff in\u0026nbsp;MySQL is\u0026nbsp;still\u0026hellip; How do\u0026nbsp;you get the data in\u0026nbsp;it?\u003c/p\u003e\n\u003ch1 id=\"1-billion-rows-to-ingest-what-to-do?\"\u003e\u003ca href=\"#1-billion-rows-to-ingest-what-to-do?\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003e1 billion rows to\u0026nbsp;ingest\u0026nbsp;\u0026mdash; What to\u0026nbsp;do?\u003c/span\u003e\u003c/a\u003e1 billion rows to\u0026nbsp;ingest\u0026nbsp;\u0026mdash; What to\u0026nbsp;do?\u003c/h1\u003e\n\u003cp\u003eLately I\u0026rsquo;ve been running benchmarks on\u0026nbsp;different DBMS.\u003c/p\u003e\n\u003cp\u003eThe data set itself contained around 1\u0026nbsp;billion rows of\u0026nbsp;data, split into yearly chunks of\u0026nbsp;around 10\u0026nbsp;million rows, formatted in\u0026nbsp;one line of\u0026nbsp;JSON data per row, compressed via gzip, and located in\u0026nbsp;a\u0026nbsp;bucket in\u0026nbsp;Amazon\u0026rsquo;s S3.\u003c/p\u003e\n\u003cp\u003eEach row has 11\u0026nbsp;columns, but for our benchmark we\u0026nbsp;only need to\u0026nbsp;extract 5\u0026nbsp;of\u0026nbsp;those: date, station_id, tempMax, tempMin, tempAvg\u003c/p\u003e\n\u003cp\u003eFor this blog, I\u0026nbsp;will limit our data set to\u0026nbsp;only one year of\u0026nbsp;data, choosing 2020\u0026nbsp;at\u0026nbsp;random.\u003c/p\u003e\n\u003cp\u003eThe gzip compressed JSON data is\u0026nbsp;98MB.\u003c/p\u003e\n\u003cp\u003eWhen decompressed the file size grows to\u0026nbsp;2.7 GB, which is\u0026nbsp;expected as\u0026nbsp;the column names contained within the JSON are of\u0026nbsp;course very compressible.\u003c/p\u003e\n\u003cp\u003eThe total number of\u0026nbsp;rows contained within the file is\u0026nbsp;12,095,646.\u003c/p\u003e\n\u003cp\u003eFor our benchmark we\u0026nbsp;chose a\u0026nbsp;small Amazon RDS instance with 2\u0026nbsp;vCPU cores, 8GB of\u0026nbsp;RAM and enough storage space to\u0026nbsp;store the entire billion rows.\u003c/p\u003e\n\u003cp\u003eAs\u0026nbsp;we\u0026nbsp;only want to\u0026nbsp;show ingestion logic within this blog post, we\u0026nbsp;limited the data set to\u0026nbsp;2020, so\u0026nbsp;the whole data set would fit into ram.\u003c/p\u003e\n\u003cp\u003eThe table definition for our ingestion case is\u0026nbsp;quite simple.\u003c/p\u003e\n\u003cp\u003eWe\u0026nbsp;have a\u0026nbsp;primary key with 2\u0026nbsp;columns (date, station_id) and a\u0026nbsp;secondary index on\u0026nbsp;station_id for a\u0026nbsp;somewhat realistic scenario:\u003c/p\u003e\n\n    \u003cdiv class=\"yfm-clipboard\"\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs\"\u003eCREATE TABLE `sensor_data` (\n  `station_id` char(11) CHARACTER SET ascii COLLATE ascii_bin NOT NULL,\n  `date` date NOT NULL,\n  `tempAvg` int DEFAULT NULL,\n  `tempMax` int DEFAULT NULL,\n  `tempMin` int DEFAULT NULL,\n  PRIMARY KEY (`date`,`station_id`),\n  KEY `station` (`station_id`)\n) ENGINE=InnoDB;\n\u003c/code\u003e\u003c/pre\u003e\n\n    \u003csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\" class=\"yfm-clipboard-button\" data-animation=\"69\"\u003e\n        \u003cpath fill=\"currentColor\" d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\"\u003e\u003c/path\u003e\n        \u003cpath stroke=\"currentColor\" fill=\"transparent\" stroke-width=\"1.5\" d=\"M9.5 13l3 3l5 -5\" visibility=\"hidden\"\u003e\n            \u003canimate id=\"visibileAnimation-69\" attributeName=\"visibility\" from=\"hidden\" to=\"visible\" dur=\"0.2s\" fill=\"freeze\" begin\u003e\u003c/animate\u003e\n            \u003canimate id=\"hideAnimation-69\" attributeName=\"visibility\" from=\"visible\" to=\"hidden\" dur=\"1s\" begin=\"visibileAnimation-69.end+1\" fill=\"freeze\"\u003e\u003c/animate\u003e\n        \u003c/path\u003e\n    \u003c/svg\u003e\n    \u003c/div\u003e\n\u003cp\u003eI\u0026nbsp;chose InnoDB as\u0026nbsp;the storage engine, as\u0026nbsp;it\u0026rsquo;s the most commonly used table engine in\u0026nbsp;MySQL.\u003c/p\u003e\n\u003ch3 id=\"how-to-start?\"\u003e\u003ca href=\"#how-to-start?\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eHow to\u0026nbsp;start?\u003c/span\u003e\u003c/a\u003eHow to\u0026nbsp;start?\u003c/h3\u003e\n\u003cp\u003eWhen trying to\u0026nbsp;ingest our test data into MySQL is\u0026nbsp;where we\u0026nbsp;hit our first hurdle.\u003c/p\u003e\n\u003cp\u003eMySQL can\u0026rsquo;t read directly from S3 or\u0026nbsp;unzip the data on\u0026nbsp;the\u0026nbsp;fly. Therefore we\u0026nbsp;have to\u0026nbsp;do\u0026nbsp;some manual steps, before we\u0026nbsp;can even start:\u003c/p\u003e\n\u003cp\u003eDownload the files: Depending on\u0026nbsp;if\u0026nbsp;you need these tasks regularly or\u0026nbsp;just one time; this step could be\u0026nbsp;easy or\u0026nbsp;complex and the amount of\u0026nbsp;time needed to\u0026nbsp;implement that logic cannot be\u0026nbsp;estimated in\u0026nbsp;a\u0026nbsp;general blog. Also the download speed depends on\u0026nbsp;multiple factors, so\u0026nbsp;we\u0026nbsp;won\u0026rsquo;t include it\u0026nbsp;here in\u0026nbsp;our comparison, just mention that it\u0026nbsp;can have an\u0026nbsp;impact.\u003c/p\u003e\n\u003cp\u003eDecompressing the files: The easiest way to\u0026nbsp;achieve this is\u0026nbsp;by\u0026nbsp;using gzip -d {filename} on\u0026nbsp;the cli.\u003c/p\u003e\n\u003cp\u003eDecompressing files of\u0026nbsp;that size normally doesn\u0026rsquo;t take much time, but still, we\u0026nbsp;have to\u0026nbsp;write the decompressed data to\u0026nbsp;the disk. In\u0026nbsp;my\u0026nbsp;tests it\u0026nbsp;took around 7\u0026nbsp;seconds per file.\u003c/p\u003e\n\u003cp\u003eNow that we\u0026nbsp;have the files locally and decompressed, we\u0026nbsp;can really start to\u0026nbsp;ingest the data.\u003c/p\u003e\n\u003cp\u003eWe\u0026nbsp;need to\u0026nbsp;read the JSON data, get rid of\u0026nbsp;the unwanted columns, and \u0026ldquo;only\u0026rdquo; ingest the 12\u0026nbsp;million rows of\u0026nbsp;data into MySQL.\u003c/p\u003e\n\u003ch1 id=\"ingesting-local-files-to-mysql\"\u003e\u003ca href=\"#ingesting-local-files-to-mysql\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eIngesting local files to\u0026nbsp;MySQL\u003c/span\u003e\u003c/a\u003eIngesting local files to\u0026nbsp;MySQL\u003c/h1\u003e\n\u003ch3 id=\"use-scripting-language-like-php\"\u003e\u003ca href=\"#use-scripting-language-like-php\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eUse scripting language like PHP\u003c/span\u003e\u003c/a\u003eUse scripting language like PHP\u003c/h3\u003e\n\u003cp\u003eA\u0026nbsp;lot of\u0026nbsp;environments which use MySQL as\u0026nbsp;their main DBMS also make use of\u0026nbsp;PHP (remember LAMP stack anyone?).\u003c/p\u003e\n\u003cp\u003eA\u0026nbsp;simple solution to\u0026nbsp;achieve our goal would be\u0026nbsp;to\u0026nbsp;read the JSON file one line at\u0026nbsp;a\u0026nbsp;time, use PHPs parsing functionality to\u0026nbsp;get the values of\u0026nbsp;the 5\u0026nbsp;needed columns and insert them directly into MySQL.\u003c/p\u003e\n\u003cp\u003eSomething like this:\u003c/p\u003e\n\n    \u003cdiv class=\"yfm-clipboard\"\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs\"\u003e$mysqli = new mysqli($mysql_host, $mysql_username, $mysql_password, '', $mysql_port);\n$filename = $argv[1];\n\n$handle = @fopen($filename, 'r');\nif ($handle)\n{\n  while (($buffer = fgets($handle, 4096)) !== false)\n  {\n      $row = json_decode($buffer, true);\n      $mysqli-\u0026gt;query(\"\n        INSERT IGNORE INTO test.sensor_data\n          (station_id, date, tempMax, tempMin, tempAvg)\n        VALUES\n          ('{$row['station_id']}', '{$row['date']}', \" .\n              ($row['tempMax'] ?? 'NULL') . \", \" . \n              ($row['tempMin'] ?? 'NULL') . \", \" . \n              ($row['tempAvg'] ?? 'NULL') . \")  \n      \");\n  }\n  fclose($handle);\n}\nexit(0);\n\u003c/code\u003e\u003c/pre\u003e\n\n    \u003csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\" class=\"yfm-clipboard-button\" data-animation=\"112\"\u003e\n        \u003cpath fill=\"currentColor\" d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\"\u003e\u003c/path\u003e\n        \u003cpath stroke=\"currentColor\" fill=\"transparent\" stroke-width=\"1.5\" d=\"M9.5 13l3 3l5 -5\" visibility=\"hidden\"\u003e\n            \u003canimate id=\"visibileAnimation-112\" attributeName=\"visibility\" from=\"hidden\" to=\"visible\" dur=\"0.2s\" fill=\"freeze\" begin\u003e\u003c/animate\u003e\n            \u003canimate id=\"hideAnimation-112\" attributeName=\"visibility\" from=\"visible\" to=\"hidden\" dur=\"1s\" begin=\"visibileAnimation-112.end+1\" fill=\"freeze\"\u003e\u003c/animate\u003e\n        \u003c/path\u003e\n    \u003c/svg\u003e\n    \u003c/div\u003e\n\u003cp\u003eDoing it\u0026nbsp;this way unfortunately has the overhead of\u0026nbsp;executing a\u0026nbsp;single transaction for each row of\u0026nbsp;the dataset, hence only achieving around 60\u0026nbsp;rows/second, or\u0026nbsp;taking approx. 59\u0026nbsp;hours for the 12\u0026nbsp;million rows.\u003c/p\u003e\n\u003cp\u003eHowever, it\u0026rsquo;s easy to\u0026nbsp;optimize this logic by\u0026nbsp;introducing the bunching of\u0026nbsp;rows, combining 10k rows per insert query, speeding up\u0026nbsp;the whole process to\u0026nbsp;take 3\u0026nbsp;minutes and 30\u0026nbsp;seconds for the whole data set.\u003c/p\u003e\n\u003cp\u003eIngestion speed is\u0026nbsp;okay for the amount of\u0026nbsp;data but it\u0026nbsp;required a\u0026nbsp;lot of\u0026nbsp;manual work and no\u0026nbsp;piece of\u0026nbsp;error handling is\u0026nbsp;written yet.\u003c/p\u003e\n\u003cp\u003eBut there are ways to\u0026nbsp;optimize data ingestion in\u0026nbsp;MySQL.\u003c/p\u003e\n\u003ch3 id=\"use-json-functionality-with-mysql\"\u003e\u003ca href=\"#use-json-functionality-with-mysql\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eUse JSON functionality with MySQL\u003c/span\u003e\u003c/a\u003eUse JSON functionality with MySQL\u003c/h3\u003e\n\u003cp\u003eUnfortunately MySQL has no\u0026nbsp;feature for working with JSON files directly, but we\u0026nbsp;can interpret the whole file as\u0026nbsp;a\u0026nbsp;CSV file, having only a\u0026nbsp;single column containing a\u0026nbsp;JSON string.\u003c/p\u003e\n\u003cp\u003eThat way we\u0026nbsp;can load the JSON file via the LOAD DATA command, and then select the needed columns via the built-in JSON functions of\u0026nbsp;MySQL.\u003c/p\u003e\n\n    \u003cdiv class=\"yfm-clipboard\"\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs\"\u003eLOAD DATA LOCAL INFILE '~/MySQLBlog/sensors.2020.json'\nINTO TABLE test.sensor_data  \n(@json)\nSET station_id = JSON_UNQUOTE(JSON_EXTRACT(@json, '$.station_id')),\n  date = JSON_UNQUOTE(JSON_EXTRACT(@json, '$.date')),\n  tempMax = nullif(JSON_UNQUOTE(JSON_EXTRACT(@json, '$.tempMax')), 'null'),\n  tempMin = nullif(JSON_UNQUOTE(JSON_EXTRACT(@json, '$.tempMin')), 'null'),\n  tempAvg = nullif(JSON_UNQUOTE(JSON_EXTRACT(@json, '$.tempAvg')), 'null')\n\u003c/code\u003e\u003c/pre\u003e\n\n    \u003csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\" class=\"yfm-clipboard-button\" data-animation=\"134\"\u003e\n        \u003cpath fill=\"currentColor\" d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\"\u003e\u003c/path\u003e\n        \u003cpath stroke=\"currentColor\" fill=\"transparent\" stroke-width=\"1.5\" d=\"M9.5 13l3 3l5 -5\" visibility=\"hidden\"\u003e\n            \u003canimate id=\"visibileAnimation-134\" attributeName=\"visibility\" from=\"hidden\" to=\"visible\" dur=\"0.2s\" fill=\"freeze\" begin\u003e\u003c/animate\u003e\n            \u003canimate id=\"hideAnimation-134\" attributeName=\"visibility\" from=\"visible\" to=\"hidden\" dur=\"1s\" begin=\"visibileAnimation-134.end+1\" fill=\"freeze\"\u003e\u003c/animate\u003e\n        \u003c/path\u003e\n    \u003c/svg\u003e\n    \u003c/div\u003e\n\u003cp\u003eThis approach took 4\u0026nbsp;minutes and 55\u0026nbsp;seconds, so\u0026nbsp;even slower than the \u0026lsquo;optimized\u0026rsquo; PHP solution, but at\u0026nbsp;least it\u0026rsquo;s easier to\u0026nbsp;type (At\u0026nbsp;least for a\u0026nbsp;SQL guy like myself).\u003c/p\u003e\n\u003cp\u003eThe main problem with this approach taking so\u0026nbsp;long though is\u0026nbsp;that we\u0026nbsp;still send the data of\u0026nbsp;all the columns to\u0026nbsp;the MySQL server, and then throw all but 5\u0026nbsp;away within the server, hence having unneeded data sent over the wire to\u0026nbsp;MySQL.\u003c/p\u003e\n\u003cp\u003eAlso loading 10\u0026nbsp;million rows in\u0026nbsp;a\u0026nbsp;single transaction issues quite a\u0026nbsp;load to\u0026nbsp;the server.\u003c/p\u003e\n\u003ch3 id=\"use-cli-and-csv-functionality-with-mysql\"\u003e\u003ca href=\"#use-cli-and-csv-functionality-with-mysql\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eUse Cli and CSV functionality with MySQL\u003c/span\u003e\u003c/a\u003eUse Cli and CSV functionality with MySQL\u003c/h3\u003e\n\u003cp\u003eWe\u0026nbsp;can avoid using the JSON functionality of\u0026nbsp;MySQL as\u0026nbsp;well as\u0026nbsp;the sending of\u0026nbsp;unwanted data by\u0026nbsp;adding another preparation step before the ingestion to\u0026nbsp;MySQL.\u003c/p\u003e\n\u003cp\u003eWe\u0026nbsp;can use a\u0026nbsp;command line JSON tool like jq\u0026nbsp;to\u0026nbsp;select only the needed columns and create a\u0026nbsp;comma separated file, only containing the 5\u0026nbsp;needed columns:\u003c/p\u003e\n\u003cp\u003ejq\u0026nbsp;-r '. | [.station_id, .date, .tempMax, .tempMin, .tempAvg] | @csv' sensors.2020.json \u0026gt; sensors_parsed.2020.csv\u003c/p\u003e\n\u003cp\u003eParsing JSON to\u0026nbsp;CSV on\u0026nbsp;cli takes around 60\u0026nbsp;seconds in\u0026nbsp;that case, generating a\u0026nbsp;CSV file with 376\u0026nbsp;MB\u0026nbsp;of\u0026nbsp;data.\u003c/p\u003e\n\u003cp\u003eNow importing the csv file can be\u0026nbsp;done quite easily within the MySQL client again:\u003c/p\u003e\n\n    \u003cdiv class=\"yfm-clipboard\"\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs\"\u003eLOAD DATA LOCAL INFILE '~/MySQLBlog/sensors_parsed.2020.csv'\nINTO TABLE test.sensor_data  \nFIELDS TERMINATED by ',' OPTIONALLY ENCLOSED BY '\"'\n(station_id, date, @tempMax, @tempMin, @tempAvg)\nSET tempMax = nullif(@tempMax, ''),\n  tempMin = nullif(@tempMin, ''),\n  tempAvg = nullif(@tempAvg, '')\n\u003c/code\u003e\u003c/pre\u003e\n\n    \u003csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\" class=\"yfm-clipboard-button\" data-animation=\"162\"\u003e\n        \u003cpath fill=\"currentColor\" d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\"\u003e\u003c/path\u003e\n        \u003cpath stroke=\"currentColor\" fill=\"transparent\" stroke-width=\"1.5\" d=\"M9.5 13l3 3l5 -5\" visibility=\"hidden\"\u003e\n            \u003canimate id=\"visibileAnimation-162\" attributeName=\"visibility\" from=\"hidden\" to=\"visible\" dur=\"0.2s\" fill=\"freeze\" begin\u003e\u003c/animate\u003e\n            \u003canimate id=\"hideAnimation-162\" attributeName=\"visibility\" from=\"visible\" to=\"hidden\" dur=\"1s\" begin=\"visibileAnimation-162.end+1\" fill=\"freeze\"\u003e\u003c/animate\u003e\n        \u003c/path\u003e\n    \u003c/svg\u003e\n    \u003c/div\u003e\n\u003cp\u003eThis approach takes 1\u0026nbsp;minute 52\u0026nbsp;seconds, which is\u0026nbsp;a\u0026nbsp;lot faster than reading directly from JSON to\u0026nbsp;MySQL.\u003c/p\u003e\n\u003cp\u003eWe\u0026nbsp;still, however, have the problem of\u0026nbsp;big transactions. So\u0026nbsp;if\u0026nbsp;we\u0026nbsp;want our MySQL instance to\u0026nbsp;handle other requests without trouble, we\u0026rsquo;d need to\u0026nbsp;split the csv file into smaller chunks of\u0026nbsp;10k rows to\u0026nbsp;reduce the load on\u0026nbsp;the server.\u003c/p\u003e\n\u003cp\u003eThus we\u0026nbsp;again need to\u0026nbsp;write simple cli logic to\u0026nbsp;split the csv into chunks and then write a\u0026nbsp;loop to\u0026nbsp;go\u0026nbsp;over the 1,000 chunk files, ingesting one after another.\u003c/p\u003e\n\u003cp\u003eOverall, regarding the 60\u0026nbsp;sec of\u0026nbsp;cli time and the 1\u0026nbsp;min 52\u0026nbsp;sec for inserting the data, the total time to\u0026nbsp;ingest our 12\u0026nbsp;million JSON rows takes 2\u0026nbsp;min 52\u0026nbsp;sec, therefore being 40% faster than reading JSON directly from MySQL and still being 17% faster than the scripted solution.\u003c/p\u003e\n\u003ch1 id=\"let-clickhouse-handle-all-the-overhead\"\u003e\u003ca href=\"#let-clickhouse-handle-all-the-overhead\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eLet ClickHouse handle all the overhead\u003c/span\u003e\u003c/a\u003eLet ClickHouse handle all the overhead\u003c/h1\u003e\n\u003cp\u003eNow even though we\u0026nbsp;can reach a\u0026nbsp;somewhat acceptable speed with cli tools and the mysql client, it\u0026rsquo;s still not a\u0026nbsp;very nice approach.\u003c/p\u003e\n\u003cp\u003eAlso\u0026hellip; imagine if\u0026nbsp;you had to\u0026nbsp;do\u0026nbsp;this on\u0026nbsp;a\u0026nbsp;regular basis, as\u0026nbsp;your software imports data from external sources on\u0026nbsp;a\u0026nbsp;nightly basis.\u003c/p\u003e\n\u003cp\u003eYou\u0026rsquo;d need to\u0026nbsp;add checks for each step to\u0026nbsp;make sure it\u0026rsquo;s successfully finished and then clean up\u0026nbsp;all the temporary data files afterwards again.\u003c/p\u003e\n\u003cp\u003eAnd if\u0026nbsp;you need to\u0026nbsp;integrate data from multiple sources, in\u0026nbsp;multiple formats, it\u0026nbsp;would get very ugly very quickly.\u003c/p\u003e\n\u003cp\u003eThat\u0026rsquo;s where ClickHouse can help you.\u003c/p\u003e\n\u003cp\u003eClickHouse has extremely rich integration capabilities, reading from multiple sources (like s3, rest APIs, local files, external DBMS etc).\u003c/p\u003e\n\u003cp\u003eSo\u0026nbsp;even if\u0026nbsp;you don\u0026rsquo;t want to\u0026nbsp;use ClickHouse as\u0026nbsp;your daily driver for your analytical queries, you can still make use of\u0026nbsp;its integration features.\u003c/p\u003e\n\u003cp\u003eI\u0026nbsp;connected to\u0026nbsp;a\u0026nbsp;managed ClickHouse cluster within the DoubleCloud sitting in\u0026nbsp;the same AWS region as\u0026nbsp;my\u0026nbsp;test MySQL instance, and then just executed this query:\u003c/p\u003e\n\n    \u003cdiv class=\"yfm-clipboard\"\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs\"\u003eSET external_storage_connect_timeout_sec = 300;\nSET mysql_max_rows_to_insert = 10000;\nINSERT INTO FUNCTION mysql('{mysqlurl}',\n  test, sensor_data,\n  '{username}', '{password}') \n(station_id, date, tempMax, tempMin, tempAvg)\nSELECT station_id, date, tempMax, tempMin, tempAvg\nFROM s3('https://{s3path}/sensors.2020.json.gz', '{aws_access_id}', '{aws_secret_access_id}', JSONEachRow,\n  $$station_id String, date Date, tempMax Nullable(Int32), tempMin Nullable(Int32), tempAvg Nullable(Int32)$$)\n\u003c/code\u003e\u003c/pre\u003e\n\n    \u003csvg width=\"16\" height=\"16\" viewBox=\"0 0 24 24\" class=\"yfm-clipboard-button\" data-animation=\"202\"\u003e\n        \u003cpath fill=\"currentColor\" d=\"M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z\"\u003e\u003c/path\u003e\n        \u003cpath stroke=\"currentColor\" fill=\"transparent\" stroke-width=\"1.5\" d=\"M9.5 13l3 3l5 -5\" visibility=\"hidden\"\u003e\n            \u003canimate id=\"visibileAnimation-202\" attributeName=\"visibility\" from=\"hidden\" to=\"visible\" dur=\"0.2s\" fill=\"freeze\" begin\u003e\u003c/animate\u003e\n            \u003canimate id=\"hideAnimation-202\" attributeName=\"visibility\" from=\"visible\" to=\"hidden\" dur=\"1s\" begin=\"visibileAnimation-202.end+1\" fill=\"freeze\"\u003e\u003c/animate\u003e\n        \u003c/path\u003e\n    \u003c/svg\u003e\n    \u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003eThe part between the $$ signs can be\u0026nbsp;omitted in\u0026nbsp;most use cases, as\u0026nbsp;ClickHouse can do\u0026nbsp;schema inference, but in\u0026nbsp;my\u0026nbsp;test data this was not possible\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe whole process of\u0026nbsp;ingesting the data to\u0026nbsp;MySQL now took 2\u0026nbsp;minutes and 11\u0026nbsp;sec, and as\u0026nbsp;the inserts are automatically bunched to\u0026nbsp;10k, the cpu of\u0026nbsp;MySQL barely noticed anything.\u003c/p\u003e\n\u003cp\u003eBut not only is\u0026nbsp;it\u0026nbsp;30% faster on\u0026nbsp;the ingestion time, it\u0026nbsp;also did the downloading and decompression step.\u003c/p\u003e\n\u003cp\u003eAnd even better, it\u0026nbsp;saved me\u0026nbsp;a\u0026nbsp;lot of\u0026nbsp;time implementing the whole logic!\u003c/p\u003e\n\u003cp\u003eClickHouse does everything for me, I\u0026nbsp;just had to\u0026nbsp;execute a\u0026nbsp;single query and wait for the result, making it\u0026nbsp;a\u0026nbsp;lot easier on\u0026nbsp;the implementation side, to\u0026nbsp;handle possible errors, cleanups (what cleanups?) etc.\u003c/p\u003e\n\u003cp\u003eAnd as\u0026nbsp;a\u0026nbsp;bonus, I\u0026nbsp;also have the full tool set of\u0026nbsp;SQL at\u0026nbsp;my\u0026nbsp;hands directly on\u0026nbsp;insert already.\u003c/p\u003e\n\u003cp\u003eI\u0026nbsp;could add WHERE conditions to\u0026nbsp;filter out rows before inserting, can manipulate or\u0026nbsp;parse columns, or\u0026nbsp;I\u0026nbsp;can even do\u0026nbsp;aggregations before inserting the data to\u0026nbsp;MySQL!\u003c/p\u003e\n\u003cp\u003eIf\u0026nbsp;anyone knows a\u0026nbsp;faster way of\u0026nbsp;getting data into MySQL, please contact me\u0026nbsp;;).\u003c/p\u003e\n\u003ch1 id=\"conclusion\"\u003e\u003ca href=\"#conclusion\" class=\"yfm-anchor\" aria-hidden=\"true\"\u003e\u003cspan class=\"visually-hidden\"\u003eConclusion\u003c/span\u003e\u003c/a\u003eConclusion\u003c/h1\u003e\n\n\u003cp\u003eAs\u0026nbsp;we\u0026nbsp;can see, utilizing ClickHouse for ingesting data to\u0026nbsp;MySQL has been the fastest way to\u0026nbsp;do\u0026nbsp;so.\u003c/p\u003e\n\u003cp\u003eClickHouse can not only spare you time if\u0026nbsp;you use it\u0026nbsp;as\u0026nbsp;DBMS for your analytical queries but it\u0026nbsp;can also spare you a\u0026nbsp;lot of\u0026nbsp;implementation time.\u003c/p\u003e\n\u003cp\u003eWith ClickHouse it\u0026rsquo;s extremely easy to\u0026nbsp;convert data into different formats, reading from multiple sources (s3 was just an\u0026nbsp;example here, you can utilize RestAPIs, other DBMS, local files, Kafka, etc.), and doing data manipulation on\u0026nbsp;the fly.\u003c/p\u003e\n\u003cp\u003eSo\u0026nbsp;especially if\u0026nbsp;you have multiple sources of\u0026nbsp;data, it\u0026nbsp;makes the whole process of\u0026nbsp;collecting the data, downloading, converting etc. a\u0026nbsp;lot easier.\u003c/p\u003e\n\u003cp\u003eYou just need to\u0026nbsp;rewrite a\u0026nbsp;single query for echo of\u0026nbsp;your ingestion process, instead of\u0026nbsp;rewriting a\u0026nbsp;whole bunch of\u0026nbsp;commands for downloading, preparing inserting etc.\u003c/p\u003e\n\u003cp\u003eIn\u0026nbsp;the end you can still utilize your MySQL instances for all your preferred workloads, and still let ClickHouse help you be\u0026nbsp;faster overall!\u003c/p\u003e\n\u003cp\u003eAnd this is\u0026nbsp;the reason why as\u0026nbsp;a\u0026nbsp;tech guy, I\u0026nbsp;fell in\u0026nbsp;love with ClickHouse.\u003c/p\u003e\n\u003cp\u003eIt\u0026rsquo;s not because it\u0026rsquo;s fast as\u0026nbsp;hell in\u0026nbsp;analytical workloads (although it\u0026nbsp;is). It\u0026rsquo;s because it\u0026rsquo;s so\u0026nbsp;extremely versatile to\u0026nbsp;use, from prototypes to\u0026nbsp;production workloads or\u0026nbsp;just for having fun with.\u003c/p\u003e\n\u003cp\u003eClickHouse is\u0026nbsp;a\u0026nbsp;trademark of\u0026nbsp;ClickHouse, Inc. https://clickhouse.com\u003c/p\u003e\n","sharing":{"title":"The fastest way to ingest data into MySQL is… ClickHouse®","description":"\u003cp\u003eWritten By: Stefan Kaeser, DoubleCloud Senior Solution Architect\u003c/p\u003e","image":"/assets/blog/articles/fastest-way-to-ingest-data-into-mysql-sharing.png","shareGenImage":"","shareGenTitle":"The fastest way to ingest data into MySQL is… ClickHouse®"},"keywords":[],"noIndex":false,"authors":[],"tags":[{"icon":null,"slug":"insights","name":"Insights","createdAt":"","updatedAt":"","count":0},{"icon":null,"slug":"ClickHouse","name":"ClickHouse","createdAt":"","updatedAt":"","count":0}],"metaSchema":{"@context":"https://schema.org","@graph":[{"@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"item":{"@id":"insights","name":"Insights"}},{"@type":"ListItem","position":2,"item":{"@id":"ClickHouse","name":"ClickHouse"}}]},{"@type":"BlogPosting","@id":"https://double.cloud/blog/posts/2023/03/fastest-way-to-ingest-data-into-mysql/","url":"https://double.cloud/blog/posts/2023/03/fastest-way-to-ingest-data-into-mysql/","name":"The fastest way to ingest data into MySQL is… ClickHouse®","headline":"The fastest way to ingest data into MySQL is… ClickHouse®","abstract":"\u003cp\u003eWritten By: Stefan Kaeser, DoubleCloud Senior Solution Architect\u003c/p\u003e","description":"\u003cp\u003eWritten By: Stefan Kaeser, DoubleCloud Senior Solution Architect\u003c/p\u003e","dateCreated":"2023-03-23T00:00:00Z","datePublished":"2023-03-23T00:00:00Z","dateModified":"2023-03-23T00:00:00Z","author":{"@type":"Organization","name":"DoubleCloud","legalName":"DoubleCloud Inc","url":"https://double.cloud/","logo":{"@type":"ImageObject","url":"","width":32,"height":32}},"creator":{"@type":"Organization","name":"DoubleCloud","legalName":"DoubleCloud Inc","url":"https://double.cloud/","logo":{"@type":"ImageObject","url":"","width":32,"height":32}},"publisher":{"@type":"Organization","name":"DoubleCloud","legalName":"DoubleCloud Inc","url":"https://double.cloud/","logo":{"@type":"ImageObject","url":"","width":32,"height":32}},"copyrightHolder":{"@type":"Organization","name":"DoubleCloud","legalName":"DoubleCloud Inc","url":"https://double.cloud/","logo":{"@type":"ImageObject","url":"","width":32,"height":32}},"copyrightYear":2025,"mainEntityOfPage":{"@type":"WebPage","@id":"https://double.cloud/blog/posts/2023/03/fastest-way-to-ingest-data-into-mysql/"},"inLanguage":{"@type":"Language","name":"English","alternateName":"en"},"keywords":["Insights","ClickHouse"],"image":"https://double.cloud/assets/blog/articles/fastest-way-to-ingest-data-into-mysql-small-cover.png","sharedContent":{"@type":"WebPage","headline":"The fastest way to ingest data into MySQL is… ClickHouse®","url":"https://double.cloud/blog/posts/2023/03/fastest-way-to-ingest-data-into-mysql/","author":{"@type":"Organization","name":"DoubleCloud","legalName":"DoubleCloud Inc","url":"https://double.cloud/","logo":{"@type":"ImageObject","url":"","width":32,"height":32}}},"wordCount":"","articleBody":""}]}},"routingData":{"hostname":"double.cloud"},"deviceData":{"isRobot":true,"isMobile":false,"isTablet":false},"csrfToken":"7P4hRsW4-3Ni-Sr8b8peTxGW71IfpXBihzYY","clientConfig":{"appTitle":"DoubleCloud","legalName":"DoubleCloud Inc","supportEmail":"","hosts":{"site":"https://double.cloud","console":"https://app.double.cloud"}},"ignoreConsent":false,"noNextImg":false,"noSnippet":null},"__N_SSP":true},"page":"/blog/posts/[...slug]","query":{"slug":["2023","03","fastest-way-to-ingest-data-into-mysql"]},"buildId":"HkxA3M0ES7gp3V0n_0ecw","isFallback":false,"gssp":true,"locale":"en","locales":["en"],"defaultLocale":"en","scriptLoader":[]}</script></body></html>